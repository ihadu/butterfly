<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Flink_Data_Sink</title>
    <url>/2022/03/19/Flink-Data-Sink/</url>
    <content><![CDATA[<h2 id="一、Data-Sinks">一、Data Sinks</h2>
<p>在使用 Flink 进行数据处理时，数据经 Data Source 流入，然后通过系列 Transformations 的转化，最终可以通过 Sink 将计算结果进行输出，Flink Data Sinks 就是用于定义数据流最终的输出位置。Flink 提供了几个较为简单的 Sink API 用于日常的开发，具体如下：</p>
<h3 id="1-1-writeAsText">1.1 writeAsText</h3>
<p><code>writeAsText</code> 用于将计算结果以文本的方式并行地写入到指定文件夹下，除了路径参数是必选外，该方法还可以通过指定第二个参数来定义输出模式，它有以下两个可选值：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>WriteMode.NO_OVERWRITE</strong>：当指定路径上不存在任何文件时，才执行写出操作；</p>
</li>
<li class="lvl-2">
<p><strong>WriteMode.OVERWRITE</strong>：不论指定路径上是否存在文件，都执行写出操作；如果原来已有文件，则进行覆盖。</p>
</li>
</ul>
<p>使用示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">streamSource.writeAsText(<span class="string">&quot;D:\\out&quot;</span>, FileSystem.WriteMode.OVERWRITE);</span><br></pre></td></tr></table></figure>
<p>以上写出是以并行的方式写出到多个文件，如果想要将输出结果全部写出到一个文件，需要设置其并行度为 1：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">streamSource.writeAsText(<span class="string">&quot;D:\\out&quot;</span>, FileSystem.WriteMode.OVERWRITE).setParallelism(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<h3 id="1-2-writeAsCsv">1.2 writeAsCsv</h3>
<p><code>writeAsCsv</code> 用于将计算结果以 CSV 的文件格式写出到指定目录，除了路径参数是必选外，该方法还支持传入输出模式，行分隔符，和字段分隔符三个额外的参数，其方法定义如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">writeAsCsv(String path, WriteMode writeMode, String rowDelimiter, String fieldDelimiter) </span><br></pre></td></tr></table></figure>
<h3 id="1-3-print-printToErr">1.3 print \ printToErr</h3>
<p><code>print \ printToErr</code> 是测试当中最常用的方式，用于将计算结果以标准输出流或错误输出流的方式打印到控制台上。</p>
<h3 id="1-4-writeUsingOutputFormat">1.4 writeUsingOutputFormat</h3>
<p>采用自定义的输出格式将计算结果写出，上面介绍的 <code>writeAsText</code> 和 <code>writeAsCsv</code> 其底层调用的都是该方法，源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> DataStreamSink&lt;T&gt; <span class="title function_">writeAsText</span><span class="params">(String path, WriteMode writeMode)</span> &#123;</span><br><span class="line">    TextOutputFormat&lt;T&gt; tof = <span class="keyword">new</span> <span class="title class_">TextOutputFormat</span>&lt;&gt;(<span class="keyword">new</span> <span class="title class_">Path</span>(path));</span><br><span class="line">    tof.setWriteMode(writeMode);</span><br><span class="line">    <span class="keyword">return</span> writeUsingOutputFormat(tof);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-5-writeToSocket">1.5 writeToSocket</h3>
<p><code>writeToSocket</code> 用于将计算结果以指定的格式写出到 Socket 中，使用示例如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">streamSource.writeToSocket(&quot;192.168.0.226&quot;, 9999, new SimpleStringSchema());</span><br></pre></td></tr></table></figure>
<h2 id="二、Streaming-Connectors">二、Streaming Connectors</h2>
<p>除了上述 API 外，Flink 中还内置了系列的 Connectors 连接器，用于将计算结果输入到常用的存储系统或者消息中间件中，具体如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>Apache Kafka (支持 source 和 sink)</p>
</li>
<li class="lvl-2">
<p>Apache Cassandra (sink)</p>
</li>
<li class="lvl-2">
<p>Amazon Kinesis Streams (source/sink)</p>
</li>
<li class="lvl-2">
<p>Elasticsearch (sink)</p>
</li>
<li class="lvl-2">
<p>Hadoop FileSystem (sink)</p>
</li>
<li class="lvl-2">
<p>RabbitMQ (source/sink)</p>
</li>
<li class="lvl-2">
<p>Apache NiFi (source/sink)</p>
</li>
<li class="lvl-2">
<p>Google PubSub (source/sink)</p>
</li>
</ul>
<p>除了内置的连接器外，你还可以通过 Apache Bahir 的连接器扩展 Flink。Apache Bahir 旨在为分布式数据分析系统 (如 Spark，Flink) 等提供功能上的扩展，当前其支持的与 Flink Sink 相关的连接器如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>Apache ActiveMQ (source/sink)</p>
</li>
<li class="lvl-2">
<p>Apache Flume (sink)</p>
</li>
<li class="lvl-2">
<p>Redis (sink)</p>
</li>
<li class="lvl-2">
<p>Akka (sink)</p>
</li>
</ul>
<p>这里接着在 Data Sources 章节介绍的整合 Kafka Source 的基础上，将 Kafka Sink 也一并进行整合，具体步骤如下。</p>
<h2 id="三、整合-Kafka-Sink">三、整合 Kafka Sink</h2>
<h3 id="3-1-addSink">3.1 addSink</h3>
<p>Flink 提供了 addSink 方法用来调用自定义的 Sink 或者第三方的连接器，想要将计算结果写出到 Kafka，需要使用该方法来调用 Kafka 的生产者 FlinkKafkaProducer，具体代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.指定Kafka的相关配置属性</span></span><br><span class="line"><span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.200.0:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.接收Kafka上的数据</span></span><br><span class="line">DataStream&lt;String&gt; stream = env</span><br><span class="line">    .addSource(<span class="keyword">new</span> <span class="title class_">FlinkKafkaConsumer</span>&lt;&gt;(<span class="string">&quot;flink-stream-in-topic&quot;</span>, <span class="keyword">new</span> <span class="title class_">SimpleStringSchema</span>(), properties));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.定义计算结果到 Kafka ProducerRecord 的转换</span></span><br><span class="line">KafkaSerializationSchema&lt;String&gt; kafkaSerializationSchema = <span class="keyword">new</span> <span class="title class_">KafkaSerializationSchema</span>&lt;String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> ProducerRecord&lt;<span class="type">byte</span>[], <span class="type">byte</span>[]&gt; serialize(String element, <span class="meta">@Nullable</span> Long timestamp) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;flink-stream-out-topic&quot;</span>, element.getBytes());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// 4. 定义Flink Kafka生产者</span></span><br><span class="line">FlinkKafkaProducer&lt;String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">FlinkKafkaProducer</span>&lt;&gt;(<span class="string">&quot;flink-stream-out-topic&quot;</span>,</span><br><span class="line">                                                                    kafkaSerializationSchema,</span><br><span class="line">                                                                    properties,</span><br><span class="line">                                               FlinkKafkaProducer.Semantic.AT_LEAST_ONCE, <span class="number">5</span>);</span><br><span class="line"><span class="comment">// 5. 将接收到输入元素*2后写出到Kafka</span></span><br><span class="line">stream.map((MapFunction&lt;String, String&gt;) value -&gt; value + value).addSink(kafkaProducer);</span><br><span class="line">env.execute(<span class="string">&quot;Flink Streaming&quot;</span>);</span><br></pre></td></tr></table></figure>
<h3 id="3-2-创建输出主题">3.2 创建输出主题</h3>
<p>创建用于输出测试的主题：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --create \</span><br><span class="line">                    --bootstrap-server hadoop001:9092 \</span><br><span class="line">                    --replication-factor 1 \</span><br><span class="line">                    --partitions 1  \</span><br><span class="line">                    --topic flink-stream-out-topic</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看所有主题</span></span><br><span class="line"> bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092</span><br></pre></td></tr></table></figure>
<h3 id="3-3-启动消费者">3.3 启动消费者</h3>
<p>启动一个 Kafka 消费者，用于查看 Flink 程序的输出情况：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server hadoop001:<span class="number">9092</span> --topic flink-stream-out-topic</span><br></pre></td></tr></table></figure>
<h3 id="3-4-测试结果">3.4 测试结果</h3>
<p>在 Kafka 生产者上发送消息到 Flink 程序，观察 Flink 程序转换后的输出情况，具体如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-kafka-producer-consumer.png"/> </div>
<p>可以看到 Kafka 生成者发出的数据已经被 Flink 程序正常接收到，并经过转换后又输出到 Kafka 对应的 Topic 上。</p>
<h2 id="四、自定义-Sink">四、自定义 Sink</h2>
<p>除了使用内置的第三方连接器外，Flink 还支持使用自定义的 Sink 来满足多样化的输出需求。想要实现自定义的 Sink ，需要直接或者间接实现 SinkFunction 接口。通常情况下，我们都是实现其抽象类 RichSinkFunction，相比于 SinkFunction ，其提供了更多的与生命周期相关的方法。两者间的关系如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-richsink.png"/> </div>
<p>这里我们以自定义一个 FlinkToMySQLSink 为例，将计算结果写出到 MySQL 数据库中，具体步骤如下：</p>
<h3 id="4-1-导入依赖">4.1 导入依赖</h3>
<p>首先需要导入 MySQL 相关的依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>8.0.16<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-2-自定义-Sink">4.2 自定义 Sink</h3>
<p>继承自 RichSinkFunction，实现自定义的 Sink ：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlinkToMySQLSink</span> <span class="keyword">extends</span> <span class="title class_">RichSinkFunction</span>&lt;Employee&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> PreparedStatement stmt;</span><br><span class="line">    <span class="keyword">private</span> Connection conn;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        Class.forName(<span class="string">&quot;com.mysql.cj.jdbc.Driver&quot;</span>);</span><br><span class="line">        conn = DriverManager.getConnection(<span class="string">&quot;jdbc:mysql://192.168.0.229:3306/employees&quot;</span> +</span><br><span class="line">                                           <span class="string">&quot;?characterEncoding=UTF-8&amp;serverTimezone=UTC&amp;useSSL=false&quot;</span>, </span><br><span class="line">                                           <span class="string">&quot;root&quot;</span>, </span><br><span class="line">                                           <span class="string">&quot;123456&quot;</span>);</span><br><span class="line">        <span class="type">String</span> <span class="variable">sql</span> <span class="operator">=</span> <span class="string">&quot;insert into emp(name, age, birthday) values(?, ?, ?)&quot;</span>;</span><br><span class="line">        stmt = conn.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">invoke</span><span class="params">(Employee value, Context context)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        stmt.setString(<span class="number">1</span>, value.getName());</span><br><span class="line">        stmt.setInt(<span class="number">2</span>, value.getAge());</span><br><span class="line">        stmt.setDate(<span class="number">3</span>, value.getBirthday());</span><br><span class="line">        stmt.executeUpdate();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="built_in">super</span>.close();</span><br><span class="line">        <span class="keyword">if</span> (stmt != <span class="literal">null</span>) &#123;</span><br><span class="line">            stmt.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (conn != <span class="literal">null</span>) &#123;</span><br><span class="line">            conn.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-3-使用自定义-Sink">4.3 使用自定义 Sink</h3>
<p>想要使用自定义的 Sink，同样是需要调用 addSink 方法，具体如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="type">Date</span> <span class="variable">date</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Date</span>(System.currentTimeMillis());</span><br><span class="line">DataStreamSource&lt;Employee&gt; streamSource = env.fromElements(</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Employee</span>(<span class="string">&quot;hei&quot;</span>, <span class="number">10</span>, date),</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Employee</span>(<span class="string">&quot;bai&quot;</span>, <span class="number">20</span>, date),</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">Employee</span>(<span class="string">&quot;ying&quot;</span>, <span class="number">30</span>, date));</span><br><span class="line">streamSource.addSink(<span class="keyword">new</span> <span class="title class_">FlinkToMySQLSink</span>());</span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>
<h3 id="4-4-测试结果">4.4 测试结果</h3>
<p>启动程序，观察数据库写入情况：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-mysql-sink.png"/> </div>
<p>数据库成功写入，代表自定义 Sink 整合成功。</p>
<blockquote>
<p>以上所有用例的源码见本仓库：<a href="https://github.com/oicio/BigData-Notes/tree/master/code/Flink/flink-kafka-integration">flink-kafka-integration</a></p>
</blockquote>
<h2 id="参考资料">参考资料</h2>
<ol>
<li class="lvl-3">
<p>data-sinks： <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#data-sinks">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#data-sinks</a></p>
</li>
<li class="lvl-3">
<p>Streaming Connectors：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html</a></p>
</li>
<li class="lvl-3">
<p>Apache Kafka Connector： <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink-Broadcast-State</title>
    <url>/2023/01/03/Flink-Broadcast-State/</url>
    <content><![CDATA[<h3 id="背景">背景</h3>
<p>有这样一个需求：flink或者spark任务需要访问数据库，或者用到表schema信息。但此时数据库中的字段有添加或者修改时(schama发生改变的时候)，这时候任务就会失败。最直接的做法就是重启flink或spark任务，但该做法会对业务数据造成一定的影响。</p>
<p>方案：<strong>将改动的schema信息放入redis中，再通过broadcast广播的方式传送给数据流。</strong></p>
<h3 id="flink-broadcast-state">flink broadcast state</h3>
<p>Broadcast State是Flink支持的一种Operator State。使用Broadcast State，可以在Flink程序的一个Stream中输入数据记录，然后将这些数据记录广播（Broadcast）到下游的每个Task中，使得这些数据记录能够为所有的Task所共享，比如一些用于配置的数据记录。这样，每个Task在处理其所对应的Stream中记录的时候，读取这些配置，来满足实际数据处理需要。</p>
<p>步骤：</p>
<ol>
<li class="lvl-3">
<p>定义一个MapStateDescriptor来描述要广播数据的地址</p>
</li>
<li class="lvl-3">
<p>添加数据源，并注册为广播流</p>
</li>
<li class="lvl-3">
<p>连接广播流和处理数据的流</p>
</li>
<li class="lvl-3">
<p>实现连接的处理方法</p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> MapStateDescriptor&lt;String, TableSchema&gt; mapStateDescriptor =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">MapStateDescriptor</span>&lt;&gt;(</span><br><span class="line">                    <span class="string">&quot;broadcast&quot;</span>,</span><br><span class="line">                    BasicTypeInfo.STRING_TYPE_INFO,</span><br><span class="line">                    TypeInformation.of(<span class="keyword">new</span> <span class="title class_">TypeHint</span>&lt;TableSchema&gt;() &#123;</span><br><span class="line">                    &#125;));</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    	首先从redis中得到最新的table schema信息</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="type">TableSchema</span> <span class="variable">meta</span> <span class="operator">=</span> getLatestMeta(..);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    	数据流</span></span><br><span class="line"><span class="comment">    	生成KeyedStream的流</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    KeyedStream&lt;String,String&gt; keyedStream = env.addSource(..)</span><br><span class="line">        .map(..)</span><br><span class="line">        .returns(String.class)</span><br><span class="line">        .keyby(<span class="keyword">new</span> <span class="title class_">KeySelector</span>&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">getKey</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    	广播流</span></span><br><span class="line"><span class="comment">    	该流主要存储了table schema的信息，数据量小，广播到各个Task</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    BroadcastStream&lt;TableSchema&gt; broadcastStream = env.addSource(..)</span><br><span class="line">        .map(..)</span><br><span class="line">        .returns(TableSchema.class)</span><br><span class="line">        .broadcast(mapStateDescriptor);</span><br><span class="line">        </span><br><span class="line">    keyedStream.connect(broadcastStream)</span><br><span class="line">        .process(<span class="keyword">new</span> <span class="title class_">KeyedBroadcastProcessFunction</span>&lt;String, String, TableSchema, String&gt;()&#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processBroadcastElement</span><span class="params">(TableSchema value, Context ctx, Collector&lt;String&gt; out)</span>&#123;</span><br><span class="line">            <span class="comment">//获取旧的值</span></span><br><span class="line">            <span class="type">TableSchema</span> <span class="variable">old</span> <span class="operator">=</span> ctx.getBroadcastState(mapStateDescriptor).get(<span class="string">&quot;id&quot;</span>);</span><br><span class="line">            System.out.println(<span class="string">&quot;old value:&quot;</span>+old+<span class="string">&quot;,new value:&quot;</span>+value);</span><br><span class="line">            <span class="comment">//更新新的值</span></span><br><span class="line">            state.put(<span class="string">&quot;id&quot;</span>, value);</span><br><span class="line">        &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(String value, ReadOnlyContext ctx,</span></span><br><span class="line"><span class="params">                                   Collector&lt;String&gt; out)</span> &#123;</span><br><span class="line">            <span class="comment">//获取上述更新后的最新值</span></span><br><span class="line">            <span class="type">TableSchema</span> <span class="variable">meta</span> <span class="operator">=</span> ctx.getBroadcastState(mapStateDescriptor).get(<span class="string">&quot;id&quot;</span>);</span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">            	具体处理逻辑...</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="spark-streaming-broadcast">spark streaming broadcast</h3>
<p>我们知道spark的广播变量允许换成一个只读的变量在每台机器上面，而不是每个任务保存一份。常见于spark在一些全局统计的场景中应用。通过广播变量，能够以一种更有效率的方式将一个大数据量输入集合的副本分配给每个节点。Spark也尝试着利用有效的广播算法去分配广播变量，以减少通信的成本</p>
<p>一个广播变量可以通过调用sparkContext.broadcast(v)方法从一个初始变量v中创建。广播变量是v的一个包装变量，它的值可以通过value方法访问，例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span>&#123;</span><br><span class="line">    <span class="type">JavaStreamingContext</span> <span class="variable">sc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaStreamingContext</span>(conf);</span><br><span class="line">    JavaPairInputDStream&lt;String,String&gt; kafka = KafkaUtils.createStream(...);</span><br><span class="line">    kafka.repartition(<span class="number">30</span>)</span><br><span class="line">        .transform(...)</span><br><span class="line">        .foreachRDD(<span class="keyword">new</span> <span class="title class_">VoidFunction</span>&lt;JavaRDD&lt;String&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">call</span><span class="params">(JavaRDD&lt;String&gt; rdd)</span> &#123;</span><br><span class="line">                <span class="comment">// 广播变量的注册一定要放在这里，否则不会广播到各个节点的task，这种方式可以做到自动更新</span></span><br><span class="line">                <span class="keyword">final</span> Broadcast&lt;String&gt; cast = JavaSparkContext</span><br><span class="line">                    	.fromSparkContext(rdd.context())</span><br><span class="line">                        .broadcast(<span class="string">&quot;broadcast value&quot;</span>);</span><br><span class="line">                rdd.foreach(<span class="keyword">new</span> <span class="title class_">VoidFunction</span>&lt;String&gt;() &#123;</span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">call</span><span class="params">(String v)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        System.out.println(cast.value());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">	sc.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据组件</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>hive CLI和Beeline命令行的基本使用</title>
    <url>/2021/10/15/CLI%E5%92%8CBeeline%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="一、Hive-CLI">一、Hive CLI</h2>
<h3 id="1-1-Help">1.1 Help</h3>
<p>使用 <code>hive -H</code> 或者 <code>hive --help</code> 命令可以查看所有命令的帮助，显示如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">usage: hive</span><br><span class="line"> -d,--define &lt;key=value&gt;          Variable subsitution to apply to hive </span><br><span class="line">                                  commands. e.g. -d A=B or --define A=B  --定义用户自定义变量</span><br><span class="line">    --database &lt;databasename&gt;     Specify the database to use  -- 指定使用的数据库</span><br><span class="line"> -e &lt;quoted-query-string&gt;         SQL from command line   -- 执行指定的 SQL</span><br><span class="line"> -f &lt;filename&gt;                    SQL from files   --执行 SQL 脚本</span><br><span class="line"> -H,--help                        Print help information  -- 打印帮助信息</span><br><span class="line">    --hiveconf &lt;property=value&gt;   Use value for given property    --自定义配置</span><br><span class="line">    --hivevar &lt;key=value&gt;         Variable subsitution to apply to hive  --自定义变量</span><br><span class="line">                                  commands. e.g. --hivevar A=B</span><br><span class="line"> -i &lt;filename&gt;                    Initialization SQL file  --在进入交互模式之前运行初始化脚本</span><br><span class="line"> -S,--silent                      Silent mode in interactive shell    --静默模式</span><br><span class="line"> -v,--verbose                     Verbose mode (echo executed SQL to the  console)  --详细模式</span><br></pre></td></tr></table></figure>
<h3 id="1-2-交互式命令行">1.2 交互式命令行</h3>
<p>直接使用 <code>Hive</code> 命令，不加任何参数，即可进入交互式命令行。</p>
<h3 id="1-3-执行SQL命令">1.3 执行SQL命令</h3>
<p>在不进入交互式命令行的情况下，可以使用 <code>hive -e </code> 执行 SQL 命令。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive <span class="operator">-</span>e <span class="string">&#x27;select * from emp&#x27;</span>;</span><br></pre></td></tr></table></figure>
<div align="center"> <img width='700px' src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-e.png"/> </div>
<p>hexo</p>
<h3 id="1-4-执行SQL脚本">1.4 执行SQL脚本</h3>
<p>用于执行的 sql 脚本可以在本地文件系统，也可以在 HDFS 上。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">本地文件系统</span></span><br><span class="line">hive -f /usr/file/simple.sql;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">HDFS文件系统</span></span><br><span class="line">hive -f hdfs://hadoop001:8020/tmp/simple.sql;</span><br></pre></td></tr></table></figure>
<p>其中 <code>simple.sql</code> 内容如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>
<h3 id="1-5-配置Hive变量">1.5 配置Hive变量</h3>
<p>可以使用 <code>--hiveconf</code> 设置 Hive 运行时的变量。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive <span class="operator">-</span>e <span class="string">&#x27;select * from emp&#x27;</span> \</span><br><span class="line"><span class="comment">--hiveconf hive.exec.scratchdir=/tmp/hive_scratch  \</span></span><br><span class="line"><span class="comment">--hiveconf mapred.reduce.tasks=4;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>hive.exec.scratchdir：指定 HDFS 上目录位置，用于存储不同 map/reduce 阶段的执行计划和这些阶段的中间输出结果。</p>
</blockquote>
<h3 id="1-6-配置文件启动">1.6 配置文件启动</h3>
<p>使用 <code>-i</code> 可以在进入交互模式之前运行初始化脚本，相当于指定配置文件启动。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive -i /usr/file/hive-init.conf;</span><br></pre></td></tr></table></figure>
<p>其中 <code>hive-init.conf</code> 的内容如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>hive.exec.mode.local.auto 默认值为 false，这里设置为 true ，代表开启本地模式。</p>
</blockquote>
<h3 id="1-7-用户自定义变量">1.7 用户自定义变量</h3>
<p><code>--define &lt;key=value&gt; </code> 和 <code>--hivevar &lt;key=value&gt;  </code> 在功能上是等价的，都是用来实现自定义变量，这里给出一个示例:</p>
<p>定义变量：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive  <span class="comment">--define  n=ename --hiveconf  --hivevar j=job;</span></span><br></pre></td></tr></table></figure>
<p>在查询中引用自定义变量：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 以下两条语句等价</span><br><span class="line">hive <span class="operator">&gt;</span> <span class="keyword">select</span> $&#123;n&#125; <span class="keyword">from</span> emp;</span><br><span class="line">hive <span class="operator">&gt;</span>  <span class="keyword">select</span> $&#123;hivevar:n&#125; <span class="keyword">from</span> emp;</span><br><span class="line"></span><br><span class="line"># 以下两条语句等价</span><br><span class="line">hive <span class="operator">&gt;</span> <span class="keyword">select</span> $&#123;j&#125; <span class="keyword">from</span> emp;</span><br><span class="line">hive <span class="operator">&gt;</span>  <span class="keyword">select</span> $&#123;hivevar:j&#125; <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<div align="center"> <img width='700px' src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-n-j.png"/> </div>
<h2 id="二、Beeline">二、Beeline</h2>
<h3 id="2-1-HiveServer2">2.1 HiveServer2</h3>
<p>Hive 内置了 HiveServer 和 HiveServer2 服务，两者都允许客户端使用多种编程语言进行连接，但是 HiveServer 不能处理多个客户端的并发请求，所以产生了 HiveServer2。</p>
<p>HiveServer2（HS2）允许远程客户端可以使用各种编程语言向 Hive 提交请求并检索结果，支持多客户端并发访问和身份验证。HS2 是由多个服务组成的单个进程，其包括基于 Thrift 的 Hive 服务（TCP 或 HTTP）和用于 Web UI 的 Jetty Web 服务器。</p>
<p>HiveServer2 拥有自己的 CLI(Beeline)，Beeline 是一个基于 SQLLine 的 JDBC 客户端。由于 HiveServer2 是 Hive 开发维护的重点 (Hive0.15 后就不再支持 hiveserver)，所以 Hive CLI 已经不推荐使用了，官方更加推荐使用 Beeline。</p>
<h3 id="2-1-Beeline">2.1 Beeline</h3>
<p>Beeline 拥有更多可使用参数，可以使用 <code>beeline --help</code> 查看，完整参数如下：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">Usage</span>: <span class="string">java org.apache.hive.cli.beeline.BeeLine</span></span><br><span class="line">   <span class="attr">-u</span> <span class="string">&lt;database url&gt;               the JDBC URL to connect to</span></span><br><span class="line">   <span class="attr">-r</span>                              <span class="string">reconnect to last saved connect url (in conjunction with !save)</span></span><br><span class="line">   <span class="attr">-n</span> <span class="string">&lt;username&gt;                   the username to connect as</span></span><br><span class="line">   <span class="attr">-p</span> <span class="string">&lt;password&gt;                   the password to connect as</span></span><br><span class="line">   <span class="attr">-d</span> <span class="string">&lt;driver class&gt;               the driver class to use</span></span><br><span class="line">   <span class="attr">-i</span> <span class="string">&lt;init file&gt;                  script file for initialization</span></span><br><span class="line">   <span class="attr">-e</span> <span class="string">&lt;query&gt;                      query that should be executed</span></span><br><span class="line">   <span class="attr">-f</span> <span class="string">&lt;exec file&gt;                  script file that should be executed</span></span><br><span class="line">   <span class="attr">-w</span> <span class="string">(or) --password-file &lt;password file&gt;  the password file to read password from</span></span><br><span class="line">   <span class="attr">--hiveconf</span> <span class="string">property=value       Use value for given property</span></span><br><span class="line">   <span class="attr">--hivevar</span> <span class="string">name=value            hive variable name and value</span></span><br><span class="line">                                   <span class="attr">This</span> <span class="string">is Hive specific settings in which variables</span></span><br><span class="line">                                   <span class="attr">can</span> <span class="string">be set at session level and referenced in Hive</span></span><br><span class="line">                                   <span class="attr">commands</span> <span class="string">or queries.</span></span><br><span class="line">   <span class="attr">--property-file</span>=<span class="string">&lt;property-file&gt; the file to read connection properties (url, driver, user, password) from</span></span><br><span class="line">   <span class="attr">--color</span>=<span class="string">[true/false]            control whether color is used for display</span></span><br><span class="line">   <span class="attr">--showHeader</span>=<span class="string">[true/false]       show column names in query results</span></span><br><span class="line">   <span class="attr">--headerInterval</span>=<span class="string">ROWS;          the interval between which heades are displayed</span></span><br><span class="line">   <span class="attr">--fastConnect</span>=<span class="string">[true/false]      skip building table/column list for tab-completion</span></span><br><span class="line">   <span class="attr">--autoCommit</span>=<span class="string">[true/false]       enable/disable automatic transaction commit</span></span><br><span class="line">   <span class="attr">--verbose</span>=<span class="string">[true/false]          show verbose error messages and debug info</span></span><br><span class="line">   <span class="attr">--showWarnings</span>=<span class="string">[true/false]     display connection warnings</span></span><br><span class="line">   <span class="attr">--showNestedErrs</span>=<span class="string">[true/false]   display nested errors</span></span><br><span class="line">   <span class="attr">--numberFormat</span>=<span class="string">[pattern]        format numbers using DecimalFormat pattern</span></span><br><span class="line">   <span class="attr">--force</span>=<span class="string">[true/false]            continue running script even after errors</span></span><br><span class="line">   <span class="attr">--maxWidth</span>=<span class="string">MAXWIDTH             the maximum width of the terminal</span></span><br><span class="line">   <span class="attr">--maxColumnWidth</span>=<span class="string">MAXCOLWIDTH    the maximum width to use when displaying columns</span></span><br><span class="line">   <span class="attr">--silent</span>=<span class="string">[true/false]           be more silent</span></span><br><span class="line">   <span class="attr">--autosave</span>=<span class="string">[true/false]         automatically save preferences</span></span><br><span class="line">   <span class="attr">--outputformat</span>=<span class="string">[table/vertical/csv2/tsv2/dsv/csv/tsv]  format mode for result display</span></span><br><span class="line">   <span class="attr">--incrementalBufferRows</span>=<span class="string">NUMROWS the number of rows to buffer when printing rows on stdout,</span></span><br><span class="line">                                   <span class="attr">defaults</span> <span class="string">to 1000; only applicable if --incremental=true</span></span><br><span class="line">                                   <span class="attr">and</span> <span class="string">--outputformat=table</span></span><br><span class="line">   <span class="attr">--truncateTable</span>=<span class="string">[true/false]    truncate table column when it exceeds length</span></span><br><span class="line">   <span class="attr">--delimiterForDSV</span>=<span class="string">DELIMITER     specify the delimiter for delimiter-separated values output format (default: |)</span></span><br><span class="line">   <span class="attr">--isolation</span>=<span class="string">LEVEL               set the transaction isolation level</span></span><br><span class="line">   <span class="attr">--nullemptystring</span>=<span class="string">[true/false]  set to true to get historic behavior of printing null as empty string</span></span><br><span class="line">   <span class="attr">--maxHistoryRows</span>=<span class="string">MAXHISTORYROWS The maximum number of rows to store beeline history.</span></span><br><span class="line">   <span class="attr">--convertBinaryArrayToString</span>=<span class="string">[true/false]    display binary column data as string or as byte array</span></span><br><span class="line">   <span class="attr">--help</span>                          <span class="string">display this message</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-3-常用参数">2.3 常用参数</h3>
<p>在 Hive CLI 中支持的参数，Beeline 都支持，常用的参数如下。更多参数说明可以参见官方文档 <a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline%E2%80%93NewCommandLineShell">Beeline Command Options</a></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>-u &lt;database URL&gt;</strong></td>
<td>数据库地址</td>
</tr>
<tr>
<td><strong>-n &lt;username&gt;</strong></td>
<td>用户名</td>
</tr>
<tr>
<td><strong>-p &lt;password&gt;</strong></td>
<td>密码</td>
</tr>
<tr>
<td><strong>-d &lt;driver class&gt;</strong></td>
<td>驱动 (可选)</td>
</tr>
<tr>
<td><strong>-e &lt;query&gt;</strong></td>
<td>执行 SQL 命令</td>
</tr>
<tr>
<td><strong>-f &lt;file&gt;</strong></td>
<td>执行 SQL 脚本</td>
</tr>
<tr>
<td><strong>-i  (or)–init  &lt;file or files&gt;</strong></td>
<td>在进入交互模式之前运行初始化脚本</td>
</tr>
<tr>
<td><strong>–property-file &lt;file&gt;</strong></td>
<td>指定配置文件</td>
</tr>
<tr>
<td><strong>–hiveconf</strong> <em>property</em>*=*<em>value</em></td>
<td>指定配置属性</td>
</tr>
<tr>
<td><strong>–hivevar</strong> <em>name</em>*=*<em>value</em></td>
<td>用户自定义属性，在会话级别有效</td>
</tr>
</tbody>
</table>
<p>示例： 使用用户名和密码连接 Hive</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">beeline -u jdbc:hive2://localhost:10000  -n username -p password</span> </span><br></pre></td></tr></table></figure>
<p>​</p>
<h2 id="三、Hive配置">三、Hive配置</h2>
<p>可以通过三种方式对 Hive 的相关属性进行配置，分别介绍如下：</p>
<h3 id="3-1-配置文件">3.1 配置文件</h3>
<p>方式一为使用配置文件，使用配置文件指定的配置是永久有效的。Hive 有以下三个可选的配置文件：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>hive-site.xml ：Hive 的主要配置文件；</p>
</li>
<li class="lvl-2">
<p>hivemetastore-site.xml： 关于元数据的配置；</p>
</li>
<li class="lvl-2">
<p>hiveserver2-site.xml：关于 HiveServer2 的配置。</p>
</li>
</ul>
<p>示例如下,在 hive-site.xml 配置 <code>hive.exec.scratchdir</code>：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp/mydir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>Scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="3-2-hiveconf">3.2 hiveconf</h3>
<p>方式二为在启动命令行 (Hive CLI / Beeline) 的时候使用 <code>--hiveconf</code> 指定配置，这种方式指定的配置作用于整个 Session。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive --hiveconf hive.exec.scratchdir=/tmp/mydir</span><br></pre></td></tr></table></figure>
<h3 id="3-3-set">3.3 set</h3>
<p>方式三为在交互式环境下 (Hive CLI / Beeline)，使用 set 命令指定。这种设置的作用范围也是 Session 级别的，配置对于执行该命令后的所有命令生效。set 兼具设置参数和查看参数的功能。如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; set hive.exec.scratchdir=/tmp/mydir;</span><br><span class="line">No rows affected (0.025 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; set hive.exec.scratchdir;</span><br><span class="line">+----------------------------------+--+</span><br><span class="line">|               set                |</span><br><span class="line">+----------------------------------+--+</span><br><span class="line">| hive.exec.scratchdir=/tmp/mydir  |</span><br><span class="line">+----------------------------------+--+</span><br></pre></td></tr></table></figure>
<h3 id="3-4-配置优先级">3.4 配置优先级</h3>
<p>配置的优先顺序如下 (由低到高)：<br>
<code>hive-site.xml</code> - &gt;<code>hivemetastore-site.xml</code>- &gt; <code>hiveserver2-site.xml</code> - &gt;<code> -- hiveconf</code>- &gt; <code>set</code></p>
<h3 id="3-5-配置参数">3.5 配置参数</h3>
<p>Hive 可选的配置参数非常多，在用到时查阅官方文档即可<a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration">AdminManual Configuration</a></p>
<h2 id="参考资料-2">参考资料</h2>
<ol>
<li class="lvl-3">
<p><a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients">HiveServer2 Clients</a></p>
</li>
<li class="lvl-3">
<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli">LanguageManual Cli</a></p>
</li>
<li class="lvl-3">
<p><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration">AdminManual Configuration</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink Data Source</title>
    <url>/2022/03/19/Flink-Data-Source/</url>
    <content><![CDATA[<h2 id="一、内置-Data-Source">一、内置 Data Source</h2>
<p>Flink Data Source 用于定义 Flink 程序的数据来源，Flink 官方提供了多种数据获取方法，用于帮助开发者简单快速地构建输入流，具体如下：</p>
<h3 id="1-1-基于文件构建">1.1 基于文件构建</h3>
<p><strong>1. readTextFile(path)</strong>：按照 TextInputFormat 格式读取文本文件，并将其内容以字符串的形式返回。示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.readTextFile(filePath).print();</span><br></pre></td></tr></table></figure>
<p><strong>2. readFile(fileInputFormat, path)</strong> ：按照指定格式读取文件。</p>
<p><strong>3. readFile(inputFormat, filePath, watchType, interval, typeInformation)</strong>：按照指定格式周期性的读取文件。其中各个参数的含义如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>inputFormat</strong>：数据流的输入格式。</p>
</li>
<li class="lvl-2">
<p><strong>filePath</strong>：文件路径，可以是本地文件系统上的路径，也可以是 HDFS 上的文件路径。</p>
</li>
<li class="lvl-2">
<p><strong>watchType</strong>：读取方式，它有两个可选值，分别是 <code>FileProcessingMode.PROCESS_ONCE</code> 和 <code>FileProcessingMode.PROCESS_CONTINUOUSLY</code>：前者表示对指定路径上的数据只读取一次，然后退出；后者表示对路径进行定期地扫描和读取。需要注意的是如果 watchType 被设置为 <code>PROCESS_CONTINUOUSLY</code>，那么当文件被修改时，其所有的内容 (包含原有的内容和新增的内容) 都将被重新处理，因此这会打破 Flink 的 <em>exactly-once</em> 语义。</p>
</li>
<li class="lvl-2">
<p><strong>interval</strong>：定期扫描的时间间隔。</p>
</li>
<li class="lvl-2">
<p><strong>typeInformation</strong>：输入流中元素的类型。</p>
</li>
</ul>
<p>使用示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="type">String</span> <span class="variable">filePath</span> <span class="operator">=</span> <span class="string">&quot;D:\\log4j.properties&quot;</span>;</span><br><span class="line"><span class="keyword">final</span> <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.readFile(<span class="keyword">new</span> <span class="title class_">TextInputFormat</span>(<span class="keyword">new</span> <span class="title class_">Path</span>(filePath)),</span><br><span class="line">             filePath,</span><br><span class="line">             FileProcessingMode.PROCESS_ONCE,</span><br><span class="line">             <span class="number">1</span>,</span><br><span class="line">             BasicTypeInfo.STRING_TYPE_INFO).print();</span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>
<h3 id="1-2-基于集合构建">1.2 基于集合构建</h3>
<p><strong>1. fromCollection(Collection)</strong>：基于集合构建，集合中的所有元素必须是同一类型。示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.fromCollection(Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)).print();</span><br></pre></td></tr></table></figure>
<p><strong>2. fromElements(T …)</strong>： 基于元素构建，所有元素必须是同一类型。示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.fromElements(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).print();</span><br></pre></td></tr></table></figure>
<p><strong>3. generateSequence(from, to)</strong>：基于给定的序列区间进行构建。示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.generateSequence(<span class="number">0</span>,<span class="number">100</span>);</span><br></pre></td></tr></table></figure>
<p><strong>4. fromCollection(Iterator, Class)</strong>：基于迭代器进行构建。第一个参数用于定义迭代器，第二个参数用于定义输出元素的类型。使用示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.fromCollection(<span class="keyword">new</span> <span class="title class_">CustomIterator</span>(), BasicTypeInfo.INT_TYPE_INFO).print();</span><br></pre></td></tr></table></figure>
<p>其中 CustomIterator 为自定义的迭代器，这里以产生 1 到 100 区间内的数据为例，源码如下。需要注意的是自定义迭代器除了要实现 Iterator 接口外，还必须要实现序列化接口 Serializable ，否则会抛出序列化失败的异常：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomIterator</span> <span class="keyword">implements</span> <span class="title class_">Iterator</span>&lt;Integer&gt;, Serializable &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Integer</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">hasNext</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> i &lt; <span class="number">100</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Integer <span class="title function_">next</span><span class="params">()</span> &#123;</span><br><span class="line">        i++;</span><br><span class="line">        <span class="keyword">return</span> i;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>5. fromParallelCollection(SplittableIterator, Class)</strong>：方法接收两个参数，第二个参数用于定义输出元素的类型，第一个参数 SplittableIterator 是迭代器的抽象基类，它用于将原始迭代器的值拆分到多个不相交的迭代器中。</p>
<h3 id="1-3-基于-Socket-构建">1.3  基于 Socket 构建</h3>
<p>Flink 提供了 socketTextStream 方法用于构建基于 Socket 的数据流，socketTextStream 方法有以下四个主要参数：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>hostname</strong>：主机名；</p>
</li>
<li class="lvl-2">
<p><strong>port</strong>：端口号，设置为 0 时，表示端口号自动分配；</p>
</li>
<li class="lvl-2">
<p><strong>delimiter</strong>：用于分隔每条记录的分隔符；</p>
</li>
<li class="lvl-2">
<p><strong>maxRetry</strong>：当 Socket 临时关闭时，程序的最大重试间隔，单位为秒。设置为 0 时表示不进行重试；设置为负值则表示一直重试。示例如下：</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">env.socketTextStream(&quot;192.168.0.229&quot;, 9999, &quot;\n&quot;, 3).print();</span><br></pre></td></tr></table></figure>
<h2 id="二、自定义-Data-Source">二、自定义 Data Source</h2>
<h3 id="2-1-SourceFunction">2.1 SourceFunction</h3>
<p>除了内置的数据源外，用户还可以使用 <code>addSource</code> 方法来添加自定义的数据源。自定义的数据源必须要实现 SourceFunction 接口，这里以产生 [0 , 1000) 区间内的数据为例，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.addSource(<span class="keyword">new</span> <span class="title class_">SourceFunction</span>&lt;Long&gt;() &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0L</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="type">boolean</span> <span class="variable">isRunning</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(SourceContext&lt;Long&gt; ctx)</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (isRunning &amp;&amp; count &lt; <span class="number">1000</span>) &#123;</span><br><span class="line">            <span class="comment">// 通过collect将输入发送出去 </span></span><br><span class="line">            ctx.collect(count);</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">cancel</span><span class="params">()</span> &#123;</span><br><span class="line">        isRunning = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;).print();</span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>
<h3 id="2-2-ParallelSourceFunction-和-RichParallelSourceFunction">2.2 ParallelSourceFunction 和 RichParallelSourceFunction</h3>
<p>上面通过 SourceFunction 实现的数据源是不具有并行度的，即不支持在得到的 DataStream 上调用 <code>setParallelism(n)</code> 方法，此时会抛出如下的异常：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Source: 1 is not a parallel source</span><br></pre></td></tr></table></figure>
<p>如果你想要实现具有并行度的输入流，则需要实现 ParallelSourceFunction 或 RichParallelSourceFunction 接口，其与 SourceFunction 的关系如下图：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-RichParallelSourceFunction.png"/> </div>
ParallelSourceFunction 直接继承自 ParallelSourceFunction，具有并行度的功能。RichParallelSourceFunction 则继承自 AbstractRichFunction，同时实现了 ParallelSourceFunction 接口，所以其除了具有并行度的功能外，还提供了额外的与生命周期相关的方法，如 open() ，closen() 。
<h2 id="三、Streaming-Connectors">三、Streaming Connectors</h2>
<h3 id="3-1-内置连接器">3.1 内置连接器</h3>
<p>除了自定义数据源外， Flink 还内置了多种连接器，用于满足大多数的数据收集场景。当前内置连接器的支持情况如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>Apache Kafka (支持 source 和 sink)</p>
</li>
<li class="lvl-2">
<p>Apache Cassandra (sink)</p>
</li>
<li class="lvl-2">
<p>Amazon Kinesis Streams (source/sink)</p>
</li>
<li class="lvl-2">
<p>Elasticsearch (sink)</p>
</li>
<li class="lvl-2">
<p>Hadoop FileSystem (sink)</p>
</li>
<li class="lvl-2">
<p>RabbitMQ (source/sink)</p>
</li>
<li class="lvl-2">
<p>Apache NiFi (source/sink)</p>
</li>
<li class="lvl-2">
<p>Twitter Streaming API (source)</p>
</li>
<li class="lvl-2">
<p>Google PubSub (source/sink)</p>
</li>
</ul>
<p>除了上述的连接器外，你还可以通过 Apache Bahir 的连接器扩展 Flink。Apache Bahir 旨在为分布式数据分析系统 (如 Spark，Flink) 等提供功能上的扩展，当前其支持的与 Flink 相关的连接器如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>Apache ActiveMQ (source/sink)</p>
</li>
<li class="lvl-2">
<p>Apache Flume (sink)</p>
</li>
<li class="lvl-2">
<p>Redis (sink)</p>
</li>
<li class="lvl-2">
<p>Akka (sink)</p>
</li>
<li class="lvl-2">
<p>Netty (source)</p>
</li>
</ul>
<p>随着 Flink 的不断发展，可以预见到其会支持越来越多类型的连接器，关于连接器的后续发展情况，可以查看其官方文档：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html">Streaming Connectors</a> 。在所有 DataSource 连接器中，使用的广泛的就是 Kafka，所以这里我们以其为例，来介绍 Connectors 的整合步骤。</p>
<h3 id="3-2-整合-Kakfa">3.2 整合 Kakfa</h3>
<h4 id="1-导入依赖">1. 导入依赖</h4>
<p>整合 Kafka 时，一定要注意所使用的 Kafka 的版本，不同版本间所需的 Maven 依赖和开发时所调用的类均不相同，具体如下：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Maven 依赖</th>
<th style="text-align:left">Flink 版本</th>
<th style="text-align:left">Consumer and Producer 类的名称</th>
<th style="text-align:left">Kafka 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">flink-connector-kafka-0.8_2.11</td>
<td style="text-align:left">1.0.0 +</td>
<td style="text-align:left">FlinkKafkaConsumer08 <br/>FlinkKafkaProducer08</td>
<td style="text-align:left">0.8.x</td>
</tr>
<tr>
<td style="text-align:left">flink-connector-kafka-0.9_2.11</td>
<td style="text-align:left">1.0.0 +</td>
<td style="text-align:left">FlinkKafkaConsumer09<br/> FlinkKafkaProducer09</td>
<td style="text-align:left">0.9.x</td>
</tr>
<tr>
<td style="text-align:left">flink-connector-kafka-0.10_2.11</td>
<td style="text-align:left">1.2.0 +</td>
<td style="text-align:left">FlinkKafkaConsumer010 <br/>FlinkKafkaProducer010</td>
<td style="text-align:left">0.10.x</td>
</tr>
<tr>
<td style="text-align:left">flink-connector-kafka-0.11_2.11</td>
<td style="text-align:left">1.4.0 +</td>
<td style="text-align:left">FlinkKafkaConsumer011 <br/>FlinkKafkaProducer011</td>
<td style="text-align:left">0.11.x</td>
</tr>
<tr>
<td style="text-align:left">flink-connector-kafka_2.11</td>
<td style="text-align:left">1.7.0 +</td>
<td style="text-align:left">FlinkKafkaConsumer <br/>FlinkKafkaProducer</td>
<td style="text-align:left">&gt;= 1.0.0</td>
</tr>
</tbody>
</table>
<p>这里我使用的 Kafka 版本为 kafka_2.12-2.2.0，添加的依赖如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="2-代码开发">2. 代码开发</h4>
<p>这里以最简单的场景为例，接收 Kafka 上的数据并打印，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"><span class="comment">// 指定Kafka的连接位置</span></span><br><span class="line">properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop001:9092&quot;</span>);</span><br><span class="line"><span class="comment">// 指定监听的主题，并定义Kafka字节消息到Flink对象之间的转换规则</span></span><br><span class="line">DataStream&lt;String&gt; stream = env</span><br><span class="line">    .addSource(<span class="keyword">new</span> <span class="title class_">FlinkKafkaConsumer</span>&lt;&gt;(<span class="string">&quot;flink-stream-in-topic&quot;</span>, <span class="keyword">new</span> <span class="title class_">SimpleStringSchema</span>(), properties));</span><br><span class="line">stream.print();</span><br><span class="line">env.execute(<span class="string">&quot;Flink Streaming&quot;</span>);</span><br></pre></td></tr></table></figure>
<h3 id="3-3-整合测试">3.3 整合测试</h3>
<h4 id="1-启动-Kakfa">1. 启动 Kakfa</h4>
<p>Kafka 的运行依赖于 zookeeper，需要预先启动，可以启动 Kafka 内置的 zookeeper，也可以启动自己安装的：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">zookeeper启动命令</span></span><br><span class="line">bin/zkServer.sh start</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">内置zookeeper启动命令</span></span><br><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties</span><br></pre></td></tr></table></figure>
<p>启动单节点 kafka 用于测试：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">bin/kafka-server-start.sh config/server.properties</span></span><br></pre></td></tr></table></figure>
<h4 id="2-创建-Topic">2. 创建 Topic</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">创建用于测试主题</span></span><br><span class="line">bin/kafka-topics.sh --create \</span><br><span class="line">                    --bootstrap-server hadoop001:9092 \</span><br><span class="line">                    --replication-factor 1 \</span><br><span class="line">                    --partitions 1  \</span><br><span class="line">                    --topic flink-stream-in-topic</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看所有主题</span></span><br><span class="line"> bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092</span><br></pre></td></tr></table></figure>
<h4 id="3-启动-Producer">3. 启动 Producer</h4>
<p>这里 启动一个 Kafka 生产者，用于发送测试数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list hadoop001:9092 --topic flink-stream-in-topic</span><br></pre></td></tr></table></figure>
<h4 id="4-测试结果">4. 测试结果</h4>
<p>在 Producer 上输入任意测试数据，之后观察程序控制台的输出：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-kafka-datasource-producer.png"/> </div>
程序控制台的输出如下：
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-kafka-datasource-console.png"/> </div>
可以看到已经成功接收并打印出相关的数据。
<h2 id="参考资料-3">参考资料</h2>
<ol>
<li class="lvl-3">
<p>data-sources：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#data-sources">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#data-sources</a></p>
</li>
<li class="lvl-3">
<p>Streaming Connectors：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html</a></p>
</li>
<li class="lvl-3">
<p>Apache Kafka Connector： <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink_Data_Transformation</title>
    <url>/2022/03/19/Flink-Data-Transformation/</url>
    <content><![CDATA[<h2 id="一、Transformations-分类">一、Transformations 分类</h2>
<p>Flink 的 Transformations 操作主要用于将一个和多个 DataStream 按需转换成新的 DataStream。它主要分为以下三类：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>DataStream Transformations</strong>：进行数据流相关转换操作；</p>
</li>
<li class="lvl-2">
<p><strong>Physical partitioning</strong>：物理分区。Flink 提供的底层 API ，允许用户定义数据的分区规则；</p>
</li>
<li class="lvl-2">
<p><strong>Task chaining and resource groups</strong>：任务链和资源组。允许用户进行任务链和资源组的细粒度的控制。</p>
</li>
</ul>
<p>以下分别对其主要 API 进行介绍：</p>
<h2 id="二、DataStream-Transformations">二、DataStream Transformations</h2>
<h3 id="2-1-Map-DataStream-→-DataStream">2.1 Map [DataStream → DataStream]</h3>
<p>对一个 DataStream 中的每个元素都执行特定的转换操作：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Integer&gt; integerDataStream = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">integerDataStream.map((MapFunction&lt;Integer, Object&gt;) value -&gt; value * <span class="number">2</span>).print();</span><br><span class="line"><span class="comment">// 输出 2,4,6,8,10</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-FlatMap-DataStream-→-DataStream">2.2 FlatMap [DataStream → DataStream]</h3>
<p>FlatMap 与 Map 类似，但是 FlatMap 中的一个输入元素可以被映射成一个或者多个输出元素，示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">string01</span> <span class="operator">=</span> <span class="string">&quot;one one one two two&quot;</span>;</span><br><span class="line"><span class="type">String</span> <span class="variable">string02</span> <span class="operator">=</span> <span class="string">&quot;third third third four&quot;</span>;</span><br><span class="line">DataStream&lt;String&gt; stringDataStream = env.fromElements(string01, string02);</span><br><span class="line">stringDataStream.flatMap(<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap</span><span class="params">(String value, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">for</span> (String s : value.split(<span class="string">&quot; &quot;</span>)) &#123;</span><br><span class="line">            out.collect(s);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();</span><br><span class="line"><span class="comment">// 输出每一个独立的单词，为节省排版，这里去掉换行，后文亦同</span></span><br><span class="line">one one one two two third third third four</span><br></pre></td></tr></table></figure>
<h3 id="2-3-Filter-DataStream-→-DataStream">2.3 Filter [DataStream → DataStream]</h3>
<p>用于过滤符合条件的数据：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>).filter(x -&gt; x &gt; <span class="number">3</span>).print();</span><br></pre></td></tr></table></figure>
<h3 id="2-4-KeyBy-和-Reduce">2.4 KeyBy 和 Reduce</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>KeyBy [DataStream → KeyedStream]</strong> ：用于将相同 Key 值的数据分到相同的分区中；</p>
</li>
<li class="lvl-2">
<p><strong>Reduce [KeyedStream → DataStream]</strong> ：用于对数据执行归约计算。</p>
</li>
</ul>
<p>如下例子将数据按照 key 值分区后，滚动进行求和计算：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2DataStream = env.fromElements(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                                                                        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), </span><br><span class="line">                                                                        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), </span><br><span class="line">                                                                        <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">5</span>));</span><br><span class="line">KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = tuple2DataStream.keyBy(<span class="number">0</span>);</span><br><span class="line">keyedStream.reduce((ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;) (value1, value2) -&gt;</span><br><span class="line">                   <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(value1.f0, value1.f1 + value2.f1)).print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 持续进行求和计算，输出：</span></span><br><span class="line">(a,<span class="number">1</span>)</span><br><span class="line">(a,<span class="number">3</span>)</span><br><span class="line">(b,<span class="number">3</span>)</span><br><span class="line">(b,<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<p>KeyBy 操作存在以下两个限制：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>KeyBy 操作用于用户自定义的 POJOs 类型时，该自定义类型必须重写 hashCode 方法；</p>
</li>
<li class="lvl-2">
<p>KeyBy 操作不能用于数组类型。</p>
</li>
</ul>
<h3 id="2-5-Aggregations-KeyedStream-→-DataStream">2.5 Aggregations [KeyedStream → DataStream]</h3>
<p>Aggregations 是官方提供的聚合算子，封装了常用的聚合操作，如上利用 Reduce 进行求和的操作也可以利用 Aggregations 中的 sum 算子重写为下面的形式：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">tuple2DataStream.keyBy(<span class="number">0</span>).sum(<span class="number">1</span>).print();</span><br></pre></td></tr></table></figure>
<p>除了 sum 外，Flink 还提供了 min , max , minBy，maxBy 等常用聚合算子：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 滚动计算指定key的最小值，可以通过index或者fieldName来指定key</span></span><br><span class="line">keyedStream.min(<span class="number">0</span>);</span><br><span class="line">keyedStream.min(<span class="string">&quot;key&quot;</span>);</span><br><span class="line"><span class="comment">// 滚动计算指定key的最大值</span></span><br><span class="line">keyedStream.max(<span class="number">0</span>);</span><br><span class="line">keyedStream.max(<span class="string">&quot;key&quot;</span>);</span><br><span class="line"><span class="comment">// 滚动计算指定key的最小值，并返回其对应的元素</span></span><br><span class="line">keyedStream.minBy(<span class="number">0</span>);</span><br><span class="line">keyedStream.minBy(<span class="string">&quot;key&quot;</span>);</span><br><span class="line"><span class="comment">// 滚动计算指定key的最大值，并返回其对应的元素</span></span><br><span class="line">keyedStream.maxBy(<span class="number">0</span>);</span><br><span class="line">keyedStream.maxBy(<span class="string">&quot;key&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-6-Union-DataStream-→-DataStream">2.6 Union [DataStream* → DataStream]</h3>
<p>用于连接两个或者多个元素类型相同的 DataStream 。当然一个 DataStream 也可以与其本生进行连接，此时该 DataStream 中的每个元素都会被获取两次：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource01 = env.fromElements(new Tuple2&lt;&gt;(&quot;a&quot;, 1), </span><br><span class="line">                                                                            new Tuple2&lt;&gt;(&quot;a&quot;, 2));</span><br><span class="line">DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource02 = env.fromElements(new Tuple2&lt;&gt;(&quot;b&quot;, 1), </span><br><span class="line">                                                                            new Tuple2&lt;&gt;(&quot;b&quot;, 2));</span><br><span class="line">streamSource01.union(streamSource02);</span><br><span class="line">streamSource01.union(streamSource01,streamSource02);</span><br></pre></td></tr></table></figure>
<h3 id="2-7-Connect-DataStream-DataStream-→-ConnectedStreams">2.7 Connect [DataStream,DataStream → ConnectedStreams]</h3>
<p>Connect 操作用于连接两个或者多个类型不同的 DataStream ，其返回的类型是 ConnectedStreams ，此时被连接的多个 DataStreams 可以共享彼此之间的数据状态。但是需要注意的是由于不同 DataStream 之间的数据类型是不同的，如果想要进行后续的计算操作，还需要通过 CoMap 或 CoFlatMap 将 ConnectedStreams  转换回 DataStream：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource01 = env.fromElements(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), </span><br><span class="line">                                                                            <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;b&quot;</span>, <span class="number">5</span>));</span><br><span class="line">DataStreamSource&lt;Integer&gt; streamSource02 = env.fromElements(<span class="number">2</span>, <span class="number">3</span>, <span class="number">9</span>);</span><br><span class="line"><span class="comment">// 使用connect进行连接</span></span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;String, Integer&gt;, Integer&gt; connect = streamSource01.connect(streamSource02);</span><br><span class="line">connect.map(<span class="keyword">new</span> <span class="title class_">CoMapFunction</span>&lt;Tuple2&lt;String, Integer&gt;, Integer, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Integer <span class="title function_">map1</span><span class="params">(Tuple2&lt;String, Integer&gt; value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">return</span> value.f1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Integer <span class="title function_">map2</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).map(x -&gt; x * <span class="number">100</span>).print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出：</span></span><br><span class="line"><span class="number">300</span> <span class="number">500</span> <span class="number">200</span> <span class="number">900</span> <span class="number">300</span></span><br></pre></td></tr></table></figure>
<h3 id="2-8-Split-和-Select">2.8 Split 和 Select</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>Split [DataStream → SplitStream]</strong>：用于将一个 DataStream 按照指定规则进行拆分为多个 DataStream，需要注意的是这里进行的是逻辑拆分，即 Split 只是将数据贴上不同的类型标签，但最终返回的仍然只是一个 SplitStream；</p>
</li>
<li class="lvl-2">
<p><strong>Select [SplitStream → DataStream]</strong>：想要从逻辑拆分的 SplitStream 中获取真实的不同类型的 DataStream，需要使用 Select 算子，示例如下：</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStreamSource&lt;Integer&gt; streamSource = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>);</span><br><span class="line"><span class="comment">// 标记</span></span><br><span class="line">SplitStream&lt;Integer&gt; split = streamSource.split(<span class="keyword">new</span> <span class="title class_">OutputSelector</span>&lt;Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Iterable&lt;String&gt; <span class="title function_">select</span><span class="params">(Integer value)</span> &#123;</span><br><span class="line">        List&lt;String&gt; output = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;String&gt;();</span><br><span class="line">        output.add(value % <span class="number">2</span> == <span class="number">0</span> ? <span class="string">&quot;even&quot;</span> : <span class="string">&quot;odd&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> output;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 获取偶数数据集</span></span><br><span class="line">split.select(<span class="string">&quot;even&quot;</span>).print();</span><br><span class="line"><span class="comment">// 输出 2,4,6,8</span></span><br></pre></td></tr></table></figure>
<h3 id="2-9-project-DataStream-→-DataStream">2.9 project [DataStream → DataStream]</h3>
<p>project 主要用于获取 tuples 中的指定字段集，示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStreamSource&lt;Tuple3&lt;String, Integer, String&gt;&gt; streamSource = env.fromElements(</span><br><span class="line">                                                                         <span class="keyword">new</span> <span class="title class_">Tuple3</span>&lt;&gt;(<span class="string">&quot;li&quot;</span>, <span class="number">22</span>, <span class="string">&quot;2018-09-23&quot;</span>),</span><br><span class="line">                                                                         <span class="keyword">new</span> <span class="title class_">Tuple3</span>&lt;&gt;(<span class="string">&quot;ming&quot;</span>, <span class="number">33</span>, <span class="string">&quot;2020-09-23&quot;</span>));</span><br><span class="line">streamSource.project(<span class="number">0</span>,<span class="number">2</span>).print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(li,<span class="number">2018</span>-09-<span class="number">23</span>)</span><br><span class="line">(ming,<span class="number">2020</span>-09-<span class="number">23</span>)</span><br></pre></td></tr></table></figure>
<h2 id="三、物理分区">三、物理分区</h2>
<p>物理分区 (Physical partitioning) 是 Flink 提供的底层的 API，允许用户采用内置的分区规则或者自定义的分区规则来对数据进行分区，从而避免数据在某些分区上过于倾斜，常用的分区规则如下：</p>
<h3 id="3-1-Random-partitioning-DataStream-→-DataStream">3.1 Random partitioning [DataStream → DataStream]</h3>
<p>随机分区 (Random partitioning) 用于随机的将数据分布到所有下游分区中，通过 shuffle 方法来进行实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">dataStream.shuffle();</span><br></pre></td></tr></table></figure>
<h3 id="3-2-Rebalancing-DataStream-→-DataStream">3.2 Rebalancing [DataStream → DataStream]</h3>
<p>Rebalancing 采用轮询的方式将数据进行分区，其适合于存在数据倾斜的场景下，通过 rebalance 方法进行实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">dataStream.rebalance();</span><br></pre></td></tr></table></figure>
<h3 id="3-3-Rescaling-DataStream-→-DataStream">3.3 Rescaling [DataStream → DataStream]</h3>
<p>当采用 Rebalancing 进行分区平衡时，其实现的是全局性的负载均衡，数据会通过网络传输到其他节点上并完成分区数据的均衡。 而 Rescaling 则是低配版本的 rebalance，它不需要额外的网络开销，它只会对上下游的算子之间进行重新均衡，通过 rescale 方法进行实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">dataStream.rescale();</span><br></pre></td></tr></table></figure>
<p>ReScale 这个单词具有重新缩放的意义，其对应的操作也是如此，具体如下：如果上游 operation 并行度为 2，而下游的 operation 并行度为 6，则其中 1 个上游的 operation 会将元素分发到 3 个下游 operation，另 1 个上游 operation 则会将元素分发到另外 3 个下游 operation。反之亦然，如果上游的 operation 并行度为 6，而下游 operation 并行度为 2，则其中 3 个上游 operation 会将元素分发到 1 个下游 operation，另 3 个上游 operation 会将元素分发到另外 1 个下游operation：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-Rescaling.png"/> </div>
<h3 id="3-4-Broadcasting-DataStream-→-DataStream">3.4 Broadcasting [DataStream → DataStream]</h3>
<p>将数据分发到所有分区上。通常用于小数据集与大数据集进行关联的情况下，此时可以将小数据集广播到所有分区上，避免频繁的跨分区关联，通过 broadcast 方法进行实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">dataStream.broadcast();</span><br></pre></td></tr></table></figure>
<h3 id="3-5-Custom-partitioning-DataStream-→-DataStream">3.5 Custom partitioning [DataStream → DataStream]</h3>
<p>Flink 运行用户采用自定义的分区规则来实现分区，此时需要通过实现 Partitioner 接口来自定义分区规则，并指定对应的分区键，示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource = env.fromElements(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;Hadoop&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;Spark&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;Flink-streaming&quot;</span>, <span class="number">2</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;Flink-batch&quot;</span>, <span class="number">4</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;Storm&quot;</span>, <span class="number">4</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(<span class="string">&quot;HBase&quot;</span>, <span class="number">3</span>));</span><br><span class="line">streamSource.partitionCustom(<span class="keyword">new</span> <span class="title class_">Partitioner</span>&lt;String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">partition</span><span class="params">(String key, <span class="type">int</span> numPartitions)</span> &#123;</span><br><span class="line">        <span class="comment">// 将第一个字段包含flink的Tuple2分配到同一个分区</span></span><br><span class="line">        <span class="keyword">return</span> key.toLowerCase().contains(<span class="string">&quot;flink&quot;</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;, <span class="number">0</span>).print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出如下：</span></span><br><span class="line"><span class="number">1</span>&gt; (Flink-streaming,<span class="number">2</span>)</span><br><span class="line"><span class="number">1</span>&gt; (Flink-batch,<span class="number">4</span>)</span><br><span class="line"><span class="number">2</span>&gt; (Hadoop,<span class="number">1</span>)</span><br><span class="line"><span class="number">2</span>&gt; (Spark,<span class="number">1</span>)</span><br><span class="line"><span class="number">2</span>&gt; (Storm,<span class="number">4</span>)</span><br><span class="line"><span class="number">2</span>&gt; (HBase,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h2 id="四、任务链和资源组">四、任务链和资源组</h2>
<p>任务链和资源组 ( Task chaining and resource groups ) 也是 Flink 提供的底层 API，用于控制任务链和资源分配。默认情况下，如果操作允许 (例如相邻的两次 map 操作) ，则 Flink 会尝试将它们在同一个线程内进行，从而可以获取更好的性能。但是 Flink 也允许用户自己来控制这些行为，这就是任务链和资源组 API：</p>
<h3 id="4-1-startNewChain">4.1 startNewChain</h3>
<p>startNewChain 用于基于当前 operation 开启一个新的任务链。如下所示，基于第一个 map 开启一个新的任务链，此时前一个 map 和 后一个 map 将处于同一个新的任务链中，但它们与 filter 操作则分别处于不同的任务链中：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">someStream.filter(...).map(...).startNewChain().map(...);</span><br></pre></td></tr></table></figure>
<h3 id="4-2-disableChaining">4.2 disableChaining</h3>
<p>disableChaining 操作用于禁止将其他操作与当前操作放置于同一个任务链中，示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">someStream.map(...).disableChaining();</span><br></pre></td></tr></table></figure>
<h3 id="4-3-slotSharingGroup">4.3 slotSharingGroup</h3>
<p>slot 是任务管理器  (TaskManager) 所拥有资源的固定子集，每个操作 (operation) 的子任务 (sub task) 都需要获取 slot 来执行计算，但每个操作所需要资源的大小都是不相同的，为了更好地利用资源，Flink 允许不同操作的子任务被部署到同一 slot 中。slotSharingGroup 用于设置操作的 slot 共享组 (slot sharing group) ，Flink 会将具有相同 slot 共享组的操作放到同一个 slot 中 。示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">someStream.filter(...).slotSharingGroup(<span class="string">&quot;slotSharingGroupName&quot;</span>);</span><br></pre></td></tr></table></figure>
<h2 id="参考资料-4">参考资料</h2>
<p>Flink Operators： <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/</a></p>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink-Standalone集群部署</title>
    <url>/2022/03/19/Flink-Standalone%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h2 id="一、部署模式">一、部署模式</h2>
<p>Flink 支持使用多种部署模式来满足不同规模应用的需求，常见的有单机模式，Standalone Cluster 模式，同时 Flink 也支持部署在其他第三方平台上，如 YARN，Mesos，Docker，Kubernetes 等。以下主要介绍其单机模式和 Standalone Cluster 模式的部署。</p>
<h2 id="二、单机模式">二、单机模式</h2>
<p>单机模式是一种开箱即用的模式，可以在单台服务器上运行，适用于日常的开发和调试。具体操作步骤如下：</p>
<h3 id="2-1-安装部署">2.1 安装部署</h3>
<p><strong>1. 前置条件</strong></p>
<p>Flink 的运行依赖 JAVA 环境，故需要预先安装好 JDK，具体步骤可以参考：<a href="https://github.com/oicio/BigData-Notes/blob/master/notes/installation/Linux%E4%B8%8BJDK%E5%AE%89%E8%A3%85.md">Linux 环境下 JDK 安装</a></p>
<p><strong>2. 下载 &amp; 解压 &amp; 运行</strong></p>
<p>Flink 所有版本的安装包可以直接从其<a href="https://flink.apache.org/downloads.html">官网</a>进行下载，这里我下载的 Flink 的版本为 <code>1.9.1</code> ，要求的 JDK 版本为 <code>1.8.x +</code>。 下载后解压到指定目录：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf flink-1.9.1-bin-scala_2.12.tgz  -C /usr/app</span><br></pre></td></tr></table></figure>
<p>不需要进行任何配置，直接使用以下命令就可以启动单机版本的 Flink：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/start-cluster.sh</span><br></pre></td></tr></table></figure>
<p><strong>3. WEB UI 界面</strong></p>
<p>Flink 提供了 WEB 界面用于直观的管理 Flink 集群，访问端口为 <code>8081</code>：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-dashboard.png"/> </div>
<p>Flink 的 WEB UI 界面支持大多数常用功能，如提交作业，取消作业，查看各个节点运行情况，查看作业执行情况等，大家可以在部署完成后，进入该页面进行详细的浏览。</p>
<h3 id="2-2-作业提交">2.2 作业提交</h3>
<p>启动后可以运行安装包中自带的词频统计案例，具体步骤如下：</p>
<p><strong>1. 开启端口</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nc -lk 9999</span><br></pre></td></tr></table></figure>
<p><strong>2. 提交作业</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/flink run examples/streaming/SocketWindowWordCount.jar --port 9999</span><br></pre></td></tr></table></figure>
<p>该 JAR 包的源码可以在 Flink 官方的 GitHub 仓库中找到，地址为 ：<a href="https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/socket/SocketWindowWordCount.java">SocketWindowWordCount</a> ，可选传参有 hostname， port，对应的词频数据需要使用空格进行分割。</p>
<p><strong>3. 输入测试数据</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a a b b c c c a e</span><br></pre></td></tr></table></figure>
<p><strong>4. 查看控制台输出</strong></p>
<p>可以通过 WEB UI 的控制台查看作业统运行情况：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-socket-wordcount.png"/> </div>
<p>也可以通过 WEB 控制台查看到统计结果：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-socket-wordcount-stdout.png"/> </div>
<h3 id="2-3-停止作业">2.3 停止作业</h3>
<p>可以直接在 WEB 界面上点击对应作业的 <code>Cancel Job</code>  按钮进行取消，也可以使用命令行进行取消。使用命令行进行取消时，需要先获取到作业的 JobId，可以使用 <code>flink list</code> 命令查看，输出如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 flink-1.9.1]# ./bin/flink list</span><br><span class="line">Waiting for response...</span><br><span class="line">------------------ Running/Restarting Jobs -------------------</span><br><span class="line">05.11.2019 08:19:53 : ba2b1cc41a5e241c32d574c93de8a2bc : Socket Window WordCount (RUNNING)</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">No scheduled jobs.</span><br></pre></td></tr></table></figure>
<p>获取到 JobId 后，就可以使用 <code>flink cancel</code> 命令取消作业：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/flink cancel ba2b1cc41a5e241c32d574c93de8a2bc</span><br></pre></td></tr></table></figure>
<h3 id="2-4-停止-Flink">2.4 停止 Flink</h3>
<p>命令如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/stop-cluster.sh</span><br></pre></td></tr></table></figure>
<h2 id="三、Standalone-Cluster">三、Standalone Cluster</h2>
<p>Standalone Cluster 模式是 Flink 自带的一种集群模式，具体配置步骤如下：</p>
<h3 id="3-1-前置条件">3.1 前置条件</h3>
<p>使用该模式前，需要确保所有服务器间都已经配置好 SSH 免密登录服务。这里我以三台服务器为例，主机名分别为 hadoop001，hadoop002，hadoop003 , 其中 hadoop001 为 master 节点，其余两台为 slave 节点，搭建步骤如下：</p>
<h3 id="3-2-搭建步骤">3.2 搭建步骤</h3>
<p>修改 <code>conf/flink-conf.yaml</code> 中 jobmanager 节点的通讯地址为 hadoop001:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">jobmanager.rpc.address:</span> <span class="string">hadoop001</span></span><br></pre></td></tr></table></figure>
<p>修改 <code>conf/slaves</code> 配置文件，将 hadoop002 和 hadoop003 配置为 slave 节点：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop002</span><br><span class="line">hadoop003</span><br></pre></td></tr></table></figure>
<p>将配置好的 Flink 安装包分发到其他两台服务器上：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scp -r /usr/app/flink-1.9.1 hadoop002:/usr/app</span><br><span class="line">scp -r /usr/app/flink-1.9.1 hadoop003:/usr/app</span><br></pre></td></tr></table></figure>
<p>在 hadoop001 上使用和单机模式相同的命令来启动集群：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/start-cluster.sh</span><br></pre></td></tr></table></figure>
<p>此时控制台输出如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-start-cluster-shell.png"/> </div>
<p>启动完成后可以使用 <code>Jps</code> 命令或者通过 WEB 界面来查看是否启动成功。</p>
<h3 id="3-3-可选配置">3.3 可选配置</h3>
<p>除了上面介绍的 <em>jobmanager.rpc.address</em> 是必选配置外，Flink h还支持使用其他可选参数来优化集群性能，主要如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>jobmanager.heap.size</strong>：JobManager 的 JVM 堆内存大小，默认为 1024m 。</p>
</li>
<li class="lvl-2">
<p><strong>taskmanager.heap.size</strong>：Taskmanager 的 JVM 堆内存大小，默认为 1024m 。</p>
</li>
<li class="lvl-2">
<p><strong>taskmanager.numberOfTaskSlots</strong>：Taskmanager 上 slots 的数量，通常设置为 CPU 核心的数量，或其一半。</p>
</li>
<li class="lvl-2">
<p><strong>parallelism.default</strong>：任务默认的并行度。</p>
</li>
<li class="lvl-2">
<p><strong>io.tmp.dirs</strong>：存储临时文件的路径，如果没有配置，则默认采用服务器的临时目录，如 LInux 的 <code>/tmp</code> 目录。</p>
</li>
</ul>
<p>更多配置可以参考 Flink 的官方手册：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/config.html">Configuration</a></p>
<h2 id="四、Standalone-Cluster-HA">四、Standalone Cluster HA</h2>
<p>上面我们配置的 Standalone 集群实际上只有一个 JobManager，此时是存在单点故障的，所以官方提供了 Standalone Cluster HA 模式来实现集群高可用。</p>
<h3 id="4-1-前置条件">4.1 前置条件</h3>
<p>在 Standalone Cluster HA 模式下，集群可以由多个 JobManager，但只有一个处于 active 状态，其余的则处于备用状态，Flink 使用 ZooKeeper 来选举出 Active JobManager，并依赖其来提供一致性协调服务，所以需要预先安装 ZooKeeper 。</p>
<p>另外在高可用模式下，还需要使用分布式文件系统来持久化存储 JobManager 的元数据，最常用的就是 HDFS，所以 Hadoop 也需要预先安装。关于 Hadoop 集群和 ZooKeeper 集群的搭建可以参考：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a href="https://github.com/oicio/BigData-Notes/blob/master/notes/installation/Hadoop%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.md">Hadoop 集群环境搭建</a></p>
</li>
<li class="lvl-2">
<p><a href="https://github.com/oicio/BigData-Notes/blob/master/notes/installation/Zookeeper%E5%8D%95%E6%9C%BA%E7%8E%AF%E5%A2%83%E5%92%8C%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.md">Zookeeper 单机环境和集群环境搭建</a></p>
</li>
</ul>
<h3 id="4-2-搭建步骤">4.2 搭建步骤</h3>
<p>修改 <code>conf/flink-conf.yaml</code> 文件，增加如下配置：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 配置使用zookeeper来开启高可用模式</span></span><br><span class="line"><span class="attr">high-availability:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="comment"># 配置zookeeper的地址，采用zookeeper集群时，可以使用逗号来分隔多个节点地址</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.quorum:</span> <span class="string">hadoop003:2181</span></span><br><span class="line"><span class="comment"># 在zookeeper上存储flink集群元信息的路径</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.path.root:</span> <span class="string">/flink</span></span><br><span class="line"><span class="comment"># 集群id</span></span><br><span class="line"><span class="attr">high-availability.cluster-id:</span> <span class="string">/standalone_cluster_one</span></span><br><span class="line"><span class="comment"># 持久化存储JobManager元数据的地址，zookeeper上存储的只是指向该元数据的指针信息</span></span><br><span class="line"><span class="attr">high-availability.storageDir:</span> <span class="string">hdfs://hadoop001:8020/flink/recovery</span></span><br></pre></td></tr></table></figure>
<p>修改 <code>conf/masters</code> 文件，将 hadoop001 和 hadoop002 都配置为 master 节点：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop001:8081</span><br><span class="line">hadoop002:8081</span><br></pre></td></tr></table></figure>
<p>确保 Hadoop 和 ZooKeeper 已经启动后，使用以下命令来启动集群：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/start-cluster.sh</span><br></pre></td></tr></table></figure>
<p>此时输出如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-standalone-cluster-ha.png"/> </div>
<p>可以看到集群已经以 HA 的模式启动，此时还需要在各个节点上使用 <code>jps</code> 命令来查看进程是否启动成功，正常情况如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-standalone-cluster-jps.png"/> </div>
<p>只有 hadoop001 和 hadoop002 的 JobManager 进程，hadoop002 和 hadoop003 上的 TaskManager 进程都已经完全启动，才表示 Standalone Cluster HA 模式搭建成功。</p>
<h3 id="4-3-常见异常">4.3 常见异常</h3>
<p>如果进程没有启动，可以通过查看 <code>log</code> 目录下的日志来定位错误，常见的一个错误如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2019-11-05 09:18:35,877 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint      </span><br><span class="line">- Shutting StandaloneSessionClusterEntrypoint down with application status FAILED. Diagnostics</span><br><span class="line">java.io.IOException: Could not create FileSystem for highly available storage (high-availability.storageDir)</span><br><span class="line">.......</span><br><span class="line">Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file </span><br><span class="line">system implementation for scheme &#x27;hdfs&#x27;. The scheme is not directly supported by Flink and no </span><br><span class="line">Hadoop file system to support this scheme could be loaded.</span><br><span class="line">.....</span><br><span class="line">Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Hadoop is not in </span><br><span class="line">the classpath/dependencies.</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<p>可以看到是因为在 classpath 目录下找不到 Hadoop 的相关依赖，此时需要检查是否在环境变量中配置了 Hadoop 的安装路径，如果路径已经配置但仍然存在上面的问题，可以从 <a href="https://flink.apache.org/downloads.html">Flink 官网</a>下载对应版本的 Hadoop 组件包：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-optional-components.png"/> </div>
<p>下载完成后，将该 JAR 包上传至<strong>所有</strong> Flink 安装目录的 <code>lib</code> 目录即可。</p>
<h2 id="参考资料-5">参考资料</h2>
<ul class="lvl-0">
<li class="lvl-2">
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/deployment/cluster_setup.html#standalone-cluster">Standalone Cluster</a></p>
</li>
<li class="lvl-2">
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/jobmanager_high_availability.html">JobManager High Availability (HA)</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink-Watermark机制</title>
    <url>/2023/01/03/Flink-Watermark%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h3 id="watermark简介">watermark简介</h3>
<h4 id="watermark的概念">watermark的概念</h4>
<p>watermark是一种衡量Event Time进展的机制，它是数据本身的一个隐藏属性。通常基于Event Time的数据，自身都包含一个timestamp，例如<code>1472693399700（2016-09-01 09:29:59.700）</code>，而这条数据的watermark时间则可能是：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">watermark(<span class="number">1472693399700</span>) = <span class="number">1472693396700</span>(<span class="number">2016</span>-09-<span class="number">01</span> 09:<span class="number">29</span>:<span class="number">56.700</span>)</span><br></pre></td></tr></table></figure>
<p>这条数据的watermark时间是什么含义呢？即：timestamp小于<code>1472693396700(2016-09-01 09:29:56.700)</code>的数据，都已经到达了</p>
<p><img src="https://img-blog.csdn.net/20160929172201717" alt="è¿éåå¾çæè¿°"></p>
<h4 id="watermark有什么用？">watermark有什么用？</h4>
<p>watermark是用于处理乱序事件的，而正确的处理乱序事件，通常用watermark机制结合window来实现。</p>
<p>我们知道，流处理从事件产生，到流经source，再到operator，中间是有一个过程和时间的。虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生（out-of-order或者说late element）。</p>
<p>但是对于late element，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发window去进行计算了。这个特别的机制，就是watermark。</p>
<h4 id="watermark如何分配？">watermark如何分配？</h4>
<p>通常，在接收到source的数据后，应该立刻生成watermark；但是，也可以在source后，应用简单的map或者filter操作，然后再生成watermark。生成watermark的方式主要有2大类：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">(<span class="number">1</span>):With Periodic <span class="title function_">Watermarks</span></span><br><span class="line"><span class="params">(<span class="number">2</span>)</span>:With Punctuated Watermarks</span><br></pre></td></tr></table></figure>
<p>第一种可以定义一个最大允许乱序的时间，这种情况应用较多。 代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * This generator generates watermarks assuming that elements come out of order to a certain degree only.</span></span><br><span class="line"><span class="comment"> * The latest elements for a certain timestamp t will arrive at most n milliseconds after the earliest</span></span><br><span class="line"><span class="comment"> * elements for timestamp t.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BoundedOutOfOrdernessGenerator</span> <span class="keyword">extends</span> <span class="title class_">AssignerWithPeriodicWatermarks</span>[MyEvent] &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="type">val</span> <span class="variable">maxOutOfOrderness</span> <span class="operator">=</span> <span class="number">3500L</span>; <span class="comment">// 3.5 seconds</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">var</span> currentMaxTimestamp: Long;</span><br><span class="line"> </span><br><span class="line">    override def <span class="title function_">extractTimestamp</span><span class="params">(element: MyEvent, previousElementTimestamp: Long)</span>: Long = &#123;</span><br><span class="line">        <span class="type">val</span> <span class="variable">timestamp</span> <span class="operator">=</span> element.getCreationTime()</span><br><span class="line">        currentMaxTimestamp = max(timestamp, currentMaxTimestamp)</span><br><span class="line">        timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    override def <span class="title function_">getCurrentWatermark</span><span class="params">()</span>: Watermark = &#123;</span><br><span class="line">        <span class="comment">// return the watermark as current highest timestamp minus the out-of-orderness bound</span></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Watermark</span>(currentMaxTimestamp - maxOutOfOrderness);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>程序中有一个extractTimestamp方法，就是根据数据本身的Event time来获取；还有一个getCurrentWatermar方法，是用currentMaxTimestamp - maxOutOfOrderness来获取的。</p>
<h3 id="watermark代码">watermark代码</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    StreamExecutionEnvironment environment=StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    environment.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">    <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">    properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;localhost:9092&quot;</span>);</span><br><span class="line">    properties.setProperty(<span class="string">&quot;zookeeper.connect&quot;</span>, <span class="string">&quot;localhost:2181&quot;</span>);</span><br><span class="line">    properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    	加载数据源kafka</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    FlinkKafkaConsumer011&lt;String&gt; myConsumer = <span class="keyword">new</span> <span class="title class_">FlinkKafkaConsumer011</span>&lt;String&gt;(<span class="string">&quot;test&quot;</span>, <span class="keyword">new</span> <span class="title class_">SimpleStringSchema</span>(),properties);</span><br><span class="line">    DataStream&lt;String&gt; stream = environment.addSource(myConsumer);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    	将输入的字符串以“，”切分</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    DataStream&lt;Tuple2&lt;String,Long&gt;&gt; inputMap = stream.map(<span class="keyword">new</span> <span class="title class_">MapFunction</span>&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title function_">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">            String [] arr = s.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(arr[<span class="number">0</span>],Long.parseLong(arr[<span class="number">1</span>]));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//赋予时间戳然后生成水位线</span></span><br><span class="line">    DataStream&lt;Tuple2&lt;String,Long&gt;&gt; watermarkStream = inputMap.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="title class_">AssignerWithPeriodicWatermarks</span>&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">        <span class="type">Long</span> <span class="variable">currentMaxTimeStamp</span> <span class="operator">=</span> <span class="number">0L</span>;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Long</span> <span class="variable">maxOutOfOrderness</span> <span class="operator">=</span> <span class="number">10000L</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">SimpleDateFormat</span> <span class="variable">sdf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SimpleDateFormat</span>(<span class="string">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Nullable</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Watermark <span class="title function_">getCurrentWatermark</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Watermark</span>(currentMaxTimeStamp-maxOutOfOrderness);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">extractTimestamp</span><span class="params">(Tuple2&lt;String, Long&gt; element, <span class="type">long</span> previousTimeStamp)</span> &#123;</span><br><span class="line"></span><br><span class="line">            <span class="type">long</span> <span class="variable">timestamp</span> <span class="operator">=</span> element.f1;</span><br><span class="line">            currentMaxTimeStamp = Math.max(currentMaxTimeStamp,timestamp);</span><br><span class="line">            <span class="keyword">assert</span> <span class="title function_">getCurrentWatermark</span><span class="params">()</span> != <span class="literal">null</span>;</span><br><span class="line">            System.out.println(<span class="string">&quot;键值 :&quot;</span>+element.f0+<span class="string">&quot;,事件时间:[ &quot;</span>+sdf.format(element.f1)+<span class="string">&quot; ],currentMaxTimestamp:[ &quot;</span>+</span><br><span class="line">                               sdf.format(currentMaxTimeStamp)+<span class="string">&quot; ],水印时间:[ &quot;</span>+sdf.format(getCurrentWatermark().getTimestamp())+<span class="string">&quot; ]&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> timestamp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    </span><br><span class="line">    DataStream&lt;String&gt; window = watermarkStream.keyBy(<span class="number">0</span>)</span><br><span class="line">        .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">3</span>)))</span><br><span class="line">        .apply(<span class="keyword">new</span> <span class="title class_">WindowFunction</span>&lt;Tuple2&lt;String, Long&gt;, String, Tuple, TimeWindow&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">apply</span><span class="params">(Tuple tuple, TimeWindow timeWindow, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; iterable, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">/*</span></span><br><span class="line"><span class="comment">                         *对window内数据进行排序，保证数据排序</span></span><br><span class="line"><span class="comment">                         *用list保存迭代流所有数据，然后排序</span></span><br><span class="line"><span class="comment">                         */</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> tuple.toString();</span><br><span class="line">                List&lt;Long&gt; arrayList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;Long&gt;();</span><br><span class="line">                Iterator&lt;Tuple2&lt;String, Long&gt;&gt; it = iterable.iterator();</span><br><span class="line">                <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">                    Tuple2&lt;String, Long&gt; next = it.next();</span><br><span class="line">                    arrayList.add(next.f1);</span><br><span class="line">                &#125;</span><br><span class="line">                Collections.sort(arrayList);</span><br><span class="line">                <span class="type">SimpleDateFormat</span> <span class="variable">sdf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SimpleDateFormat</span>(<span class="string">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>);</span><br><span class="line">                <span class="type">String</span> <span class="variable">result</span> <span class="operator">=</span> <span class="string">&quot;\n 键值 : &quot;</span>+ key + <span class="string">&quot;\n              触发窗内数据个数 : &quot;</span> + arrayList.size() + <span class="string">&quot;\n              触发窗起始数据： &quot;</span> + sdf.format(arrayList.get(<span class="number">0</span>)) + <span class="string">&quot;\n              触发窗最后（可能是延时）数据：&quot;</span> +</span><br><span class="line">                    sdf.format(arrayList.get(arrayList.size() - <span class="number">1</span>))</span><br><span class="line">                    + <span class="string">&quot;\n              实际窗起始和结束时间： &quot;</span> + sdf.format(timeWindow.getStart()) + <span class="string">&quot;《----》&quot;</span> + sdf.format(timeWindow.getEnd()) + <span class="string">&quot; \n \n &quot;</span>;</span><br><span class="line"></span><br><span class="line">                collector.collect(result);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    window.print();</span><br><span class="line">    environment.execute(<span class="string">&quot;eventtime-watermark&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>程序详解</p>
</li>
</ul>
<blockquote>
<p>接收kafka数据</p>
</blockquote>
<blockquote>
<p>将每行数据按照字符分隔，每行map成一个tuple类型（code，time）</p>
</blockquote>
<blockquote>
<p>抽取timestamp生成watermark。并打印（code，time，格式化的time，currentMaxTimestamp，currentMaxTimestamp的格式化时间，watermark时间）。</p>
</blockquote>
<blockquote>
<p>event time每隔3秒触发一次窗口，输出（code，窗口内元素个数，窗口内最早元素的时间，窗口内最晚元素的时间，窗口自身开始时间，窗口自身结束时间）</p>
</blockquote>
<p>注意：<code>new AssignerWithPeriodicWatermarks[(String,Long)</code>中有抽取timestamp和生成watermark2个方法，在执行时，它是先抽取timestamp，后生成watermark，因此我们在这里print的watermark时间，其实是上一条的watermark时间.</p>
<h3 id="watermark实验">watermark实验</h3>
<h4 id="正常数据">正常数据</h4>
<p>在上述代码中设置了允许最大的延迟时间是10s</p>
<p>输入：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756862000</span></span><br><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756866000</span></span><br><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756872000</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">键值 :<span class="number">000001</span>,事件时间:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">22.000</span> ],currentMaxTimestamp:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">22.000</span> ],水印时间:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">12.000</span> ]</span><br><span class="line">键值 :<span class="number">000001</span>,事件时间:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">26.000</span> ],currentMaxTimestamp:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">26.000</span> ],水印时间:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">16.000</span> ]</span><br><span class="line">键值 :<span class="number">000001</span>,事件时间:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">32.000</span> ],currentMaxTimestamp:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">32.000</span> ],水印时间:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">22.000</span> ]</span><br></pre></td></tr></table></figure>
<p>我们可以看到输入已经达到10s，等于第一条数据的Event Time了，却没有触发window窗口计算，此时再输入：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756862000</span></span><br><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756866000</span></span><br><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756872000</span></span><br><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756874000</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">键值 :<span class="number">000001</span>,事件时间:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">34.000</span> ],currentMaxTimestamp:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">34.000</span> ],水印时间:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">24.000</span> ]</span><br><span class="line"><span class="number">3</span>&gt; </span><br><span class="line"> 键值 : (<span class="number">000001</span>)</span><br><span class="line">              触发窗内数据个数 : <span class="number">1</span></span><br><span class="line">              触发窗起始数据： <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">22.000</span></span><br><span class="line">              触发窗最后（可能是延时）数据：<span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">22.000</span></span><br><span class="line">              实际窗起始和结束时间： <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">21.000</span>《----》<span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">24.000</span> </span><br></pre></td></tr></table></figure>
<p>此时watermark的时间<code>2016-04-27 19:34:24.000</code>是可以看到已经触发窗口计算了，因此得到一个结论，当<code>watermark时间 &gt;= window_end_time</code>的时候才会触发窗口计算，需要注意的是watermark与key无关，是系统设定的，即使是不同的key，只要是时间达到了即可触发。</p>
<p>window的触发条件：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">watermark时间 &gt;= window_end_time</span></span><br><span class="line"><span class="comment">在[window_start_time,window_end_time)中有数据存在</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<p>只有同时满足上述条件才会触发窗口计算。</p>
<h4 id="乱序数据">乱序数据</h4>
<p>我们此时输入一个乱序的数据，此时watermark的时间<code>2016-04-27 19:34:24.000</code>，我们输入一个：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756871000</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">键值 :<span class="number">000001</span>,事件时间:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">31.000</span> ],currentMaxTimestamp:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">34.000</span> ],水印时间:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">24.000</span> ]</span><br></pre></td></tr></table></figure>
<p>可以看到，虽然我们输入了一个19:34:31的数据，但是currentMaxTimestamp和watermark都没变。此时，按照我们上面提到的公式：watermark时间（19:34:24） &lt; window_end_time（19:34:27），因此不能触发window。此时再输入一条数据：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756881000</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">键值 :<span class="number">000001</span>,事件时间:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">41.000</span> ],currentMaxTimestamp:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">41.000</span> ],水印时间:[ <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">31.000</span> ]</span><br><span class="line"><span class="number">3</span>&gt; </span><br><span class="line"> 键值 : (<span class="number">000001</span>)</span><br><span class="line">              触发窗内数据个数 : <span class="number">1</span></span><br><span class="line">              触发窗起始数据： <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">26.000</span></span><br><span class="line">              触发窗最后（可能是延时）数据：<span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">26.000</span></span><br><span class="line">              实际窗起始和结束时间： <span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">24.000</span>《----》<span class="number">2016</span>-<span class="number">04</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">34</span>:<span class="number">27.000</span> </span><br></pre></td></tr></table></figure>
<p>可以看到 <code>2016-04-27 19:34:31.000&gt;2016-04-27 19:34:27.000</code>，此时watermark时间大于窗口结束时间了，因此会触发窗口计算，并更新最新的窗口结束位置以及最新的watermark。</p>
<h4 id="乱序很多的数据">乱序很多的数据</h4>
<p>我们输入一个乱序很多的（其实只要Event Time &lt; watermark时间）数据来测试下：</p>
<p>输入：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756862000</span></span><br><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756866000</span></span><br><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756872000</span></span><br><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756874000</span></span><br><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756871000</span></span><br><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756881000</span></span><br><span class="line">&gt;<span class="number">000001</span>,<span class="number">1461756841000</span></span><br></pre></td></tr></table></figure>
<p>此时<code>Event Time &lt; watermark</code>时间，所以来一条就触发一个window。</p>
<h3 id="总结">总结</h3>
<ol>
<li class="lvl-3">
<p>Flink如何处理乱序？</p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">watermark + window机制</span><br></pre></td></tr></table></figure>
<ol start="2">
<li class="lvl-3">
<p>Flink何时触发window？</p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">watermark时间 &gt; Event Time（对于late element太多的数据而言）</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">	或者</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">watermark时间 &gt;= window_end_time（对于out-of-order以及正常的数据而言</span><br><span class="line">在[window_start_time,window_end_time)中有数据存在</span><br></pre></td></tr></table></figure>
<ol start="3">
<li class="lvl-3">
<p>Flink应该如何设置最大乱序时间？</p>
</li>
</ol>
<p>这个要结合自己的业务以及数据情况去设置。如果maxOutOfOrderness设置的太小，而自身数据发送时由于网络等原因导致乱序或者late太多，那么最终的结果就是会有很多单条的数据在window中被触发，数据的正确性影响太大。</p>
<p><img src="https://img-blog.csdn.net/20160930135616800" alt="è¿éåå¾çæè¿°"></p>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink开发环境搭建</title>
    <url>/2022/03/19/Flink%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h2 id="一、安装-Scala-插件">一、安装 Scala 插件</h2>
<p>Flink 分别提供了基于 Java 语言和 Scala 语言的 API ，如果想要使用 Scala 语言来开发 Flink 程序，可以通过在 IDEA 中安装 Scala 插件来提供语法提示，代码高亮等功能。打开 IDEA , 依次点击 <code>File =&gt; settings =&gt; plugins</code> 打开插件安装页面，搜索 Scala 插件并进行安装，安装完成后，重启 IDEA 即可生效。</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/scala-plugin.png"/> </div>
<h2 id="二、Flink-项目初始化">二、Flink 项目初始化</h2>
<h3 id="2-1-使用官方脚本构建">2.1 使用官方脚本构建</h3>
<p>Flink 官方支持使用 Maven 和 Gradle 两种构建工具来构建基于 Java 语言的 Flink 项目；支持使用 SBT 和 Maven 两种构建工具来构建基于 Scala 语言的 Flink 项目。 这里以 Maven 为例进行说明，因为其可以同时支持 Java 语言和 Scala 语言项目的构建。需要注意的是 Flink 1.9 只支持 Maven 3.0.4 以上的版本，Maven 安装完成后，可以通过以下两种方式来构建项目：</p>
<p><strong>1. 直接基于 Maven Archetype 构建</strong></p>
<p>直接使用下面的 mvn 语句来进行构建，然后根据交互信息的提示，依次输入 groupId , artifactId 以及包名等信息后等待初始化的完成：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mvn archetype:generate                               \</span><br><span class="line">      -DarchetypeGroupId=org.apache.flink              \</span><br><span class="line">      -DarchetypeArtifactId=flink-quickstart-java      \</span><br><span class="line">      -DarchetypeVersion=1.9.0</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：如果想要创建基于 Scala 语言的项目，只需要将 flink-quickstart-java 换成 flink-quickstart-scala 即可，后文亦同。</p>
</blockquote>
<p><strong>2. 使用官方脚本快速构建</strong></p>
<p>为了更方便的初始化项目，官方提供了快速构建脚本，可以直接通过以下命令来进行调用：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">curl https://flink.apache.org/q/quickstart.sh | bash -s 1.9.0</span></span><br></pre></td></tr></table></figure>
<p>该方式其实也是通过执行 maven archetype 命令来进行初始化，其脚本内容如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">PACKAGE=quickstart</span><br><span class="line"></span><br><span class="line">mvn archetype:generate \</span><br><span class="line">  -DarchetypeGroupId=org.apache.flink \</span><br><span class="line">  -DarchetypeArtifactId=flink-quickstart-java \</span><br><span class="line">  -DarchetypeVersion=$&#123;1:-1.8.0&#125; \</span><br><span class="line">  -DgroupId=org.myorg.quickstart \</span><br><span class="line">  -DartifactId=$PACKAGE	\</span><br><span class="line">  -Dversion=0.1 \</span><br><span class="line">  -Dpackage=org.myorg.quickstart \</span><br><span class="line">  -DinteractiveMode=false</span><br></pre></td></tr></table></figure>
<p>可以看到相比于第一种方式，该种方式只是直接指定好了 groupId ，artifactId ，version 等信息而已。</p>
<h3 id="2-2-使用-IDEA-构建">2.2 使用 IDEA 构建</h3>
<p>如果你使用的是开发工具是 IDEA ，可以直接在项目创建页面选择 Maven Flink Archetype 进行项目初始化：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-maven.png"/> </div>
<p>如果你的 IDEA 没有上述 Archetype， 可以通过点击右上角的 <code>ADD ARCHETYPE</code> ，来进行添加，依次填入所需信息，这些信息都可以从上述的 <code>archetype:generate </code> 语句中获取。点击  <code>OK</code> 保存后，该 Archetype 就会一直存在于你的 IDEA 中，之后每次创建项目时，只需要直接选择该 Archetype 即可：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-maven-new.png"/> </div>
<p>选中 Flink Archetype ，然后点击 <code>NEXT</code> 按钮，之后的所有步骤都和正常的 Maven 工程相同。</p>
<h2 id="三、项目结构">三、项目结构</h2>
<h3 id="3-1-项目结构">3.1 项目结构</h3>
<p>创建完成后的自动生成的项目结构如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-basis-project.png"/> </div>
<p>其中 BatchJob 为批处理的样例代码，源码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchJob</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">      ....</span><br><span class="line">    env.execute(<span class="string">&quot;Flink Batch Scala API Skeleton&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>getExecutionEnvironment 代表获取批处理的执行环境，如果是本地运行则获取到的就是本地的执行环境；如果在集群上运行，得到的就是集群的执行环境。如果想要获取流处理的执行环境，则只需要将 <code>ExecutionEnvironment</code> 替换为 <code>StreamExecutionEnvironment</code>， 对应的代码样例在 StreamingJob 中：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamingJob</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">      ...</span><br><span class="line">    env.execute(<span class="string">&quot;Flink Streaming Scala API Skeleton&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>需要注意的是对于流处理项目 <code>env.execute()</code> 这句代码是必须的，否则流处理程序就不会被执行，但是对于批处理项目则是可选的。</p>
<h3 id="3-2-主要依赖">3.2 主要依赖</h3>
<p>基于 Maven 骨架创建的项目主要提供了以下核心依赖：其中 <code>flink-scala</code> 用于支持开发批处理程序 ；<code>flink-streaming-scala</code> 用于支持开发流处理程序 ；<code>scala-library</code> 用于提供 Scala 语言所需要的类库。如果在使用 Maven 骨架创建时选择的是 Java 语言，则默认提供的则是 <code>flink-java</code> 和 <code>flink-streaming-java</code> 依赖。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Apache Flink dependencies --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- These dependencies are provided, because they should not be packaged into the JAR file. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Scala Library, provided by Flink as well. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>需要特别注意的以上依赖的 <code>scope</code> 标签全部被标识为 provided ，这意味着这些依赖都不会被打入最终的 JAR 包。因为 Flink 的安装包中已经提供了这些依赖，位于其 lib 目录下，名为  <code>flink-dist_*.jar</code>  ，它包含了 Flink 的所有核心类和依赖：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-lib.png"/> </div>
<p><code>scope</code> 标签被标识为 provided 会导致你在 IDEA 中启动项目时会抛出 ClassNotFoundException 异常。基于这个原因，在使用 IDEA 创建项目时还自动生成了以下 profile 配置：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- This profile helps to make things run out of the box in IntelliJ --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Its adds Flink&#x27;s core classes to the runtime class path. --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Otherwise they are missing in IntelliJ, because the dependency is &#x27;provided&#x27; --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">profiles</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">profile</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>add-dependencies-for-IDEA<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">activation</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>idea.version<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">activation</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">profiles</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>在 id 为 <code>add-dependencies-for-IDEA</code> 的 profile 中，所有的核心依赖都被标识为 compile，此时你可以无需改动任何代码，只需要在 IDEA 的 Maven 面板中勾选该 profile，即可直接在 IDEA 中运行 Flink 项目：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-maven-profile.png"/> </div>
<h2 id="四、词频统计案例">四、词频统计案例</h2>
<p>项目创建完成后，可以先书写一个简单的词频统计的案例来尝试运行 Flink 项目，以下以 Scala 语言为例，分别介绍流处理程序和批处理程序的编程示例：</p>
<h3 id="4-1-批处理示例">4.1 批处理示例</h3>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountBatch</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> benv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> dataSet = benv.readTextFile(<span class="string">&quot;D:\\wordcount.txt&quot;</span>)</span><br><span class="line">    dataSet.flatMap &#123; _.toLowerCase.split(<span class="string">&quot;,&quot;</span>)&#125;</span><br><span class="line">            .filter (_.nonEmpty)</span><br><span class="line">            .map &#123; (_, <span class="number">1</span>) &#125;</span><br><span class="line">            .groupBy(<span class="number">0</span>)</span><br><span class="line">            .sum(<span class="number">1</span>)</span><br><span class="line">            .print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中 <code>wordcount.txt</code> 中的内容如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a,a,a,a,a</span><br><span class="line">b,b,b</span><br><span class="line">c,c</span><br><span class="line">d,d</span><br></pre></td></tr></table></figure>
<p>本机不需要配置其他任何的 Flink 环境，直接运行 Main 方法即可，结果如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-word-count.png"/> </div>
<h3 id="4-2-流处理示例">4.2 流处理示例</h3>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountStreaming</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> senv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">String</span>] = senv.socketTextStream(<span class="string">&quot;192.168.0.229&quot;</span>, <span class="number">9999</span>, &#x27;\n&#x27;)</span><br><span class="line">    dataStream.flatMap &#123; line =&gt; line.toLowerCase.split(<span class="string">&quot;,&quot;</span>) &#125;</span><br><span class="line">              .filter(_.nonEmpty)</span><br><span class="line">              .map &#123; word =&gt; (word, <span class="number">1</span>) &#125;</span><br><span class="line">              .keyBy(<span class="number">0</span>)</span><br><span class="line">              .timeWindow(<span class="type">Time</span>.seconds(<span class="number">3</span>))</span><br><span class="line">              .sum(<span class="number">1</span>)</span><br><span class="line">              .print()</span><br><span class="line">    senv.execute(<span class="string">&quot;Streaming WordCount&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里以监听指定端口号上的内容为例，使用以下命令来开启端口服务：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nc -lk 9999</span><br></pre></td></tr></table></figure>
<p>之后输入测试数据即可观察到流处理程序的处理情况。</p>
<h2 id="五、使用-Scala-Shell">五、使用 Scala Shell</h2>
<p>对于日常的 Demo 项目，如果你不想频繁地启动 IDEA 来观察测试结果，可以像 Spark 一样，直接使用 Scala Shell 来运行程序，这对于日常的学习来说，效果更加直观，也更省时。Flink 安装包的下载地址如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">https://flink.apache.org/downloads.html</span><br></pre></td></tr></table></figure>
<p>Flink 大多数版本都提供有 Scala 2.11 和 Scala 2.12 两个版本的安装包可供下载：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-download.png"/> </div>
<p>下载完成后进行解压即可，Scala Shell 位于安装目录的 bin 目录下，直接使用以下命令即可以本地模式启动：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./start-scala-shell.sh local</span><br></pre></td></tr></table></figure>
<p>命令行启动完成后，其已经提供了批处理 （benv 和 btenv）和流处理（senv 和 stenv）的运行环境，可以直接运行 Scala Flink 程序，示例如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-scala-shell.png"/> </div>
<p>最后解释一个常见的异常：这里我使用的 Flink 版本为 1.9.1，启动时会抛出如下异常。这里因为按照官方的说明，目前所有 Scala 2.12 版本的安装包暂时都不支持 Scala Shell，所以如果想要使用 Scala Shell，只能选择 Scala 2.11 版本的安装包。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 bin]# ./start-scala-shell.sh local</span><br><span class="line">错误: 找不到或无法加载主类 org.apache.flink.api.scala.FlinkShell</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink核心概念综述</title>
    <url>/2022/03/19/Flink%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<h2 id="一、Flink-简介">一、Flink 简介</h2>
<p>Apache Flink 诞生于柏林工业大学的一个研究性项目，原名 StratoSphere 。2014 年，由 StratoSphere 项目孵化出 Flink，并于同年捐赠 Apache，之后成为 Apache 的顶级项目。2019 年 1 年，阿里巴巴收购了 Flink 的母公司 Data Artisans，并宣布开源内部的 Blink，Blink 是阿里巴巴基于 Flink 优化后的版本，增加了大量的新功能，并在性能和稳定性上进行了各种优化，经历过阿里内部多种复杂业务的挑战和检验。同时阿里巴巴也表示会逐步将这些新功能和特性 Merge 回社区版本的 Flink 中，因此 Flink 成为目前最为火热的大数据处理框架。</p>
<p>简单来说，Flink 是一个分布式的流处理框架，它能够对有界和无界的数据流进行高效的处理。Flink 的核心是流处理，当然它也能支持批处理，Flink 将批处理看成是流处理的一种特殊情况，即数据流是有明确界限的。这和 Spark Streaming 的思想是完全相反的，Spark Streaming 的核心是批处理，它将流处理看成是批处理的一种特殊情况， 即把数据流进行极小粒度的拆分，拆分为多个微批处理。</p>
<p>Flink 有界数据流和无界数据流：</p>
<div align="center"> <img width="600px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-bounded-unbounded.png"/> </div>
<p>Spark Streaming 数据流的拆分：</p>
<div align="center"> <img width="600px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/streaming-flow.png"/> </div>
<h2 id="二、Flink-核心架构">二、Flink 核心架构</h2>
<p>Flink 采用分层的架构设计，从而保证各层在功能和职责上的清晰。如下图所示，由上而下分别是 API &amp; Libraries 层、Runtime 核心层以及物理部署层：</p>
<div align="center"> <img width="600px"  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-stack.png"/> </div>
<h3 id="2-1-API-Libraries-层">2.1 API &amp; Libraries 层</h3>
<p>这一层主要提供了编程 API 和 顶层类库：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>编程 API : 用于进行流处理的 DataStream API 和用于进行批处理的 DataSet API；</p>
</li>
<li class="lvl-2">
<p>顶层类库：包括用于复杂事件处理的 CEP 库；用于结构化数据查询的 SQL &amp; Table 库，以及基于批处理的机器学习库 FlinkML 和 图形处理库 Gelly。</p>
</li>
</ul>
<h3 id="2-2-Runtime-核心层">2.2 Runtime 核心层</h3>
<p>这一层是 Flink 分布式计算框架的核心实现层，包括作业转换，任务调度，资源分配，任务执行等功能，基于这一层的实现，可以在流式引擎下同时运行流处理程序和批处理程序。</p>
<h3 id="2-3-物理部署层">2.3 物理部署层</h3>
<p>Flink 的物理部署层，用于支持在不同平台上部署运行 Flink 应用。</p>
<h2 id="三、Flink-分层-API">三、Flink 分层 API</h2>
<p>在上面介绍的 API &amp; Libraries 这一层，Flink 又进行了更为具体的划分。具体如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-api-stack.png"/> </div>
<p>按照如上的层次结构，API 的一致性由下至上依次递增，接口的表现能力由下至上依次递减，各层的核心功能如下：</p>
<h3 id="3-1-SQL-Table-API">3.1 SQL &amp; Table API</h3>
<p>SQL &amp; Table API 同时适用于批处理和流处理，这意味着你可以对有界数据流和无界数据流以相同的语义进行查询，并产生相同的结果。除了基本查询外， 它还支持自定义的标量函数，聚合函数以及表值函数，可以满足多样化的查询需求。</p>
<h3 id="3-2-DataStream-DataSet-API">3.2 DataStream &amp; DataSet API</h3>
<p>DataStream &amp;  DataSet API 是 Flink 数据处理的核心 API，支持使用 Java 语言或 Scala 语言进行调用，提供了数据读取，数据转换和数据输出等一系列常用操作的封装。</p>
<h3 id="3-3-Stateful-Stream-Processing">3.3 Stateful Stream Processing</h3>
<p>Stateful Stream Processing 是最低级别的抽象，它通过 Process Function 函数内嵌到 DataStream API 中。 Process Function 是 Flink 提供的最底层 API，具有最大的灵活性，允许开发者对于时间和状态进行细粒度的控制。</p>
<h2 id="四、Flink-集群架构">四、Flink 集群架构</h2>
<h3 id="4-1-核心组件">4.1  核心组件</h3>
<p>按照上面的介绍，Flink 核心架构的第二层是 Runtime 层， 该层采用标准的 Master - Slave 结构， 其中，Master 部分又包含了三个核心组件：Dispatcher、ResourceManager 和 JobManager，而 Slave 则主要是 TaskManager 进程。它们的功能分别如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>JobManagers</strong> (也称为 <em>masters</em>) ：JobManagers 接收由 Dispatcher 传递过来的执行程序，该执行程序包含了作业图 (JobGraph)，逻辑数据流图 (logical dataflow graph) 及其所有的 classes 文件以及第三方类库 (libraries) 等等 。紧接着 JobManagers 会将 JobGraph 转换为执行图 (ExecutionGraph)，然后向 ResourceManager 申请资源来执行该任务，一旦申请到资源，就将执行图分发给对应的 TaskManagers 。因此每个作业 (Job) 至少有一个 JobManager；高可用部署下可以有多个 JobManagers，其中一个作为 <em>leader</em>，其余的则处于 <em>standby</em> 状态。</p>
</li>
<li class="lvl-2">
<p><strong>TaskManagers</strong> (也称为 <em>workers</em>) : TaskManagers 负责实际的子任务 (subtasks) 的执行，每个 TaskManagers 都拥有一定数量的 slots。Slot 是一组固定大小的资源的合集 (如计算能力，存储空间)。TaskManagers 启动后，会将其所拥有的 slots 注册到 ResourceManager 上，由 ResourceManager 进行统一管理。</p>
</li>
<li class="lvl-2">
<p><strong>Dispatcher</strong>：负责接收客户端提交的执行程序，并传递给 JobManager 。除此之外，它还提供了一个 WEB UI 界面，用于监控作业的执行情况。</p>
</li>
<li class="lvl-2">
<p><strong>ResourceManager</strong> ：负责管理 slots 并协调集群资源。ResourceManager 接收来自 JobManager 的资源请求，并将存在空闲 slots 的 TaskManagers 分配给 JobManager 执行任务。Flink 基于不同的部署平台，如 YARN , Mesos，K8s 等提供了不同的资源管理器，当 TaskManagers 没有足够的 slots 来执行任务时，它会向第三方平台发起会话来请求额外的资源。</p>
</li>
</ul>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-application-submission.png"/> </div>
<h3 id="4-2-Task-SubTask">4.2  Task &amp; SubTask</h3>
<p>上面我们提到：TaskManagers 实际执行的是 SubTask，而不是 Task，这里解释一下两者的区别：</p>
<p>在执行分布式计算时，Flink 将可以链接的操作 (operators) 链接到一起，这就是 Task。之所以这样做， 是为了减少线程间切换和缓冲而导致的开销，在降低延迟的同时可以提高整体的吞吐量。 但不是所有的 operator 都可以被链接，如下 keyBy 等操作会导致网络 shuffle 和重分区，因此其就不能被链接，只能被单独作为一个 Task。  简单来说，一个 Task 就是一个可以链接的最小的操作链 (Operator Chains) 。如下图，source 和 map 算子被链接到一块，因此整个作业就只有三个 Task：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-task-subtask.png"/> </div>
<p>解释完 Task ，我们在解释一下什么是 SubTask，其准确的翻译是： <em>A subtask is one parallel slice of a task</em>，即一个 Task 可以按照其并行度拆分为多个 SubTask。如上图，source &amp; map 具有两个并行度，KeyBy 具有两个并行度，Sink 具有一个并行度，因此整个虽然只有 3 个 Task，但是却有 5 个 SubTask。Jobmanager 负责定义和拆分这些 SubTask，并将其交给 Taskmanagers 来执行，每个 SubTask 都是一个单独的线程。</p>
<h3 id="4-3-资源管理">4.3  资源管理</h3>
<p>理解了 SubTasks ，我们再来看看其与 Slots 的对应情况。一种可能的分配情况如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-tasks-slots.png"/> </div>
<p>这时每个 SubTask 线程运行在一个独立的 TaskSlot， 它们共享所属的 TaskManager 进程的TCP 连接（通过多路复用技术）和心跳信息 (heartbeat messages)，从而可以降低整体的性能开销。此时看似是最好的情况，但是每个操作需要的资源都是不尽相同的，这里假设该作业 keyBy 操作所需资源的数量比 Sink 多很多 ，那么此时 Sink 所在 Slot 的资源就没有得到有效的利用。</p>
<p>基于这个原因，Flink 允许多个 subtasks 共享 slots，即使它们是不同 tasks 的 subtasks，但只要它们来自同一个 Job 就可以。假设上面 souce &amp; map 和 keyBy 的并行度调整为 6，而 Slot 的数量不变，此时情况如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-subtask-slots.png"/> </div>
<p>可以看到一个 Task Slot 中运行了多个 SubTask 子任务，此时每个子任务仍然在一个独立的线程中执行，只不过共享一组 Sot 资源而已。那么 Flink 到底如何确定一个 Job 至少需要多少个 Slot 呢？Flink 对于这个问题的处理很简单，默认情况一个 Job 所需要的 Slot 的数量就等于其 Operation 操作的最高并行度。如下， A，B，D 操作的并行度为 4，而 C，E 操作的并行度为 2，那么此时整个 Job 就需要至少四个 Slots 来完成。通过这个机制，Flink 就可以不必去关心一个 Job 到底会被拆分为多少个 Tasks 和 SubTasks。</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-task-parallelism.png"/> </div>
<h3 id="4-4-组件通讯">4.4 组件通讯</h3>
<p>Flink 的所有组件都基于 Actor System 来进行通讯。Actor system是多种角色的 actor 的容器，它提供调度，配置，日志记录等多种服务，并包含一个可以启动所有 actor 的线程池，如果 actor 是本地的，则消息通过共享内存进行共享，但如果 actor 是远程的，则通过 RPC 的调用来传递消息。</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-process.png"/> </div>
<h2 id="五、Flink-的优点">五、Flink 的优点</h2>
<p>最后基于上面的介绍，来总结一下 Flink 的优点：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>Flink 是基于事件驱动 (Event-driven) 的应用，能够同时支持流处理和批处理；</p>
</li>
<li class="lvl-2">
<p>基于内存的计算，能够保证高吞吐和低延迟，具有优越的性能表现；</p>
</li>
<li class="lvl-2">
<p>支持精确一次 (Exactly-once) 语意，能够完美地保证一致性和正确性；</p>
</li>
<li class="lvl-2">
<p>分层 API ，能够满足各个层次的开发需求；</p>
</li>
<li class="lvl-2">
<p>支持高可用配置，支持保存点机制，能够提供安全性和稳定性上的保证；</p>
</li>
<li class="lvl-2">
<p>多样化的部署方式，支持本地，远端，云端等多种部署方案；</p>
</li>
<li class="lvl-2">
<p>具有横向扩展架构，能够按照用户的需求进行动态扩容；</p>
</li>
<li class="lvl-2">
<p>活跃度极高的社区和完善的生态圈的支持。</p>
</li>
</ul>
<h2 id="参考资料-6">参考资料</h2>
<ul class="lvl-0">
<li class="lvl-2">
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/concepts/programming-model.html">Dataflow Programming Model</a></p>
</li>
<li class="lvl-2">
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/concepts/runtime.html">Distributed Runtime Environment</a></p>
</li>
<li class="lvl-3">
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/internals/components.html">Component Stack</a></p>
</li>
<li class="lvl-2">
<p>Fabian Hueske , Vasiliki Kalavri . 《Stream Processing with Apache Flink》.  O’Reilly Media .  2019-4-30</p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink状态管理与Checkpoint机制(一)</title>
    <url>/2023/01/03/Flink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E4%B8%8ECheckpoint%E6%9C%BA%E5%88%B6-%E4%B8%80/</url>
    <content><![CDATA[<h3 id="何为状态？">何为状态？</h3>
<p>计算任务的结果不仅仅依赖于输入，还依赖于它的当前状态，其实大多数的计算都是有状态的计算。比如wordcount,给一些word,其计算它的count,这是一个很常见的业务场景。count做为输出，在计算的过程中要不断的把输入累加到count上去，那么count就是一个state。</p>
<p>在批处理过程中，数据是划分为块分片去完成的，然后每一个Task去处理一个分片。当分片执行完成后，把输出聚合起来就是最终的结果。在这个过程当中，对于state的需求还是比较小的。</p>
<p>在流处理过程中，对State有非常高的要求，因为在流系统中输入是一个无限制的流，会持续运行从不间断。在这个过程当中，就需要将状态数据很好的管理起来</p>
<h3 id="检查点checkpoint与Barrier">检查点checkpoint与Barrier</h3>
<p>checkpoint【可以理解为checkpoint是把state数据持久化存储了】，则表示Flink job在一个特定时刻的一份全局状态快照，即包含了所有的task/operator的状态。</p>
<p>Checkpoint是Flink实现容错机制最核心的功能，它能根据配置周期性地基于Stream中各个Operator/Task的状态来生成快照，从而将这些状态数据定期持久化存储下来，当Flink程序一旦意外崩溃时，重新运行程序时可以有选择地从这些快照进行恢复，从而修正因为故障带来的程序数据异常。</p>
<p>Flink分布式快照算法Asynchronous Barrier Snapshots算法借鉴了经典的Chandy-Lamport算法的主要思想，同时做了一些改进。<a href="https://arxiv.org/abs/1506.08603">Lightweight Asynchronous Snapshots for Distributed Dataflows</a></p>
<h4 id="Chandy-Lamport算法">Chandy-Lamport算法</h4>
<p>分布式系统是一个包含有限进程和有限消息通道的系统，这些进程和通道可以用一个有向图描述，其中节点表示进程，边表示通道。如下图所示：p、q分别是进程，c-&gt;c’则是消息通道，分布式系统快照是了保存分布式系统的state。分布式系统State是由进程状态和通道状态组成的。</p>
<p><img src="https://gitee.com/liurio/image_save/raw/master/flink/chandy-lamport%E7%AE%97%E6%B3%95.jpg" alt="img"></p>
<ul class="lvl-0">
<li class="lvl-2">
<p>Event：分布式系统中发生的一个事件，在类似于Flink这样的分布式计算系统中从Source输入的新消息相当于一个事件</p>
</li>
<li class="lvl-2">
<p>进程状态：包含一个初始状态（initial state)，和持续发生的若干Events。初始状态可以理解为Flink中刚启动的计算节点，计算节点每处理一条Event，就转换到一个新的状态。</p>
</li>
<li class="lvl-2">
<p>通道状态：我们用在通道上传输的消息（Event）来描述一个通道的状态。</p>
</li>
</ul>
<p>进程p启动这个算法，记录自身状态，并发出Marker。随着Marker不断的沿着分布式系统的相连通道逐渐传输到所有的进程，所有的进程都会执行算法以记录自身状态和入射通道的状态，**待到所有进程执行完该算法，一个分布式Snapshot就完成了记录。**Marker相当于是一个信使，它随着消息流流经所有的进程，通知每个进程记录自身状态。且Marker对整个分布式系统的计算过程没有任何影响。只要保证Marker能在有限时间内通过通道传输到进程，每个进程能够在有限时间内完成自身状态的记录，这个算法就能在有限的时间内执行完成。</p>
<h4 id="Flink分布式快照算法ABS">Flink分布式快照算法ABS</h4>
<p>在ABS算法中用<strong>Barrier</strong>代替了C-L算法中的Marker。</p>
<ol>
<li class="lvl-3">
<p>Barrier周期性的被注入到所有的Source中，Source节点看到Barrier之后，就会立即记录自己的状态，然后将Barrier发送到Transformation Operator。</p>
</li>
<li class="lvl-3">
<p>当Operator从某个input channel收到Barrier之后，会立即Block住这条通道，直到收到所有的input channel的Barrier，此时Operator会记录自身状态，并向自己所有的output channel广播Barrier。</p>
</li>
<li class="lvl-3">
<p>Sink接受Barrier的操作流程与Transformation Operator一样。当所有的Barrier都到达Sink之后，并且所有的Sink也完成了Checkpoint，这一轮Snapshot就完成了。</p>
</li>
</ol>
<p><img src="https://gitee.com/liurio/image_save/raw/master/flink/abs%E7%AE%97%E6%B3%95.jpg" alt="img"></p>
<p>上述算法中Block Input实际上是有负面效果的，一旦某个input channel发生延迟，Barrier迟迟未到，就会导致Operator上的其他通道全部堵塞，导致系统吞吐下降。但有个一好处是可以实现<strong>Exactly Once</strong>。</p>
<p>一轮快照整个执行流程如下所示：</p>
<p><img src="http://chenyuzhao.me/2018/01/29/Flink-%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E7%9A%84%E8%AE%BE%E8%AE%A1-%E6%B5%81%E7%A8%8B/flink_ckp_flow.png" alt="flink_ckp_flow"></p>
<p>Checkpoint统一由JobManager发起，中间涉及到JobManager和TaskManager的交互，一轮快照可以分为4个阶段：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>JobManager checkpoint的发起</p>
</li>
</ul>
<blockquote>
<p>全局协调控制的核心抽象是CheckpointCoordinator，发起时的checkpoint被抽象成PendingCheckpoint，向所有的Source节点发送barrier。图中第一步</p>
</blockquote>
<ul class="lvl-0">
<li class="lvl-2">
<p>barrier的传递</p>
</li>
</ul>
<blockquote>
<p>当operator收到所有input channel的barrier之后，将barrier传递给下一个operator/task。图中第二步</p>
</blockquote>
<ul class="lvl-0">
<li class="lvl-2">
<p>operator/task的checkpoint</p>
</li>
</ul>
<blockquote>
<p>当operator/task收到所有input channels的barrier，本地计算完成后，进行状态持久化。图中第三步</p>
</blockquote>
<ul class="lvl-0">
<li class="lvl-2">
<p>ack消息回传</p>
</li>
</ul>
<blockquote>
<p>当TaskManager完成本地备份之后，并将数据的地址以及快照句柄等通过akka以ack消息的形式发送给CheckpointCoordinator，由其负责维护这一轮快照的全局状态视图。当CheckpointCoordinator 收到所有的ack消息后，此时checkpoint的状态由PendingCheckpoint变为 CompletedCheckpoint。此时一次checkpoint完成。图中剩余步骤</p>
</blockquote>
<h5 id="单流">单流</h5>
<p><img src="http://chenyuzhao.me/2018/01/29/Flink-%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E7%9A%84%E8%AE%BE%E8%AE%A1-%E6%B5%81%E7%A8%8B/stream_barriers.svg" alt="stream_barriers"></p>
<ul class="lvl-0">
<li class="lvl-2">
<p>每个Barrier携带着快照的ID，快照记录着ID，并将其放在快照数据的前面。</p>
</li>
<li class="lvl-2">
<p>单流时两个Barrier之间的数据，存储在相应的barrierID中，例如barrier n-1和n之间的数据存储在Barrier n中。</p>
</li>
</ul>
<h5 id="多流">多流</h5>
<p><img src="http://chenyuzhao.me/2018/01/29/Flink-%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E7%9A%84%E8%AE%BE%E8%AE%A1-%E6%B5%81%E7%A8%8B/stream_aligning.svg" alt="stream_aligning"></p>
<ul class="lvl-0">
<li class="lvl-2">
<p>比如此operator有两个输入流，当收到第一个流的barrier n时，下一个流的barrier n-1还有数据流入，此时会先临时搁置此流的数据，将数据放入缓存buffer中，即1 2 3临时存储起来。待所有输入通道都收到了barrier n时，此时所有之前的数据都是barrier n-1的数据。然后该operator会释放buffer中的数据，继续处理。</p>
</li>
<li class="lvl-2">
<p>虽然该方法有效的实现了Exactly Once，但是一旦某个input channel发生延迟，Barrier迟迟未到，这会导致Transformation Operator上的其它通道全部堵塞，系统吞吐大幅下降。Flink提供了选项，可以关闭Exactly once并仅保留at least once。</p>
</li>
</ul>
<h3 id="checkpoint的存储">checkpoint的存储</h3>
<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-07-093258.jpg" alt="img"></p>
<p>用户可以根据自己的需求选择，如果数据量较小，可以存放到MemoryStateBackend和FsStateBackend中，如果数据量较大，可以放到RockDB中。</p>
<h4 id="HeapStateBackend">HeapStateBackend</h4>
<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-07-093320.jpg" alt="img"></p>
<p>MemoryStateBackend 和 FsStateBackend 都是存储在内存中，被保存在一个由多层Java Map嵌套而成的数据结构中，默认不超过5M。优点：速度快，缺点：容量小</p>
<h4 id="RocksDBStateBackend">RocksDBStateBackend</h4>
<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-07-093345.jpg" alt="img"></p>
<p>RockDBKeyedStateBackend 每个State单独存储在一个ColumnFamily中。会在本地文件系统中维护状态，state会直接写入本地rocksdb中。同时RocksDB需要配置一个远端的filesystem。uri（一般是HDFS），在做checkpoint的时候，会把本地的数据直接复制到filesystem中。fail over的时候从filesystem中恢复到本地。RocksDB克服了state受内存限制的缺点，同时又能够持久化到远端文件系统中，比较适合在生产中使用。</p>
<h5 id="RocksDB全量快照">RocksDB全量快照</h5>
<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-07-093447.jpg" alt="img"></p>
<p>全量checkpoint会在每个节点做备份数据时，需要将数据都遍历一遍，然后写入到外部存储中，这种情况会影响备份性能。</p>
<p>RocksDB 自身的snapshot 全量写出，主要步骤如下：</p>
<ol>
<li class="lvl-3">
<p>拿到RocksDB 自身的 snapshot 对象</p>
</li>
<li class="lvl-3">
<p>通过 CheckpointStreamFactory 拿到 CheckpointStateOutputStream 作为快照写出流</p>
</li>
<li class="lvl-3">
<p>分别将快照的 meta 信息和数据写到 2 对应的输出流中</p>
</li>
<li class="lvl-3">
<p>拿到 2 输出流的句柄，获取状态offset，将 k-v 数据读取到RocksDB中，这里要注意的是快照时留下的 meta 起始标志位【标志一个新的 state 起始或者一个 keyGroup 结束】，快照恢复时需要复原.</p>
</li>
<li class="lvl-3">
<p>将 RocksDB 的快照对象及一些辅助资源释放</p>
</li>
</ol>
<p><img src="http://chenyuzhao.me/2017/12/24/Flink-%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E7%9A%84%E8%AE%BE%E8%AE%A1-%E5%AD%98%E5%82%A8/rocksdb-full-spt-data-write-format.png" alt="rocksdb-full-spt-data-write-format.png"></p>
<h5 id="RocksDB增量快照">RocksDB增量快照</h5>
<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-07-093511.jpg" alt="img"></p>
<p>RocksDB 的数据会更新到内存，当内存满时，会写入到磁盘中。增量的机制会将新产生的文件copy持久化中，而之前产生的文件就不需要COPY到持久化中了。这种方式减少了COPY的数据量，并提高性能。</p>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink状态管理与checkpoint容错机制(二)</title>
    <url>/2023/01/03/Flink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E4%B8%8Echeckpoint%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6-%E4%BA%8C/</url>
    <content><![CDATA[<h3 id="状态分类">状态分类</h3>
<p>Flink支持两种状态<code>Keyed State</code>和<code>Operator State</code>。两类状态又都包括原始状态<code>row state</code>和托管状态<code>managed state</code>。</p>
<blockquote>
<p>原始状态：由用户自行管理状态具体的数据结构，框架在做checkpoint的时候，使用byte[]来读写状态内容，对其内部数据结构一无所知。当实现一个用户自定义的operator时，会使用到原始状态</p>
</blockquote>
<blockquote>
<p>托管状态是由Flink框架管理的状态，通常在DataStream上的状态推荐使用托管的状态。</p>
</blockquote>
<h3 id="Keyed-State">Keyed State</h3>
<p>该类状态是基于KeyedStream上的状态，这个状态是根据特定的key绑定的，对keyedStream流上的每一个key，都对应着一个state。<code>stream.keyBy(...)</code></p>
<p><strong>数据结构：</strong></p>
<ul class="lvl-0">
<li class="lvl-2">
<p>ValueState: 即类型为T的单值状态。这个状态与对应的key绑定，是最简单的状态了。它可以通过update方法更新状态值，通过value()方法获取状态值</p>
</li>
<li class="lvl-2">
<p>ListState: 即key上的状态值为一个列表。可以通过add方法往列表中附加值；也可以通过get()方法返回一个Iterable来遍历状态值。</p>
</li>
<li class="lvl-2">
<p>ReducingState:这种状态通过用户传入的reduceFunction，每次调用add方法添加值的时候，会调用reduceFunction，最后合并到一个单一的状态值。</p>
</li>
<li class="lvl-2">
<p>MapState&lt;UK, UV&gt;:即状态值为一个map。用户通过put或putAll方法添加元素。</p>
</li>
</ul>
<p>通过<code>value()</code>获取值，通过<code>update()</code>更新值，Keyed State继承RichFunction类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">int</span> <span class="variable">sourceCount</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span>  StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//打开并设置checkpoint</span></span><br><span class="line">    <span class="comment">// 1.设置checkpoint目录，这里我用的是本地路径，记得本地路径要file开头</span></span><br><span class="line">    <span class="comment">// 2.设置checkpoint类型，at lease onece or EXACTLY_ONCE</span></span><br><span class="line">    <span class="comment">// 3.设置间隔时间，同时打开checkpoint功能，该大小的设置很重要，设置过短可能会导致flink一直处于checkpoint状态，过长会影响效果</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="title class_">FsStateBackend</span>(<span class="string">&quot;file:///D://hadoop//data//checkpoint&quot;</span>));</span><br><span class="line">    env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">    env.getCheckpointConfig().setCheckpointInterval(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">    DataStream&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; source = env.addSource(<span class="keyword">new</span> <span class="title class_">KeyedStateSource</span>());</span><br><span class="line">    source.keyBy(<span class="number">0</span>)</span><br><span class="line">        .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">2</span>)))</span><br><span class="line">        <span class="comment">//窗口函数，比如是richwindowsfunction 否侧无法使用manage state</span></span><br><span class="line">        .apply(<span class="keyword">new</span> <span class="title class_">KeyedStateRichFunction</span>());</span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">	新建数据源</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">KeyedStateSource</span> <span class="keyword">implements</span> <span class="title class_">SourceFunction</span>&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Boolean</span> <span class="variable">isRunning</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(SourceContext&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; sourceContext)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">while</span>(isRunning)&#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">                sourceContext.collect(Tuple3.of(<span class="number">1</span>,<span class="string">&quot;ahah&quot;</span>,count));</span><br><span class="line">                count++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(count&gt;<span class="number">100</span>)&#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;err_________________&quot;</span>);</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">Exception</span>(<span class="string">&quot;123&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//                System.out.println(&quot;source:&quot;+count);</span></span><br><span class="line">            sourceCount = count;</span><br><span class="line">            Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">cancel</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">KeyedStateRichFunction</span> <span class="keyword">extends</span> <span class="title class_">RichWindowFunction</span>&lt;Tuple3&lt;Integer,String,Integer&gt;, Integer, Tuple, TimeWindow&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ValueState&lt;Integer&gt; state;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">apply</span><span class="params">(Tuple tuple, TimeWindow timeWindow, Iterable&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; iterable, Collector&lt;Integer&gt; collector)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">//从state中获取值，获取上一个结果值</span></span><br><span class="line">        count=state.value();</span><br><span class="line">        <span class="keyword">for</span>(Tuple3&lt;Integer, String, Integer&gt; item : iterable)&#123;</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//更新state值</span></span><br><span class="line">        state.update(count);</span><br><span class="line">        System.out.println(<span class="string">&quot;windows count:&quot;</span>+count+<span class="string">&quot;   all count:&quot;</span> + sourceCount);</span><br><span class="line">        collector.collect(count);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取state</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">//            System.out.println(&quot;##open&quot;);</span></span><br><span class="line">        ValueStateDescriptor&lt;Integer&gt; descriptor =</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">ValueStateDescriptor</span>&lt;Integer&gt;(</span><br><span class="line">            <span class="string">&quot;average&quot;</span>, <span class="comment">// the state name</span></span><br><span class="line">            TypeInformation.of(<span class="keyword">new</span> <span class="title class_">TypeHint</span>&lt;Integer&gt;() &#123;&#125;), <span class="comment">// type information</span></span><br><span class="line">            <span class="number">0</span>);</span><br><span class="line">        state = getRuntimeContext().getState(descriptor);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>代码详情：</p>
<ol>
<li class="lvl-3">
<p>加载数据源，每次<code>count=10</code>休眠2s，当达到<code>count=100</code>时中断数据源，重新开始…</p>
</li>
<li class="lvl-3">
<p>并把window窗口大小设置2s负责触发计算，观察每次throw exception后，能不能从之前的结果开始算…</p>
</li>
</ol>
<p>输出：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">all count:<span class="number">10</span>   source count:<span class="number">10</span></span><br><span class="line">all count:<span class="number">20</span>   source count:<span class="number">20</span></span><br><span class="line">all count:<span class="number">30</span>   source count:<span class="number">30</span></span><br><span class="line">all count:<span class="number">40</span>   source count:<span class="number">40</span></span><br><span class="line">all count:<span class="number">50</span>   source count:<span class="number">50</span></span><br><span class="line">all count:<span class="number">60</span>   source count:<span class="number">60</span></span><br><span class="line">all count:<span class="number">70</span>   source count:<span class="number">70</span></span><br><span class="line">all count:<span class="number">80</span>   source count:<span class="number">80</span></span><br><span class="line">all count:<span class="number">90</span>   source count:<span class="number">90</span></span><br><span class="line">all count:<span class="number">100</span>   source count:<span class="number">100</span></span><br><span class="line">err_________________</span><br><span class="line">all count:<span class="number">110</span>   source count:<span class="number">10</span></span><br><span class="line">all count:<span class="number">120</span>   source count:<span class="number">20</span></span><br></pre></td></tr></table></figure>
<p>从结果可以看出达到了想要的效果，all count的值并没有从0开始计算，而是从之前的结果计算。</p>
<h3 id="Operator-State">Operator State</h3>
<p>该类State与key无关，整个operator对应一个state，该类State没有Keyed Key支持的数据结构多，仅支持ListState。举例来说，Flink中的Kafka Connector，就使用了operator state。它会在每个connector实例中，保存该实例中消费topic的所有(partition, offset)映射。</p>
<p>有两种方式：</p>
<h4 id="CheckpointedFunction-很少使用，需要自己初始化">CheckpointedFunction(很少使用，需要自己初始化)</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">	每当checkpoint执行的时候，snapshotState会被调用</span></span><br><span class="line"><span class="comment">**/</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">	该方法不仅可以用来初始化state，还可以用于处理state recovery的逻辑</span></span><br><span class="line"><span class="comment">**/</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">BufferingSink</span> <span class="keyword">implements</span> <span class="title class_">SinkFunction</span>&lt;Tuple2&lt;String, Integer&gt;&gt;,</span><br><span class="line">         CheckpointedFunction &#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> threshold;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState;</span><br><span class="line">  <span class="keyword">private</span> List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements;</span><br><span class="line">  <span class="keyword">public</span> <span class="title function_">BufferingSink</span><span class="params">(<span class="type">int</span> threshold)</span> &#123;</span><br><span class="line">    <span class="built_in">this</span>.threshold = threshold;</span><br><span class="line">    <span class="built_in">this</span>.bufferedElements = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line"> &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">invoke</span><span class="params">(Tuple2&lt;String, Integer&gt; value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    bufferedElements.add(value);</span><br><span class="line">    <span class="keyword">if</span> (bufferedElements.size() == threshold) &#123;</span><br><span class="line">      <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element: bufferedElements) &#123;</span><br><span class="line">        <span class="comment">// send it to the sink</span></span><br><span class="line">     &#125;</span><br><span class="line">      bufferedElements.clear();</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception</span><br><span class="line">&#123;</span><br><span class="line">    checkpointedState.clear();</span><br><span class="line">    <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element : bufferedElements) &#123;</span><br><span class="line">      checkpointedState.add(element);</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span></span><br><span class="line">Exception &#123;</span><br><span class="line">    ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class="line">      <span class="keyword">new</span> <span class="title class_">ListStateDescriptor</span>&lt;&gt;(</span><br><span class="line">        <span class="string">&quot;buffered-elements&quot;</span>,</span><br><span class="line">        TypeInformation.of(<span class="keyword">new</span> <span class="title class_">TypeHint</span>&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;&#125;));</span><br><span class="line">    checkpointedState =</span><br><span class="line">context.getOperatorStateStore().getListState(descriptor);</span><br><span class="line">    <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">      <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123;</span><br><span class="line">        bufferedElements.add(element);</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="ListCheckpointed-常用，Flink自动初始化">ListCheckpointed(常用，Flink自动初始化)</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;T&gt; <span class="title function_">snapshotState</span><span class="params">(<span class="type">long</span> checkpointId, <span class="type">long</span> timestamp)</span> <span class="keyword">throws</span> Exception;</span><br><span class="line"><span class="keyword">void</span> <span class="title function_">restoreState</span><span class="params">(List&lt;T&gt; state)</span> <span class="keyword">throws</span> Exception;</span><br></pre></td></tr></table></figure>
<p>仅支持list state，ListCheckpointed是CheckpointedFunction的限制版，它仅仅支持Even-splitredistribution模式的list-style state。ListCheckpointed定义了两个方法，分别是snapshotState方法及restoreState方法；snapshotState方法在master触发checkpoint的时候被调用，用户需要返回当前的状态，而restoreState方法会在failure recovery的时候被调用，传递的参数为List类型的state，方法里头可以将state恢复到本地.</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">int</span> <span class="variable">sourceCount</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">    <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span>  StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="title class_">FsStateBackend</span>(<span class="string">&quot;file:///D://hadoop//data//checkpoint&quot;</span>));</span><br><span class="line">    env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">    env.getCheckpointConfig().setCheckpointInterval(<span class="number">1000</span>);</span><br><span class="line">    DataStream&lt;Tuple4&lt;Integer,String,String,Integer&gt;&gt; source = env.addSource(<span class="keyword">new</span> <span class="title class_">OperatorStateSource</span>());</span><br><span class="line">    source.keyBy(<span class="number">0</span>)</span><br><span class="line">        .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">2</span>)))</span><br><span class="line">        .apply(<span class="keyword">new</span> <span class="title class_">OperatorStateAppy</span>());</span><br><span class="line">    env.execute(<span class="string">&quot;&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">	加载数据源，并且实现ListCheckPointed接口，没有key，一个operator就是一个state</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">OperatorStateSource</span> <span class="keyword">extends</span> <span class="title class_">RichSourceFunction</span>&lt;Tuple4&lt;Integer,String,String,Integer&gt;&gt; <span class="keyword">implements</span> <span class="title class_">ListCheckpointed</span>&lt;UserState&gt;&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span><span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">is_running</span> <span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> List&lt;UserState&gt; <span class="title function_">snapshotState</span><span class="params">(<span class="type">long</span> l, <span class="type">long</span> l1)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        List&lt;UserState&gt; userStateList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="type">UserState</span> <span class="variable">state</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">UserState</span>();</span><br><span class="line">        state.setCount(count);</span><br><span class="line">        userStateList.add(state);</span><br><span class="line">        <span class="keyword">return</span> userStateList;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">restoreState</span><span class="params">(List&lt;UserState&gt; list)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        count = list.get(<span class="number">0</span>).getCount();</span><br><span class="line">        System.out.println(<span class="string">&quot;OperatorStateSource restoreState: &quot;</span>+count);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(SourceContext&lt;Tuple4&lt;Integer, String, String, Integer&gt;&gt; sourceContext)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">while</span> (is_running)&#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">                sourceContext.collect(Tuple4.of(<span class="number">1</span>, <span class="string">&quot;hello-&quot;</span> + count, <span class="string">&quot;alphabet&quot;</span>, count));</span><br><span class="line">                count++;</span><br><span class="line">            &#125;</span><br><span class="line">            sourceCount = count;</span><br><span class="line">            Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">            <span class="keyword">if</span>(count&gt;=<span class="number">100</span>)&#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;err_________________&quot;</span>);</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">Exception</span>(<span class="string">&quot;exception made by ourself!&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">cancel</span><span class="params">()</span> &#123;</span><br><span class="line">        is_running = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">	下游计算operator，也实现了ListCheckpoint接口</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">OperatorStateAppy</span> <span class="keyword">implements</span> <span class="title class_">WindowFunction</span>&lt;Tuple4&lt;Integer,String,String,Integer&gt;,Integer,Tuple,TimeWindow&gt;,ListCheckpointed&lt;UserState&gt;&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="variable">total</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> List&lt;UserState&gt; <span class="title function_">snapshotState</span><span class="params">(<span class="type">long</span> l, <span class="type">long</span> l1)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        List&lt;UserState&gt; userStateList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="type">UserState</span> <span class="variable">state</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">UserState</span>();</span><br><span class="line">        state.setCount(total);</span><br><span class="line">        userStateList.add(state);</span><br><span class="line">        <span class="keyword">return</span> userStateList;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">restoreState</span><span class="params">(List&lt;UserState&gt; list)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        total = list.get(<span class="number">0</span>).getCount();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">apply</span><span class="params">(Tuple tuple, TimeWindow timeWindow, Iterable&lt;Tuple4&lt;Integer, String, String, Integer&gt;&gt; iterable, Collector&lt;Integer&gt; collector)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (Tuple4&lt;Integer,String,String,Integer&gt; data:iterable)&#123;</span><br><span class="line">            count ++;</span><br><span class="line">        &#125;</span><br><span class="line">        total = total + count;</span><br><span class="line">        System.out.println(<span class="string">&quot;all count:&quot;</span>+total+<span class="string">&quot;   source count:&quot;</span> + sourceCount);</span><br><span class="line">        collector.collect(total);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">UserState</span> <span class="keyword">implements</span> <span class="title class_">Serializable</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> count;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getCount</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setCount</span><span class="params">(<span class="type">int</span> count)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.count = count;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">all count:<span class="number">10</span>   source count:<span class="number">10</span></span><br><span class="line">all count:<span class="number">20</span>   source count:<span class="number">20</span></span><br><span class="line">all count:<span class="number">30</span>   source count:<span class="number">30</span></span><br><span class="line">all count:<span class="number">40</span>   source count:<span class="number">40</span></span><br><span class="line">all count:<span class="number">50</span>   source count:<span class="number">50</span></span><br><span class="line">all count:<span class="number">60</span>   source count:<span class="number">60</span></span><br><span class="line">all count:<span class="number">70</span>   source count:<span class="number">70</span></span><br><span class="line">all count:<span class="number">80</span>   source count:<span class="number">80</span></span><br><span class="line">all count:<span class="number">90</span>   source count:<span class="number">90</span></span><br><span class="line">all count:<span class="number">100</span>   source count:<span class="number">100</span></span><br><span class="line">err_________________</span><br><span class="line">OperatorStateSource restoreState: <span class="number">100</span></span><br><span class="line">all count:<span class="number">110</span>   source count:<span class="number">110</span></span><br></pre></td></tr></table></figure>
<p>从结果可以看出达到了想要的结果，当数据源中断后，调用了restore方法，恢复了state的值。</p>
<p><strong>总结：</strong></p>
<p>两者的区别，实现CheckpointedFunction接口，有两种形式的ListState API可以使用，分别是getListState以及getListUnionState，它们都会返回一个ListState，但是他们在重新分区的时候会有区别，后面会详细介绍。如果我们直接实现ListCheckpointed接口，那么就会规定使用ListState，不需要我们进行初始化，Flink内部帮我们解决。</p>
<h3 id="state重分区">state重分区</h3>
<p>当我们在一个job中重新设置了一个operator的并行度之后，之前的state该如何被分配呢？下面就ListState、ListUnionState以及BroadcastState来说明如何进行重分区。</p>
<p><img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/2019-07-07-093120.jpg" alt="img"></p>
<p>参考：</p>
<ol>
<li class="lvl-3">
<p><a href="https://www.jianshu.com/p/87a45238980a?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation">Fault Tolerant与保证Exactly- Once语义</a></p>
</li>
<li class="lvl-3">
<p><a href="http://chenyuzhao.me/2017/12/24/Flink-%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E7%9A%84%E8%AE%BE%E8%AE%A1-%E5%AD%98%E5%82%A8/">Flink 分布式快照的设计-存储</a></p>
</li>
<li class="lvl-3">
<p><a href="http://chenyuzhao.me/2018/01/29/Flink-%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E7%9A%84%E8%AE%BE%E8%AE%A1-%E6%B5%81%E7%A8%8B/">Flink-分布式快照的设计-流程</a></p>
</li>
<li class="lvl-3">
<p><a href="http://www.54tianzhisheng.cn/2019/06/18/flink-state/#">Flink状态管理和容错机制介绍</a></p>
</li>
<li class="lvl-3">
<p><a href="https://blog.csdn.net/u013560925/article/details/82667085">Flink 异常处理-State和Checkpoint实践</a></p>
</li>
<li class="lvl-3">
<p><a href="https://juejin.im/post/5bf93517f265da611510760d">Flink 状态管理与checkPoint数据容错机制深入剖析-Flink牛刀小试</a></p>
</li>
<li class="lvl-3">
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/state/checkpoints.html">https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/state/checkpoints.html</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink窗口模型</title>
    <url>/2022/03/19/Flink%E7%AA%97%E5%8F%A3%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="一、窗口概念">一、窗口概念</h2>
<p>在大多数场景下，我们需要统计的数据流都是无界的，因此我们无法等待整个数据流终止后才进行统计。通常情况下，我们只需要对某个时间范围或者数量范围内的数据进行统计分析：如每隔五分钟统计一次过去一小时内所有商品的点击量；或者每发生1000次点击后，都去统计一下每个商品点击率的占比。在 Flink 中，我们使用窗口 (Window) 来实现这类功能。按照统计维度的不同，Flink 中的窗口可以分为 时间窗口 (Time Windows) 和 计数窗口 (Count Windows) 。</p>
<h2 id="二、Time-Windows">二、Time Windows</h2>
<p>Time Windows 用于以时间为维度来进行数据聚合，具体分为以下四类：</p>
<h3 id="2-1-Tumbling-Windows">2.1 Tumbling Windows</h3>
<p>滚动窗口 (Tumbling Windows) 是指彼此之间没有重叠的窗口。例如：每隔1小时统计过去1小时内的商品点击量，那么 1 天就只能分为 24 个窗口，每个窗口彼此之间是不存在重叠的，具体如下：</p>
<div align="center"> <img width="600px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-tumbling-windows.png"/> </div>
<p>这里我们以词频统计为例，给出一个具体的用例，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// 接收socket上的数据输入</span></span><br><span class="line">DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(<span class="string">&quot;hadoop001&quot;</span>, <span class="number">9999</span>, <span class="string">&quot;\n&quot;</span>, <span class="number">3</span>);</span><br><span class="line">streamSource.flatMap(<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        String[] words = value.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            out.collect(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(word, <span class="number">1L</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).keyBy(<span class="number">0</span>).timeWindow(Time.seconds(<span class="number">3</span>)).sum(<span class="number">1</span>).print(); <span class="comment">//每隔3秒统计一次每个单词出现的数量</span></span><br><span class="line">env.execute(<span class="string">&quot;Flink Streaming&quot;</span>);</span><br></pre></td></tr></table></figure>
<p>测试结果如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-window-word-count.png"/> </div>
<h3 id="2-2-Sliding-Windows">2.2 Sliding Windows</h3>
<p>滑动窗口用于滚动进行聚合分析，例如：每隔 6 分钟统计一次过去一小时内所有商品的点击量，那么统计窗口彼此之间就是存在重叠的，即 1天可以分为 240 个窗口。图示如下：</p>
<div align="center"> <img width="600px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-sliding-windows.png"/> </div>
<p>可以看到 window 1 - 4 这四个窗口彼此之间都存在着时间相等的重叠部分。想要实现滑动窗口，只需要在使用 timeWindow 方法时额外传递第二个参数作为滚动时间即可，具体如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 每隔3秒统计一次过去1分钟内的数据</span></span><br><span class="line">timeWindow(Time.minutes(<span class="number">1</span>),Time.seconds(<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<h3 id="2-3-Session-Windows">2.3 Session Windows</h3>
<p>当用户在进行持续浏览时，可能每时每刻都会有点击数据，例如在活动区间内，用户可能频繁的将某类商品加入和移除购物车，而你只想知道用户本次浏览最终的购物车情况，此时就可以在用户持有的会话结束后再进行统计。想要实现这类统计，可以通过 Session Windows 来进行实现。</p>
<div align="center"> <img width="600px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-session-windows.png"/> </div>
<p>具体的实现代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 以处理时间为衡量标准，如果10秒内没有任何数据输入，就认为会话已经关闭，此时触发统计</span></span><br><span class="line">window(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class="number">10</span>)))</span><br><span class="line"><span class="comment">// 以事件时间为衡量标准    </span></span><br><span class="line">window(EventTimeSessionWindows.withGap(Time.seconds(<span class="number">10</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="2-4-Global-Windows">2.4 Global Windows</h3>
<p>最后一个窗口是全局窗口， 全局窗口会将所有 key 相同的元素分配到同一个窗口中，其通常配合触发器 (trigger) 进行使用。如果没有相应触发器，则计算将不会被执行。</p>
<div align="center"> <img width="600px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-non-windowed.png"/> </div>
<p>这里继续以上面词频统计的案例为例，示例代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 当单词累计出现的次数每达到10次时，则触发计算，计算整个窗口内该单词出现的总数</span></span><br><span class="line">window(GlobalWindows.create()).trigger(CountTrigger.of(<span class="number">10</span>)).sum(<span class="number">1</span>).print();</span><br></pre></td></tr></table></figure>
<h2 id="三、Count-Windows">三、Count Windows</h2>
<p>Count Windows 用于以数量为维度来进行数据聚合，同样也分为滚动窗口和滑动窗口，实现方式也和时间窗口完全一致，只是调用的 API 不同，具体如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 滚动计数窗口，每1000次点击则计算一次</span></span><br><span class="line">countWindow(<span class="number">1000</span>)</span><br><span class="line"><span class="comment">// 滑动计数窗口，每10次点击发生后，则计算过去1000次点击的情况</span></span><br><span class="line">countWindow(<span class="number">1000</span>,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>实际上计数窗口内部就是调用的我们上一部分介绍的全局窗口来实现的，其源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title function_">countWindow</span><span class="params">(<span class="type">long</span> size)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create()).trigger(PurgingTrigger.of(CountTrigger.of(size)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title function_">countWindow</span><span class="params">(<span class="type">long</span> size, <span class="type">long</span> slide)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create())</span><br><span class="line">        .evictor(CountEvictor.of(size))</span><br><span class="line">        .trigger(CountTrigger.of(slide));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="参考资料-7">参考资料</h2>
<p>Flink Windows： <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/windows.html">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/windows.html</a></p>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink状态管理与检查点机制</title>
    <url>/2022/03/19/Flink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E4%B8%8E%E6%A3%80%E6%9F%A5%E7%82%B9%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h2 id="一、状态分类">一、状态分类</h2>
<p>相对于其他流计算框架，Flink 一个比较重要的特性就是其支持有状态计算。即你可以将中间的计算结果进行保存，并提供给后续的计算使用：</p>
<div align="center"> <img width="500px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-stateful-stream.png" rel="noreferrer"/> </div>
<p>具体而言，Flink 又将状态 (State) 分为 Keyed State 与 Operator State：</p>
<h3 id="2-1-算子状态">2.1 算子状态</h3>
<p>算子状态 (Operator State)：顾名思义，状态是和算子进行绑定的，一个算子的状态不能被其他算子所访问到。官方文档上对 Operator State 的解释是：<em>each operator state is bound to one parallel operator instance</em>，所以更为确切的说一个算子状态是与一个并发的算子实例所绑定的，即假设算子的并行度是 2，那么其应有两个对应的算子状态：</p>
<div align="center" rel="noreferrer"> <img width="500px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-operator-state.png"/> </div>
<h3 id="2-2-键控状态">2.2 键控状态</h3>
<p>键控状态 (Keyed State) ：是一种特殊的算子状态，即状态是根据 key 值进行区分的，Flink 会为每类键值维护一个状态实例。如下图所示，每个颜色代表不同 key 值，对应四个不同的状态实例。需要注意的是键控状态只能在 <code>KeyedStream</code> 上进行使用，我们可以通过 <code>stream.keyBy(...)</code> 来得到 <code>KeyedStream</code> 。</p>
<div align="center" rel="noreferrer"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-keyed-state.png" /> </div>
<h2 id="二、状态编程">二、状态编程</h2>
<h3 id="2-1-键控状态">2.1 键控状态</h3>
<p>Flink 提供了以下数据格式来管理和存储键控状态 (Keyed State)：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>ValueState</strong>：存储单值类型的状态。可以使用  <code>update(T)</code> 进行更新，并通过 <code>T value()</code> 进行检索。</p>
</li>
<li class="lvl-2">
<p><strong>ListState</strong>：存储列表类型的状态。可以使用 <code>add(T)</code> 或 <code>addAll(List)</code> 添加元素；并通过 <code>get()</code> 获得整个列表。</p>
</li>
<li class="lvl-2">
<p><strong>ReducingState</strong>：用于存储经过 ReduceFunction 计算后的结果，使用 <code>add(T)</code> 增加元素。</p>
</li>
<li class="lvl-2">
<p><strong>AggregatingState</strong>：用于存储经过 AggregatingState 计算后的结果，使用 <code>add(IN)</code> 添加元素。</p>
</li>
<li class="lvl-2">
<p><strong>FoldingState</strong>：已被标识为废弃，会在未来版本中移除，官方推荐使用 <code>AggregatingState</code> 代替。</p>
</li>
<li class="lvl-3">
<p><strong>MapState</strong>：维护 Map 类型的状态。</p>
</li>
</ul>
<p>以上所有增删改查方法不必硬记，在使用时通过语法提示来调用即可。这里给出一个具体的使用示例：假设我们正在开发一个监控系统，当监控数据超过阈值一定次数后，需要发出报警信息。这里之所以要达到一定次数，是因为由于偶发原因，偶尔一次超过阈值并不能代表什么，故需要达到一定次数后才触发报警，这就需要使用到 Flink 的状态编程。相关代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ThresholdWarning</span> <span class="keyword">extends</span> </span><br><span class="line">    <span class="title class_">RichFlatMapFunction</span>&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;String, List&lt;Long&gt;&gt;&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过ListState来存储非正常数据的状态</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;Long&gt; abnormalData;</span><br><span class="line">    <span class="comment">// 需要监控的阈值</span></span><br><span class="line">    <span class="keyword">private</span> Long threshold;</span><br><span class="line">    <span class="comment">// 触发报警的次数</span></span><br><span class="line">    <span class="keyword">private</span> Integer numberOfTimes;</span><br><span class="line"></span><br><span class="line">    ThresholdWarning(Long threshold, Integer numberOfTimes) &#123;</span><br><span class="line">        <span class="built_in">this</span>.threshold = threshold;</span><br><span class="line">        <span class="built_in">this</span>.numberOfTimes = numberOfTimes;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">open</span><span class="params">(Configuration parameters)</span> &#123;</span><br><span class="line">        <span class="comment">// 通过状态名称(句柄)获取状态实例，如果不存在则会自动创建</span></span><br><span class="line">        abnormalData = getRuntimeContext().getListState(</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">ListStateDescriptor</span>&lt;&gt;(<span class="string">&quot;abnormalData&quot;</span>, Long.class));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap</span><span class="params">(Tuple2&lt;String, Long&gt; value, Collector&lt;Tuple2&lt;String, List&lt;Long&gt;&gt;&gt; out)</span></span><br><span class="line">        <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">Long</span> <span class="variable">inputValue</span> <span class="operator">=</span> value.f1;</span><br><span class="line">        <span class="comment">// 如果输入值超过阈值，则记录该次不正常的数据信息</span></span><br><span class="line">        <span class="keyword">if</span> (inputValue &gt;= threshold) &#123;</span><br><span class="line">            abnormalData.add(inputValue);</span><br><span class="line">        &#125;</span><br><span class="line">        ArrayList&lt;Long&gt; list = Lists.newArrayList(abnormalData.get().iterator());</span><br><span class="line">        <span class="comment">// 如果不正常的数据出现达到一定次数，则输出报警信息</span></span><br><span class="line">        <span class="keyword">if</span> (list.size() &gt;= numberOfTimes) &#123;</span><br><span class="line">            out.collect(Tuple2.of(value.f0 + <span class="string">&quot; 超过指定阈值 &quot;</span>, list));</span><br><span class="line">            <span class="comment">// 报警信息输出后，清空状态</span></span><br><span class="line">            abnormalData.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>调用自定义的状态监控，这里我们使用 a，b 来代表不同类型的监控数据，分别对其数据进行监控：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">DataStreamSource&lt;Tuple2&lt;String, Long&gt;&gt; tuple2DataStreamSource = env.fromElements(</span><br><span class="line">    Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">50L</span>), Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">80L</span>), Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">400L</span>),</span><br><span class="line">    Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">100L</span>), Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">200L</span>), Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">200L</span>),</span><br><span class="line">    Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">100L</span>), Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">200L</span>), Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">200L</span>),</span><br><span class="line">    Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">500L</span>), Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">600L</span>), Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">700L</span>));</span><br><span class="line">tuple2DataStreamSource</span><br><span class="line">    .keyBy(<span class="number">0</span>)</span><br><span class="line">    .flatMap(<span class="keyword">new</span> <span class="title class_">ThresholdWarning</span>(<span class="number">100L</span>, <span class="number">3</span>))  <span class="comment">// 超过100的阈值3次后就进行报警</span></span><br><span class="line">    .printToErr();</span><br><span class="line">env.execute(<span class="string">&quot;Managed Keyed State&quot;</span>);</span><br></pre></td></tr></table></figure>
<p>输出如下结果如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-state-management.png"/> </div>
<h3 id="2-2-状态有效期">2.2 状态有效期</h3>
<p>以上任何类型的 keyed state 都支持配置有效期 (TTL) ，示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">StateTtlConfig</span> <span class="variable">ttlConfig</span> <span class="operator">=</span> StateTtlConfig</span><br><span class="line">    <span class="comment">// 设置有效期为 10 秒</span></span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">10</span>))  </span><br><span class="line">    <span class="comment">// 设置有效期更新规则，这里设置为当创建和写入时，都重置其有效期到规定的10秒</span></span><br><span class="line">    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) </span><br><span class="line">    <span class="comment">/*设置只要值过期就不可见，另外一个可选值是ReturnExpiredIfNotCleanedUp，</span></span><br><span class="line"><span class="comment">     代表即使值过期了，但如果还没有被物理删除，就是可见的*/</span></span><br><span class="line">    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)</span><br><span class="line">    .build();</span><br><span class="line">ListStateDescriptor&lt;Long&gt; descriptor = <span class="keyword">new</span> <span class="title class_">ListStateDescriptor</span>&lt;&gt;(<span class="string">&quot;abnormalData&quot;</span>, Long.class);</span><br><span class="line">descriptor.enableTimeToLive(ttlConfig);</span><br></pre></td></tr></table></figure>
<h3 id="2-3-算子状态">2.3 算子状态</h3>
<p>相比于键控状态，算子状态目前支持的存储类型只有以下三种：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>ListState</strong>：存储列表类型的状态。</p>
</li>
<li class="lvl-2">
<p><strong>UnionListState</strong>：存储列表类型的状态，与 ListState 的区别在于：如果并行度发生变化，ListState 会将该算子的所有并发的状态实例进行汇总，然后均分给新的 Task；而 UnionListState 只是将所有并发的状态实例汇总起来，具体的划分行为则由用户进行定义。</p>
</li>
<li class="lvl-2">
<p><strong>BroadcastState</strong>：用于广播的算子状态。</p>
</li>
</ul>
<p>这里我们继续沿用上面的例子，假设此时我们不需要区分监控数据的类型，只要有监控数据超过阈值并达到指定的次数后，就进行报警，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ThresholdWarning</span> <span class="keyword">extends</span> <span class="title class_">RichFlatMapFunction</span>&lt;Tuple2&lt;String, Long&gt;, </span><br><span class="line">Tuple2&lt;String, List&lt;Tuple2&lt;String, Long&gt;&gt;&gt;&gt; <span class="keyword">implements</span> <span class="title class_">CheckpointedFunction</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 非正常数据</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;Tuple2&lt;String, Long&gt;&gt; bufferedData;</span><br><span class="line">    <span class="comment">// checkPointedState</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;Tuple2&lt;String, Long&gt;&gt; checkPointedState;</span><br><span class="line">    <span class="comment">// 需要监控的阈值</span></span><br><span class="line">    <span class="keyword">private</span> Long threshold;</span><br><span class="line">    <span class="comment">// 次数</span></span><br><span class="line">    <span class="keyword">private</span> Integer numberOfTimes;</span><br><span class="line"></span><br><span class="line">    ThresholdWarning(Long threshold, Integer numberOfTimes) &#123;</span><br><span class="line">        <span class="built_in">this</span>.threshold = threshold;</span><br><span class="line">        <span class="built_in">this</span>.numberOfTimes = numberOfTimes;</span><br><span class="line">        <span class="built_in">this</span>.bufferedData = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 注意这里获取的是OperatorStateStore</span></span><br><span class="line">        checkPointedState = context.getOperatorStateStore().</span><br><span class="line">            getListState(<span class="keyword">new</span> <span class="title class_">ListStateDescriptor</span>&lt;&gt;(<span class="string">&quot;abnormalData&quot;</span>,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> <span class="title class_">TypeHint</span>&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                &#125;)));</span><br><span class="line">        <span class="comment">// 如果发生重启，则需要从快照中将状态进行恢复</span></span><br><span class="line">        <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Tuple2&lt;String, Long&gt; element : checkPointedState.get()) &#123;</span><br><span class="line">                bufferedData.add(element);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap</span><span class="params">(Tuple2&lt;String, Long&gt; value, </span></span><br><span class="line"><span class="params">                        Collector&lt;Tuple2&lt;String, List&lt;Tuple2&lt;String, Long&gt;&gt;&gt;&gt; out)</span> &#123;</span><br><span class="line">        <span class="type">Long</span> <span class="variable">inputValue</span> <span class="operator">=</span> value.f1;</span><br><span class="line">        <span class="comment">// 超过阈值则进行记录</span></span><br><span class="line">        <span class="keyword">if</span> (inputValue &gt;= threshold) &#123;</span><br><span class="line">            bufferedData.add(value);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 超过指定次数则输出报警信息</span></span><br><span class="line">        <span class="keyword">if</span> (bufferedData.size() &gt;= numberOfTimes) &#123;</span><br><span class="line">             <span class="comment">// 顺便输出状态实例的hashcode</span></span><br><span class="line">             out.collect(Tuple2.of(checkPointedState.hashCode() + <span class="string">&quot;阈值警报！&quot;</span>, bufferedData));</span><br><span class="line">            bufferedData.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 在进行快照时，将数据存储到checkPointedState</span></span><br><span class="line">        checkPointedState.clear();</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;String, Long&gt; element : bufferedData) &#123;</span><br><span class="line">            checkPointedState.add(element);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>调用自定义算子状态，这里需要将并行度设置为 1：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// 开启检查点机制</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br><span class="line"><span class="comment">// 设置并行度为1</span></span><br><span class="line">DataStreamSource&lt;Tuple2&lt;String, Long&gt;&gt; tuple2DataStreamSource = env.setParallelism(<span class="number">1</span>).fromElements(</span><br><span class="line">    Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">50L</span>), Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">80L</span>), Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">400L</span>),</span><br><span class="line">    Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">100L</span>), Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">200L</span>), Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">200L</span>),</span><br><span class="line">    Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">100L</span>), Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">200L</span>), Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">200L</span>),</span><br><span class="line">    Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">500L</span>), Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">600L</span>), Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">700L</span>));</span><br><span class="line">tuple2DataStreamSource</span><br><span class="line">    .flatMap(<span class="keyword">new</span> <span class="title class_">ThresholdWarning</span>(<span class="number">100L</span>, <span class="number">3</span>))</span><br><span class="line">    .printToErr();</span><br><span class="line">env.execute(<span class="string">&quot;Managed Keyed State&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此时输出如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-operator-state-para1.png"/> </div>
<p>在上面的调用代码中，我们将程序的并行度设置为 1，可以看到三次输出中状态实例的 hashcode 全是一致的，证明它们都同一个状态实例。假设将并行度设置为 2，此时输出如下：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-operator-state-para2.png"/> </div>
<p>可以看到此时两次输出中状态实例的 hashcode 是不一致的，代表它们不是同一个状态实例，这也就是上文提到的，一个算子状态是与一个并发的算子实例所绑定的。同时这里只输出两次，是因为在并发处理的情况下，线程 1 可能拿到 5 个非正常值，线程 2 可能拿到 4 个非正常值，因为要大于 3 次才能输出，所以在这种情况下就会出现只输出两条记录的情况，所以需要将程序的并行度设置为 1。</p>
<h2 id="三、检查点机制">三、检查点机制</h2>
<h3 id="3-1-CheckPoints">3.1 CheckPoints</h3>
<p>为了使 Flink 的状态具有良好的容错性，Flink 提供了检查点机制 (CheckPoints)  。通过检查点机制，Flink 定期在数据流上生成 checkpoint barrier ，当某个算子收到 barrier 时，即会基于当前状态生成一份快照，然后再将该 barrier 传递到下游算子，下游算子接收到该 barrier 后，也基于当前状态生成一份快照，依次传递直至到最后的 Sink 算子上。当出现异常后，Flink 就可以根据最近的一次的快照数据将所有算子恢复到先前的状态。</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-stream-barriers.png"/> </div>
<h3 id="3-2-开启检查点">3.2 开启检查点</h3>
<p>默认情况下，检查点机制是关闭的，需要在程序中进行开启：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 开启检查点机制，并指定状态检查点之间的时间间隔</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 其他可选配置如下：</span></span><br><span class="line"><span class="comment">// 设置语义</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"><span class="comment">// 设置两个检查点之间的最小时间间隔</span></span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line"><span class="comment">// 设置执行Checkpoint操作时的超时时间</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line"><span class="comment">// 设置最大并发执行的检查点的数量</span></span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 将检查点持久化到外部存储</span></span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(</span><br><span class="line">    ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"><span class="comment">// 如果有更近的保存点时，是否将作业回退到该检查点</span></span><br><span class="line">env.getCheckpointConfig().setPreferCheckpointForRecovery(<span class="literal">true</span>);</span><br></pre></td></tr></table></figure>
<h3 id="3-3-保存点机制">3.3 保存点机制</h3>
<p>保存点机制 (Savepoints) 是检查点机制的一种特殊的实现，它允许你通过手工的方式来触发 Checkpoint，并将结果持久化存储到指定路径中，主要用于避免 Flink 集群在重启或升级时导致状态丢失。示例如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">触发指定<span class="built_in">id</span>的作业的Savepoint，并将结果存储到指定目录下</span></span><br><span class="line">bin/flink savepoint :jobId [:targetDirectory]</span><br></pre></td></tr></table></figure>
<p>更多命令和配置可以参考官方文档：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/zh/ops/state/savepoints.html">savepoints</a></p>
<h2 id="四、状态后端">四、状态后端</h2>
<h3 id="4-1-状态管理器分类">4.1 状态管理器分类</h3>
<p>默认情况下，所有的状态都存储在 JVM 的堆内存中，在状态数据过多的情况下，这种方式很有可能导致内存溢出，因此 Flink 该提供了其它方式来存储状态数据，这些存储方式统一称为状态后端 (或状态管理器)：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/flink-checkpoints-backend.png"/> </div>
<p>主要有以下三种：</p>
<h4 id="1-MemoryStateBackend">1. MemoryStateBackend</h4>
<p>默认的方式，即基于 JVM 的堆内存进行存储，主要适用于本地开发和调试。</p>
<h4 id="2-FsStateBackend">2. FsStateBackend</h4>
<p>基于文件系统进行存储，可以是本地文件系统，也可以是 HDFS 等分布式文件系统。 需要注意而是虽然选择使用了 FsStateBackend ，但正在进行的数据仍然是存储在 TaskManager 的内存中的，只有在 checkpoint 时，才会将状态快照写入到指定文件系统上。</p>
<h4 id="3-RocksDBStateBackend">3. RocksDBStateBackend</h4>
<p>RocksDBStateBackend 是 Flink 内置的第三方状态管理器，采用嵌入式的 key-value 型数据库 RocksDB 来存储正在进行的数据。等到 checkpoint 时，再将其中的数据持久化到指定的文件系统中，所以采用 RocksDBStateBackend 时也需要配置持久化存储的文件系统。之所以这样做是因为 RocksDB 作为嵌入式数据库安全性比较低，但比起全文件系统的方式，其读取速率更快；比起全内存的方式，其存储空间更大，因此它是一种比较均衡的方案。</p>
<h3 id="4-2-配置方式">4.2 配置方式</h3>
<p>Flink 支持使用两种方式来配置后端管理器：</p>
<p><strong>第一种方式</strong>：基于代码方式进行配置，只对当前作业生效：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 配置 FsStateBackend</span></span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> <span class="title class_">FsStateBackend</span>(<span class="string">&quot;hdfs://namenode:40010/flink/checkpoints&quot;</span>));</span><br><span class="line"><span class="comment">// 配置 RocksDBStateBackend</span></span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> <span class="title class_">RocksDBStateBackend</span>(<span class="string">&quot;hdfs://namenode:40010/flink/checkpoints&quot;</span>));</span><br></pre></td></tr></table></figure>
<p>配置 RocksDBStateBackend 时，需要额外导入下面的依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-statebackend-rocksdb_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>第二种方式</strong>：基于 <code>flink-conf.yaml</code> 配置文件的方式进行配置，对所有部署在该集群上的作业都生效：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">state.backend:</span> <span class="string">filesystem</span></span><br><span class="line"><span class="attr">state.checkpoints.dir:</span> <span class="string">hdfs://namenode:40010/flink/checkpoints</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：本篇文章所有示例代码下载地址：<a href="https://github.com/oicio/BigData-Notes/tree/master/code/Flink/flink-state-management">flink-state-management</a></p>
</blockquote>
<h2 id="参考资料-8">参考资料</h2>
<ul class="lvl-0">
<li class="lvl-2">
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/state/state.html">Working with State</a></p>
</li>
<li class="lvl-2">
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/state/checkpointing.html">Checkpointing</a></p>
</li>
<li class="lvl-2">
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/state/savepoints.html#savepoints">Savepoints</a></p>
</li>
<li class="lvl-2">
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/state/state_backends.html">State Backends</a></p>
</li>
<li class="lvl-2">
<p>Fabian Hueske , Vasiliki Kalavri . 《Stream Processing with Apache Flink》.  O’Reilly Media .  2019-4-30</p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>flink背压机制</title>
    <url>/2023/01/03/Flink%E8%83%8C%E5%8E%8B%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h3 id="背压产生的原因">背压产生的原因</h3>
<p>流处理系统需要能优雅地处理反压（backpressure）问题。反压通常产生于这样的场景：短时负载高峰导致系统接收数据的速率远高于它处理数据的速率。许多日常问题都会导致反压，例如，垃圾回收停顿可能会导致流入的数据快速堆积，或者遇到大促或秒杀活动导致流量陡增。反压如果不能得到正确的处理，可能会导致资源耗尽甚至系统崩溃。</p>
<p>目前主流的流处理系统 Storm/JStorm/Spark Streaming/Flink都提供了反压机制，但实现各不相同。</p>
<h3 id="Strom的处理方法">Strom的处理方法</h3>
<p>Strom是通过监控Blot中的接收队列负载情况，如果超过高水位值就会将反压信息写到 Zookeeper ，Zookeeper 上的 watch 会通知该拓扑的所有 Worker 都进入反压状态，<strong>最后 Spout 停止发送 tuple</strong>。具体实现可以看这个 JIRA <a href="https://github.com/apache/storm/pull/700">STORM-886</a>。</p>
<p>这种方式会存在问题，当下游出现阻塞时，上游停止发送，下游消除阻塞后，上游又开闸放水，过了一会儿，下游又阻塞，上游又限流，如此反复，整个数据流会一直处在一个颠簸状态。很不稳定。</p>
<h3 id="Flink的处理方法">Flink的处理方法</h3>
<p>Flink 在运行时主要由 <strong>operators</strong> 和 <strong>streams</strong> 两大组件构成。每个 operator 会消费中间态的流，并在流上进行转换，然后生成新的流。对于 Flink 的网络机制一种形象的类比是，<strong>Flink 使用了高效有界的分布式阻塞队列，就像 Java 通用的阻塞队列（BlockingQueue）一样</strong>。还记得经典的线程间通信案例：生产者消费者模型吗？使用 BlockingQueue 的话，一个较慢的接受者会降低发送者的发送速率，因为一旦队列满了（有界队列）发送者会被阻塞。Flink 解决反压的方案就是这种感觉。</p>
<p>在 Flink 中，这些分布式阻塞队列就是这些逻辑流，而队列容量是通过缓冲池来（<code>LocalBufferPool</code>）实现的。每个被生产和被消费的流都会被分配一个缓冲池。缓冲池管理着一组缓冲(<code>Buffer</code>)，缓冲在被消费后可以被回收循环利用。这很好理解：你从池子中拿走一个缓冲，填上数据，在数据消费完之后，又把缓冲还给池子，之后你可以再次使用它。</p>
<p>在解释 Flink 的反压原理之前，我们必须先对 Flink 中网络传输的内存管理有个了解。</p>
<h4 id="网络传输中的内存管理">网络传输中的内存管理</h4>
<p>如下图所示展示了 Flink 在网络传输场景下的内存管理。网络上传输的数据会写到 Task 的 InputGate（IG） 中，经过 Task 的处理后，再由 Task 写到 ResultPartition（RS） 中。每个 Task 都包括了输入和输入，输入和输出的数据存在 <code>Buffer</code> 中（都是字节数据）。Buffer 是 MemorySegment 的包装类。</p>
<p><img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/TB14fLsHVXXXXXWXFXXXXXXXXXX.png" alt="img"></p>
<ol>
<li class="lvl-3">
<p>TaskManager（TM）在启动时，会先初始化<code>NetworkEnvironment</code>对象，TM 中所有与网络相关的东西都由该类来管理（如 Netty 连接），其中就包括<code>NetworkBufferPool</code>。根据配置，Flink 会在 NetworkBufferPool 中生成一定数量（默认2048个）的内存块 MemorySegment（关于 Flink 的内存管理，<a href="http://www.atatech.org/articles/54033">后续文章</a>会详细谈到），内存块的总数量就代表了网络传输中所有可用的内存。NetworkEnvironment 和 NetworkBufferPool 是 Task 之间共享的，每个 TM 只会实例化一个。</p>
</li>
<li class="lvl-3">
<p>Task 线程启动时，会向 NetworkEnvironment 注册，NetworkEnvironment 会为 Task 的 InputGate（IG）和 ResultPartition（RP） 分别创建一个 LocalBufferPool（缓冲池）并设置可申请的 MemorySegment（内存块）数量。IG 对应的缓冲池初始的内存块数量与 IG 中 InputChannel 数量一致，RP 对应的缓冲池初始的内存块数量与 RP 中的 ResultSubpartition 数量一致。不过，每当创建或销毁缓冲池时，NetworkBufferPool 会计算剩余空闲的内存块数量，并平均分配给已创建的缓冲池。注意，这个过程只是指定了缓冲池所能使用的内存块数量，并没有真正分配内存块，只有当需要时才分配。为什么要动态地为缓冲池扩容呢？因为内存越多，意味着系统可以更轻松地应对瞬时压力（如GC），不会频繁地进入反压状态，所以我们要利用起那部分闲置的内存块。</p>
</li>
<li class="lvl-3">
<p>在 Task 线程执行过程中，当 Netty 接收端收到数据时，为了将 Netty 中的数据拷贝到 Task 中，InputChannel（实际是 RemoteInputChannel）会向其对应的缓冲池申请内存块（上图中的①）。如果缓冲池中也没有可用的内存块且已申请的数量还没到池子上限，则会向 NetworkBufferPool 申请内存块（上图中的②）并交给 InputChannel 填上数据（上图中的③和④）。如果缓冲池已申请的数量达到上限了呢？或者 NetworkBufferPool 也没有可用内存块了呢？这时候，Task 的 Netty Channel 会暂停读取，上游的发送端会立即响应停止发送，拓扑会进入反压状态。当 Task 线程写数据到 ResultPartition 时，也会向缓冲池请求内存块，如果没有可用内存块时，会阻塞在请求内存块的地方，达到暂停写入的目的。</p>
</li>
<li class="lvl-3">
<p>当一个内存块被消费完成之后（在输入端是指内存块中的字节被反序列化成对象了，在输出端是指内存块中的字节写入到 Netty Channel 了），会调用 <code>Buffer.recycle()</code> 方法，会将内存块还给 LocalBufferPool （上图中的⑤）。如果LocalBufferPool中当前申请的数量超过了池子容量（由于上文提到的动态容量，由于新注册的 Task 导致该池子容量变小），则LocalBufferPool会将该内存块回收给 NetworkBufferPool（上图中的⑥）。如果没超过池子容量，则会继续留在池子中，减少反复申请的开销。</p>
</li>
</ol>
<h4 id="背压的过程">背压的过程</h4>
<p>下面这张图简单展示了两个 Task 之间的数据传输以及 Flink 如何感知到反压的</p>
<p><img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/TB1rCIvJpXXXXcKXXXXXXXXXXXX.png" alt="img"></p>
<ol>
<li class="lvl-3">
<p>记录“A”进入了 Flink 并且被 Task 1 处理。（这里省略了 Netty 接收、反序列化等过程）</p>
</li>
<li class="lvl-3">
<p>记录被序列化到 buffer 中。</p>
</li>
<li class="lvl-3">
<p>该 buffer 被发送到 Task 2，然后 Task 2 从这个 buffer 中读出记录。</p>
</li>
</ol>
<p><strong>不要忘了：记录能被 Flink 处理的前提是，必须有空闲可用的 Buffer。</strong></p>
<p>结合上面两张图看：Task 1 在输出端有一个相关联的 LocalBufferPool（称缓冲池1），Task 2 在输入端也有一个相关联的 LocalBufferPool（称缓冲池2）。如果缓冲池1中有空闲可用的 buffer 来序列化记录 “A”，我们就序列化并发送该 buffer。</p>
<p>这里我们需要注意两个场景：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>本地传输：如果 Task 1 和 Task 2 运行在同一个 worker 节点（TaskManager），该 buffer 可以直接交给下一个 Task。一旦 Task 2 消费了该 buffer，则该 buffer 会被缓冲池1回收。如果 Task 2 的速度比 1 慢，那么 buffer 回收的速度就会赶不上 Task 1 取 buffer 的速度，导致缓冲池1无可用的 buffer，Task 1 等待在可用的 buffer 上。最终形成 Task 1 的降速。</p>
</li>
<li class="lvl-2">
<p>远程传输：如果 Task 1 和 Task 2 运行在不同的 worker 节点上，那么 buffer 会在发送到网络（TCP Channel）后被回收。在接收端，会从 LocalBufferPool 中申请 buffer，然后拷贝网络中的数据到 buffer 中。如果没有可用的 buffer，会停止从 TCP 连接中读取数据。在输出端，通过 Netty 的水位值机制来保证不往网络中写入太多数据（后面会说）<strong>。如果网络中的数据（Netty输出缓冲中的字节数）超过了高水位值，我们会等到其降到低水位值以下才继续写入数据</strong>。这保证了网络中不会有太多的数据。<strong>如果接收端停止消费网络中的数据（由于接收端缓冲池没有可用 buffer），网络中的缓冲数据就会堆积，那么发送端也会暂停发送</strong>。另外，这会使得发送端的缓冲池得不到回收，writer 阻塞在向 LocalBufferPool 请求 buffer，阻塞了 writer 往 ResultSubPartition 写数据。</p>
</li>
</ul>
<h4 id="Netty水位值机制">Netty水位值机制</h4>
<p>下方的代码是初始化 NettyServer 时配置的水位值参数。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 默认高水位值为2个buffer大小, 当接收端消费速度跟不上，发送端会立即感知到</span></span><br><span class="line">bootstrap.childOption(ChannelOption.WRITE_BUFFER_LOW_WATER_MARK, </span><br><span class="line">                      config.getMemorySegmentSize() + <span class="number">1</span>);</span><br><span class="line">bootstrap.childOption(ChannelOption.WRITE_BUFFER_HIGH_WATER_MARK, </span><br><span class="line">                      <span class="number">2</span> * config.getMemorySegmentSize());</span><br></pre></td></tr></table></figure>
<p>当输出缓冲中的字节数超过了高水位值, 则 Channel.isWritable() 会返回false。当输出缓存中的字节数又掉到了低水位值以下, 则 Channel.isWritable() 会重新返回true。Flink 中发送数据的核心代码在 <code>PartitionRequestQueue</code> 中，该类是 server channel pipeline 的最后一层。发送数据关键代码如下所示。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">writeAndFlushNextMessageIfPossible</span><span class="params">(<span class="keyword">final</span> Channel channel)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="keyword">if</span> (fatalError) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="type">Buffer</span> <span class="variable">buffer</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// channel.isWritable() 配合 WRITE_BUFFER_LOW_WATER_MARK </span></span><br><span class="line">        <span class="comment">// 和 WRITE_BUFFER_HIGH_WATER_MARK 实现发送端的流量控制</span></span><br><span class="line">        <span class="keyword">if</span> (channel.isWritable()) &#123;</span><br><span class="line">            <span class="comment">// 注意: 一个while循环也就最多只发送一个BufferResponse, 连续发送BufferResponse是通过writeListener回调实现的</span></span><br><span class="line">            <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (currentPartitionQueue == <span class="literal">null</span> &amp;&amp; (currentPartitionQueue = queue.poll()) == <span class="literal">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125;</span><br><span class="line"> </span><br><span class="line">                buffer = currentPartitionQueue.getNextBuffer();</span><br><span class="line"> </span><br><span class="line">                <span class="keyword">if</span> (buffer == <span class="literal">null</span>) &#123;</span><br><span class="line">                    <span class="comment">// 跳过这部分代码</span></span><br><span class="line">                    ...</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// 构造一个response返回给客户端</span></span><br><span class="line">                    <span class="type">BufferResponse</span> <span class="variable">resp</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferResponse</span>(buffer, currentPartitionQueue.getSequenceNumber(), currentPartitionQueue.getReceiverId());</span><br><span class="line"> </span><br><span class="line">                    <span class="keyword">if</span> (!buffer.isBuffer() &amp;&amp;</span><br><span class="line">                            EventSerializer.fromBuffer(buffer, getClass().getClassLoader()).getClass() == EndOfPartitionEvent.class) &#123;</span><br><span class="line">                        <span class="comment">// 跳过这部分代码。batch 模式中 subpartition 的数据准备就绪，通知下游消费者。</span></span><br><span class="line">                        ...</span><br><span class="line">                    &#125;</span><br><span class="line"> </span><br><span class="line">                    <span class="comment">// 将该response发到netty channel, 当写成功后, </span></span><br><span class="line">                    <span class="comment">// 通过注册的writeListener又会回调进来, 从而不断地消费 queue 中的请求</span></span><br><span class="line">                    channel.writeAndFlush(resp).addListener(writeListener);</span><br><span class="line"> </span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">        <span class="keyword">if</span> (buffer != <span class="literal">null</span>) &#123;</span><br><span class="line">            buffer.recycle();</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IOException</span>(t.getMessage(), t);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 当水位值降下来后（channel 再次可写），会重新触发发送函数</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">channelWritabilityChanged</span><span class="params">(ChannelHandlerContext ctx)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    writeAndFlushNextMessageIfPossible(ctx.channel());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>核心发送方法中如果channel不可写，则会跳过发送。当channel再次可写后，Netty 会调用该Handle的 <code>channelWritabilityChanged</code> 方法，从而重新触发发送函数。</p>
<h4 id="背压监控">背压监控</h4>
<p>在 Storm/JStorm 中，只要监控到队列满了，就可以记录下拓扑进入反压了。但是 Flink 的反压太过于天然了，导致我们无法简单地通过监控队列来监控反压状态。Flink 在这里使用了一个 trick 来实现对反压的监控。<strong>如果一个 Task 因为反压而降速了，那么它会卡在向 LocalBufferPool 申请内存块上</strong>。那么这时候，该 Task 的 stack trace 就会长下面这样：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java.lang.Object.wait(Native Method)</span><br><span class="line">o.a.f.[...].LocalBufferPool.requestBuffer(LocalBufferPool.java:<span class="number">163</span>)</span><br><span class="line">o.a.f.[...].LocalBufferPool.requestBufferBlocking(LocalBufferPool.java:<span class="number">133</span>) &lt;--- BLOCKING request</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p>那么事情就简单了。通过不断地采样每个 task 的 stack trace 就可以实现反压监控。</p>
<p><img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/TB1T3cJJpXXXXXLXpXXXXXXXXXX.png" alt="img"></p>
<p>Flink 的实现中，只有当 Web 页面切换到某个 Job 的 Backpressure 页面，才会对这个 Job 触发反压检测，因为反压检测还是挺昂贵的。JobManager 会通过 Akka 给每个 TaskManager 发送<code>TriggerStackTraceSample</code>消息。默认情况下，TaskManager 会触发100次 stack trace 采样，每次间隔 50ms（也就是说一次反压检测至少要等待5秒钟）。并将这 100 次采样的结果返回给 JobManager，由 JobManager 来计算反压比率（反压出现的次数/采样的次数），最终展现在 UI 上。UI 刷新的默认周期是一分钟，目的是不对 TaskManager 造成太大的负担。</p>
<h3 id="总结-2">总结</h3>
<p>Flink 不需要一种特殊的机制来处理反压，因为 Flink 中的数据传输相当于已经提供了应对反压的机制。因此，Flink 所能获得的最大吞吐量由其 pipeline 中最慢的组件决定。相对于 Storm/JStorm 的实现，Flink 的实现更为简洁优雅，源码中也看不见与反压相关的代码，无需 Zookeeper/TopologyMaster 的参与也降低了系统的负载，也利于对反压更迅速的响应。</p>
<p>转载: <a href="https://blog.csdn.net/u011750989/article/details/82191298">flink背压</a></p>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS常用shell命令</title>
    <url>/2021/10/15/HDFS%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p><strong>1. 显示当前目录结构</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">显示当前目录结构</span></span><br><span class="line">hadoop fs -ls  &lt;path&gt;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">递归显示当前目录结构</span></span><br><span class="line">hadoop fs -ls  -R  &lt;path&gt;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">显示根目录下内容</span></span><br><span class="line">hadoop fs -ls  /</span><br></pre></td></tr></table></figure>
<p><strong>2. 创建目录</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">创建目录</span></span><br><span class="line">hadoop fs -mkdir  &lt;path&gt; </span><br><span class="line"><span class="meta"># </span><span class="language-bash">递归创建目录</span></span><br><span class="line">hadoop fs -mkdir -p  &lt;path&gt;  </span><br></pre></td></tr></table></figure>
<p><strong>3. 删除操作</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">删除文件</span></span><br><span class="line">hadoop fs -rm  &lt;path&gt;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">递归删除目录和文件</span></span><br><span class="line">hadoop fs -rm -R  &lt;path&gt; </span><br></pre></td></tr></table></figure>
<p><strong>4. 从本地加载文件到 HDFS</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">二选一执行即可</span></span><br><span class="line">hadoop fs -put  [localsrc] [dst] </span><br><span class="line">hadoop fs - copyFromLocal [localsrc] [dst] </span><br></pre></td></tr></table></figure>
<p><strong>5. 从 HDFS 导出文件到本地</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">二选一执行即可</span></span><br><span class="line">hadoop fs -get  [dst] [localsrc] </span><br><span class="line">hadoop fs -copyToLocal [dst] [localsrc] </span><br></pre></td></tr></table></figure>
<p><strong>6. 查看文件内容</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">二选一执行即可</span></span><br><span class="line">hadoop fs -text  &lt;path&gt; </span><br><span class="line">hadoop fs -cat  &lt;path&gt;  </span><br></pre></td></tr></table></figure>
<p><strong>7. 显示文件的最后一千字节</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -tail  &lt;path&gt; </span><br><span class="line"><span class="meta"># </span><span class="language-bash">和Linux下一样，会持续监听文件内容变化 并显示文件的最后一千字节</span></span><br><span class="line">hadoop fs -tail -f  &lt;path&gt; </span><br></pre></td></tr></table></figure>
<p><strong>8. 拷贝文件</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -cp [src] [dst]</span><br></pre></td></tr></table></figure>
<p><strong>9. 移动文件</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -mv [src] [dst] </span><br></pre></td></tr></table></figure>
<p><strong>10. 统计当前目录下各文件大小</strong></p>
<ul class="lvl-0">
<li class="lvl-2">
<p>默认单位字节</p>
</li>
<li class="lvl-2">
<p>-s : 显示所有文件大小总和，</p>
</li>
<li class="lvl-2">
<p>-h : 将以更友好的方式显示文件大小（例如 64.0m 而不是 67108864）</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -du  &lt;path&gt;  </span><br></pre></td></tr></table></figure>
<p><strong>11. 合并下载多个文件</strong></p>
<ul class="lvl-0">
<li class="lvl-2">
<p>-nl  在每个文件的末尾添加换行符（LF）</p>
</li>
<li class="lvl-2">
<p>-skip-empty-file 跳过空文件</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -getmerge</span><br><span class="line"><span class="meta"># </span><span class="language-bash">示例 将HDFS上的hbase-policy.xml和hbase-site.xml文件合并后下载到本地的/usr/test.xml</span></span><br><span class="line">hadoop fs -getmerge -nl  /test/hbase-policy.xml /test/hbase-site.xml /usr/test.xml</span><br></pre></td></tr></table></figure>
<p><strong>12. 统计文件系统的可用空间信息</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -df -h /</span><br></pre></td></tr></table></figure>
<p><strong>13. 更改文件复制因子</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -setrep [-R] [-w] &lt;numReplicas&gt; &lt;path&gt;</span><br></pre></td></tr></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>更改文件的复制因子。如果 path 是目录，则更改其下所有文件的复制因子</p>
</li>
<li class="lvl-2">
<p>-w : 请求命令是否等待复制完成</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">示例</span></span><br><span class="line">hadoop fs -setrep -w 3 /user/hadoop/dir1</span><br></pre></td></tr></table></figure>
<p><strong>14. 权限控制</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">权限控制和Linux上使用方式一致</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">变更文件或目录的所属群组。 用户必须是文件的所有者或超级用户。</span></span><br><span class="line">hadoop fs -chgrp [-R] GROUP URI [URI ...]</span><br><span class="line"><span class="meta"># </span><span class="language-bash">修改文件或目录的访问权限  用户必须是文件的所有者或超级用户。</span></span><br><span class="line">hadoop fs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI ...]</span><br><span class="line"><span class="meta"># </span><span class="language-bash">修改文件的拥有者  用户必须是超级用户。</span></span><br><span class="line">hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</span><br></pre></td></tr></table></figure>
<p><strong>15. 文件检测</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -test - [defsz]  URI</span><br></pre></td></tr></table></figure>
<p>可选选项：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>-d：如果路径是目录，返回 0。</p>
</li>
<li class="lvl-2">
<p>-e：如果路径存在，则返回 0。</p>
</li>
<li class="lvl-2">
<p>-f：如果路径是文件，则返回 0。</p>
</li>
<li class="lvl-2">
<p>-s：如果路径不为空，则返回 0。</p>
</li>
<li class="lvl-2">
<p>-r：如果路径存在且授予读权限，则返回 0。</p>
</li>
<li class="lvl-2">
<p>-w：如果路径存在且授予写入权限，则返回 0。</p>
</li>
<li class="lvl-2">
<p>-z：如果文件长度为零，则返回 0。</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">示例</span></span><br><span class="line">hadoop fs -test -e filename</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS JAVA API</title>
    <url>/2021/10/15/HDFS%20API/</url>
    <content><![CDATA[<h2 id="一、-简介">一、 简介</h2>
<p>想要使用 HDFS API，需要导入依赖 <code>hadoop-client</code>。如果是 CDH 版本的 Hadoop，还需要额外指明其仓库地址：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="line">&lt;project xmlns=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span><br><span class="line">         xmlns:xsi=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span><br><span class="line">         xsi:schemaLocation=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 </span></span><br><span class="line"><span class="string">         http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span><br><span class="line">    &lt;modelVersion&gt;<span class="number">4.0</span><span class="number">.0</span>&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;groupId&gt;com.ihadu&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hdfs-java-api&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">1.0</span>&lt;/version&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;properties&gt;</span><br><span class="line">        &lt;project.build.sourceEncoding&gt;UTF-<span class="number">8</span>&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">        &lt;hadoop.version&gt;<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.15</span><span class="number">.2</span>&lt;/hadoop.version&gt;</span><br><span class="line">    &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;!---配置 CDH 仓库地址--&gt;</span><br><span class="line">    &lt;repositories&gt;</span><br><span class="line">        &lt;repository&gt;</span><br><span class="line">            &lt;id&gt;cloudera&lt;/id&gt;</span><br><span class="line">            &lt;url&gt;https:<span class="comment">//repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;</span></span><br><span class="line">        &lt;/repository&gt;</span><br><span class="line">    &lt;/repositories&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;!--Hadoop-client--&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;<span class="number">4.12</span>&lt;/version&gt;</span><br><span class="line">            &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">    &lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure>
<h2 id="二、API的使用">二、API的使用</h2>
<h3 id="2-1-FileSystem">2.1 FileSystem</h3>
<p>FileSystem 是所有 HDFS 操作的主入口。由于之后的每个单元测试都需要用到它，这里使用 <code>@Before</code> 注解进行标注。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">HDFS_PATH</span> <span class="operator">=</span> <span class="string">&quot;hdfs://192.168.0.106:8020&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">HDFS_USER</span> <span class="operator">=</span> <span class="string">&quot;root&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> FileSystem fileSystem;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">prepare</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="comment">// 这里我启动的是单节点的 Hadoop,所以副本系数设置为 1,默认值为 3</span></span><br><span class="line">        configuration.set(<span class="string">&quot;dfs.replication&quot;</span>, <span class="string">&quot;1&quot;</span>);</span><br><span class="line">        fileSystem = FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(HDFS_PATH), configuration, HDFS_USER);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (URISyntaxException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@After</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">destroy</span><span class="params">()</span> &#123;</span><br><span class="line">    fileSystem = <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-2-创建目录">2.2 创建目录</h3>
<p>支持递归创建目录：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">mkDir</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    fileSystem.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test0/&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-创建指定权限的目录">2.3 创建指定权限的目录</h3>
<p><code>FsPermission(FsAction u, FsAction g, FsAction o)</code> 的三个参数分别对应：创建者权限，同组其他用户权限，其他用户权限，权限值定义在 <code>FsAction</code> 枚举类中。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">mkDirWithPermission</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    fileSystem.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test1/&quot;</span>),</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">FsPermission</span>(FsAction.READ_WRITE, FsAction.READ, FsAction.READ));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-4-创建文件，并写入内容">2.4 创建文件，并写入内容</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">// 如果文件存在，默认会覆盖, 可以通过第二个参数进行控制。第三个参数可以控制使用缓冲区的大小</span></span><br><span class="line">    <span class="type">FSDataOutputStream</span> <span class="variable">out</span> <span class="operator">=</span> fileSystem.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/a.txt&quot;</span>),</span><br><span class="line">                                               <span class="literal">true</span>, <span class="number">4096</span>);</span><br><span class="line">    out.write(<span class="string">&quot;hello hadoop!&quot;</span>.getBytes());</span><br><span class="line">    out.write(<span class="string">&quot;hello spark!&quot;</span>.getBytes());</span><br><span class="line">    out.write(<span class="string">&quot;hello flink!&quot;</span>.getBytes());</span><br><span class="line">    <span class="comment">// 强制将缓冲区中内容刷出</span></span><br><span class="line">    out.flush();</span><br><span class="line">    out.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-5-判断文件是否存在">2.5 判断文件是否存在</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">exist</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">boolean</span> <span class="variable">exists</span> <span class="operator">=</span> fileSystem.exists(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/a.txt&quot;</span>));</span><br><span class="line">    System.out.println(exists);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-6-查看文件内容">2.6 查看文件内容</h3>
<p>查看小文本文件的内容，直接转换成字符串后输出：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readToString</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">FSDataInputStream</span> <span class="variable">inputStream</span> <span class="operator">=</span> fileSystem.open(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/a.txt&quot;</span>));</span><br><span class="line">    <span class="type">String</span> <span class="variable">context</span> <span class="operator">=</span> inputStreamToString(inputStream, <span class="string">&quot;utf-8&quot;</span>);</span><br><span class="line">    System.out.println(context);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>inputStreamToString</code> 是一个自定义方法，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 把输入流转换为指定编码的字符</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> inputStream 输入流</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> encode      指定编码类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String <span class="title function_">inputStreamToString</span><span class="params">(InputStream inputStream, String encode)</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (encode == <span class="literal">null</span> || (<span class="string">&quot;&quot;</span>.equals(encode))) &#123;</span><br><span class="line">            encode = <span class="string">&quot;utf-8&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">BufferedReader</span> <span class="variable">reader</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(inputStream, encode));</span><br><span class="line">        <span class="type">StringBuilder</span> <span class="variable">builder</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuilder</span>();</span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="keyword">while</span> ((str = reader.readLine()) != <span class="literal">null</span>) &#123;</span><br><span class="line">            builder.append(str).append(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> builder.toString();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-7-文件重命名">2.7 文件重命名</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">rename</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">Path</span> <span class="variable">oldPath</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/a.txt&quot;</span>);</span><br><span class="line">    <span class="type">Path</span> <span class="variable">newPath</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/b.txt&quot;</span>);</span><br><span class="line">    <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> fileSystem.rename(oldPath, newPath);</span><br><span class="line">    System.out.println(result);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-8-删除目录或文件">2.8 删除目录或文件</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">delete</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     *  第二个参数代表是否递归删除</span></span><br><span class="line"><span class="comment">     *    +  如果 path 是一个目录且递归删除为 true, 则删除该目录及其中所有文件;</span></span><br><span class="line"><span class="comment">     *    +  如果 path 是一个目录但递归删除为 false,则会则抛出异常。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> fileSystem.delete(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/b.txt&quot;</span>), <span class="literal">true</span>);</span><br><span class="line">    System.out.println(result);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-9-上传文件到HDFS">2.9 上传文件到HDFS</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">copyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">// 如果指定的是目录，则会把目录及其中的文件都复制到指定目录下</span></span><br><span class="line">    <span class="type">Path</span> <span class="variable">src</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\BigData-Notes\\notes\\installation&quot;</span>);</span><br><span class="line">    <span class="type">Path</span> <span class="variable">dst</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/&quot;</span>);</span><br><span class="line">    fileSystem.copyFromLocalFile(src, dst);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-10-上传大文件并显示上传进度">2.10 上传大文件并显示上传进度</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">copyFromLocalBigFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">File</span> <span class="variable">file</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;D:\\kafka.tgz&quot;</span>);</span><br><span class="line">        <span class="keyword">final</span> <span class="type">float</span> <span class="variable">fileSize</span> <span class="operator">=</span> file.length();</span><br><span class="line">        <span class="type">InputStream</span> <span class="variable">in</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedInputStream</span>(<span class="keyword">new</span> <span class="title class_">FileInputStream</span>(file));</span><br><span class="line"></span><br><span class="line">        <span class="type">FSDataOutputStream</span> <span class="variable">out</span> <span class="operator">=</span> fileSystem.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/kafka5.tgz&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Progressable</span>() &#123;</span><br><span class="line">                  <span class="type">long</span> <span class="variable">fileCount</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">                  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">progress</span><span class="params">()</span> &#123;</span><br><span class="line">                     fileCount++;</span><br><span class="line">                     <span class="comment">// progress 方法每上传大约 64KB 的数据后就会被调用一次</span></span><br><span class="line">                     System.out.println(<span class="string">&quot;上传进度：&quot;</span> + (fileCount * <span class="number">64</span> * <span class="number">1024</span> / fileSize) * <span class="number">100</span> + <span class="string">&quot; %&quot;</span>);</span><br><span class="line">                   &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        IOUtils.copyBytes(in, out, <span class="number">4096</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-11-从HDFS上下载文件">2.11 从HDFS上下载文件</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">copyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">Path</span> <span class="variable">src</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/kafka.tgz&quot;</span>);</span><br><span class="line">    <span class="type">Path</span> <span class="variable">dst</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\app\\&quot;</span>);</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 第一个参数控制下载完成后是否删除源文件,默认是 true,即删除;</span></span><br><span class="line"><span class="comment">     * 最后一个参数表示是否将 RawLocalFileSystem 用作本地文件系统;</span></span><br><span class="line"><span class="comment">     * RawLocalFileSystem 默认为 false,通常情况下可以不设置,</span></span><br><span class="line"><span class="comment">     * 但如果你在执行时候抛出 NullPointerException 异常,则代表你的文件系统与程序可能存在不兼容的情况 (window 下常见),</span></span><br><span class="line"><span class="comment">     * 此时可以将 RawLocalFileSystem 设置为 true</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    fileSystem.copyToLocalFile(<span class="literal">false</span>, src, dst, <span class="literal">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-12-查看指定目录下所有文件的信息">2.12 查看指定目录下所有文件的信息</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">listFiles</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    FileStatus[] statuses = fileSystem.listStatus(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api&quot;</span>));</span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : statuses) &#123;</span><br><span class="line">        <span class="comment">//fileStatus 的 toString 方法被重写过，直接打印可以看到所有信息</span></span><br><span class="line">        System.out.println(fileStatus.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>FileStatus</code> 中包含了文件的基本信息，比如文件路径，是否是文件夹，修改时间，访问时间，所有者，所属组，文件权限，是否是符号链接等，输出内容示例如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FileStatus&#123;</span><br><span class="line">path=hdfs://192.168.0.106:8020/hdfs-api/test; </span><br><span class="line">isDirectory=true; </span><br><span class="line">modification_time=1556680796191; </span><br><span class="line">access_time=0; </span><br><span class="line">owner=root; </span><br><span class="line">group=supergroup; </span><br><span class="line">permission=rwxr-xr-x; </span><br><span class="line">isSymlink=false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-13-递归查看指定目录下所有文件的信息">2.13 递归查看指定目录下所有文件的信息</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">listFilesRecursive</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; files = fileSystem.listFiles(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hbase&quot;</span>), <span class="literal">true</span>);</span><br><span class="line">    <span class="keyword">while</span> (files.hasNext()) &#123;</span><br><span class="line">        System.out.println(files.next());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>和上面输出类似，只是多了文本大小，副本系数，块大小信息。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">LocatedFileStatus&#123;</span><br><span class="line">path=hdfs://192.168.0.106:8020/hbase/hbase.version; </span><br><span class="line">isDirectory=false; </span><br><span class="line">length=7; </span><br><span class="line">replication=1; </span><br><span class="line">blocksize=134217728; </span><br><span class="line">modification_time=1554129052916; </span><br><span class="line">access_time=1554902661455; </span><br><span class="line">owner=root; </span><br><span class="line">group=supergroup;</span><br><span class="line">permission=rw-r--r--; </span><br><span class="line">isSymlink=false&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-14-查看文件的块信息">2.14 查看文件的块信息</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">getFileBlockLocations</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">FileStatus</span> <span class="variable">fileStatus</span> <span class="operator">=</span> fileSystem.getFileStatus(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/hdfs-api/test/kafka.tgz&quot;</span>));</span><br><span class="line">    BlockLocation[] blocks = fileSystem.getFileBlockLocations(fileStatus, <span class="number">0</span>, fileStatus.getLen());</span><br><span class="line">    <span class="keyword">for</span> (BlockLocation block : blocks) &#123;</span><br><span class="line">        System.out.println(block);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>块输出信息有三个值，分别是文件的起始偏移量 (offset)，文件大小 (length)，块所在的主机名 (hosts)。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0,57028557,hadoop001</span><br></pre></td></tr></table></figure>
<p>这里我上传的文件只有 57M(小于 128M)，且程序中设置了副本系数为 1，所有只有一个块信息。</p>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop分布式文件系统—HDFS</title>
    <url>/2021/10/15/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E2%80%94HDFS/</url>
    <content><![CDATA[<h2 id="一、介绍">一、介绍</h2>
<p><strong>HDFS</strong> （<strong>Hadoop Distributed File System</strong>）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。</p>
<h2 id="二、HDFS-设计原理">二、HDFS 设计原理</h2>
<div align="center"> <img width="600px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hdfsarchitecture.png"/> </div>
<h3 id="2-1-HDFS-架构">2.1 HDFS 架构</h3>
<p>HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>NameNode</strong> : 负责执行有关 <code>文件系统命名空间</code> 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。</p>
</li>
<li class="lvl-2">
<p><strong>DataNode</strong>：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。</p>
</li>
</ul>
<h3 id="2-2-文件系统命名空间">2.2 文件系统命名空间</h3>
<p>HDFS 的 <code>文件系统命名空间</code> 的层次结构与大多数文件系统类似 (如 Linux)， 支持目录和文件的创建、移动、删除和重命名等操作，支持配置用户和访问权限，但不支持硬链接和软连接。<code>NameNode</code> 负责维护文件系统名称空间，记录对名称空间或其属性的任何更改。</p>
<h3 id="2-3-数据复制">2.3 数据复制</h3>
<p>由于 Hadoop 被设计运行在廉价的机器上，这意味着硬件是不可靠的，为了保证容错性，HDFS 提供了数据复制机制。HDFS 将每一个文件存储为一系列<strong>块</strong>，每个块由多个副本来保证容错，块的大小和复制因子可以自行配置（默认情况下，块大小是 128M，默认复制因子是 3）。</p>
<div align="center"> <img width="600px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hdfsdatanodes.png"/> </div>
<h3 id="2-4-数据复制的实现原理">2.4 数据复制的实现原理</h3>
<p>大型的 HDFS 实例在通常分布在多个机架的多台服务器上，不同机架上的两台服务器之间通过交换机进行通讯。在大多数情况下，同一机架中的服务器间的网络带宽大于不同机架中的服务器之间的带宽。因此 HDFS 采用机架感知副本放置策略，对于常见情况，当复制因子为 3 时，HDFS 的放置策略是：</p>
<p>在写入程序位于 <code>datanode</code> 上时，就优先将写入文件的一个副本放置在该 <code>datanode</code> 上，否则放在随机 <code>datanode</code> 上。之后在另一个远程机架上的任意一个节点上放置另一个副本，并在该机架上的另一个节点上放置最后一个副本。此策略可以减少机架间的写入流量，从而提高写入性能。</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hdfs-机架.png"/> </div>
<p>如果复制因子大于 3，则随机确定第 4 个和之后副本的放置位置，同时保持每个机架的副本数量低于上限，上限值通常为 <code>（复制系数 - 1）/机架数量 + 2</code>，需要注意的是不允许同一个 <code>dataNode</code> 上具有同一个块的多个副本。</p>
<h3 id="2-5-副本的选择">2.5  副本的选择</h3>
<p>为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器最近的副本。如果在与读取器节点相同的机架上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择本地数据中心上的副本。</p>
<h3 id="2-6-架构的稳定性">2.6 架构的稳定性</h3>
<h4 id="1-心跳机制和重新复制">1. 心跳机制和重新复制</h4>
<p>每个 DataNode 定期向 NameNode 发送心跳消息，如果超过指定时间没有收到心跳消息，则将 DataNode 标记为死亡。NameNode 不会将任何新的 IO 请求转发给标记为死亡的 DataNode，也不会再使用这些 DataNode 上的数据。 由于数据不再可用，可能会导致某些块的复制因子小于其指定值，NameNode 会跟踪这些块，并在必要的时候进行重新复制。</p>
<h4 id="2-数据的完整性">2. 数据的完整性</h4>
<p>由于存储设备故障等原因，存储在 DataNode 上的数据块也会发生损坏。为了避免读取到已经损坏的数据而导致错误，HDFS 提供了数据完整性校验机制来保证数据的完整性，具体操作如下：</p>
<p>当客户端创建 HDFS 文件时，它会计算文件的每个块的 <code>校验和</code>，并将 <code>校验和</code> 存储在同一 HDFS 命名空间下的单独的隐藏文件中。当客户端检索文件内容时，它会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的 <code>校验和</code> 匹配。如果匹配失败，则证明数据已经损坏，此时客户端会选择从其他 DataNode 获取该块的其他可用副本。</p>
<h4 id="3-元数据的磁盘故障">3.元数据的磁盘故障</h4>
<p><code>FsImage</code> 和 <code>EditLog</code> 是 HDFS 的核心数据，这些数据的意外丢失可能会导致整个 HDFS 服务不可用。为了避免这个问题，可以配置 NameNode 使其支持 <code>FsImage</code> 和 <code>EditLog</code> 多副本同步，这样 <code>FsImage</code> 或 <code>EditLog</code> 的任何改变都会引起每个副本 <code>FsImage</code> 和 <code>EditLog</code> 的同步更新。</p>
<h4 id="4-支持快照">4.支持快照</h4>
<p>快照支持在特定时刻存储数据副本，在数据意外损坏时，可以通过回滚操作恢复到健康的数据状态。</p>
<h2 id="三、HDFS-的特点">三、HDFS 的特点</h2>
<h3 id="3-1-高容错">3.1 高容错</h3>
<p>由于 HDFS 采用数据的多副本方案，所以部分硬件的损坏不会导致全部数据的丢失。</p>
<h3 id="3-2-高吞吐量">3.2 高吞吐量</h3>
<p>HDFS 设计的重点是支持高吞吐量的数据访问，而不是低延迟的数据访问。</p>
<h3 id="3-3-大文件支持">3.3  大文件支持</h3>
<p>HDFS 适合于大文件的存储，文档的大小应该是是 GB 到 TB 级别的。</p>
<h3 id="3-3-简单一致性模型">3.3 简单一致性模型</h3>
<p>HDFS 更适合于一次写入多次读取 (write-once-read-many) 的访问模型。支持将内容追加到文件末尾，但不支持数据的随机访问，不能从文件任意位置新增数据。</p>
<h3 id="3-4-跨平台移植性">3.4 跨平台移植性</h3>
<p>HDFS 具有良好的跨平台移植性，这使得其他大数据计算框架都将其作为数据持久化存储的首选方案。</p>
<h2 id="附：图解HDFS存储原理">附：图解HDFS存储原理</h2>
<blockquote>
<p>说明：以下图片引用自博客：<a href="https://blog.csdn.net/hudiefenmu/article/details/37655491">翻译经典 HDFS 原理讲解漫画</a></p>
</blockquote>
<h3 id="1-HDFS写数据原理">1. HDFS写数据原理</h3>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hdfs-write-1.jpg"/> </div>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hdfs-write-2.jpg"/> </div>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hdfs-write-3.jpg"/> </div>
<h3 id="2-HDFS读数据原理">2. HDFS读数据原理</h3>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hdfs-read-1.jpg"/> </div>
<h3 id="3-HDFS故障类型和其检测方法">3. HDFS故障类型和其检测方法</h3>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hdfs-tolerance-1.jpg"/> </div>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hdfs-tolerance-2.jpg"/> </div>
<p><strong>第二部分：读写故障的处理</strong></p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hdfs-tolerance-3.jpg"/> </div>
<p><strong>第三部分：DataNode 故障处理</strong></p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hdfs-tolerance-4.jpg"/> </div>
<p><strong>副本布局策略</strong>：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hdfs-tolerance-5.jpg"/> </div>
<h2 id="参考资料-9">参考资料</h2>
<ol>
<li class="lvl-3">
<p><a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">Apache Hadoop 2.9.2 &gt; HDFS Architecture</a></p>
</li>
<li class="lvl-3">
<p>Tom White . hadoop 权威指南 [M] . 清华大学出版社 . 2017.</p>
</li>
<li class="lvl-3">
<p><a href="https://blog.csdn.net/hudiefenmu/article/details/37655491">翻译经典 HDFS 原理讲解漫画</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop集群环境搭建</title>
    <url>/2021/10/15/Hadoop%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h2 id="一、集群规划">一、集群规划</h2>
<p>这里搭建一个 3 节点的 Hadoop 集群，其中三台主机均部署 <code>DataNode</code> 和 <code>NodeManager</code> 服务，但只有 hadoop001 上部署 <code>NameNode</code> 和 <code>ResourceManager</code> 服务。</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hadoop集群规划.png"/> </div>
<h2 id="二、前置条件">二、前置条件</h2>
<p>Hadoop 的运行依赖 JDK，需要预先安装。其安装步骤单独整理至：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a href="https://github.com/oicio/BigData-Notes/blob/master/notes/installation/Linux%E4%B8%8BJDK%E5%AE%89%E8%A3%85.md">Linux 下 JDK 的安装</a></p>
</li>
</ul>
<h2 id="三、配置免密登录">三、配置免密登录</h2>
<h3 id="3-1-生成密匙">3.1 生成密匙</h3>
<p>在每台主机上使用 <code>ssh-keygen</code> 命令生成公钥私钥对：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure>
<h3 id="3-2-免密登录">3.2 免密登录</h3>
<p>将 <code>hadoop001</code> 的公钥写到本机和远程机器的 <code> ~/ .ssh/authorized_key</code> 文件中：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop001</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop002</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop003</span><br></pre></td></tr></table></figure>
<h3 id="3-3-验证免密登录">3.3 验证免密登录</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh hadoop002</span><br><span class="line">ssh hadoop003</span><br></pre></td></tr></table></figure>
<h2 id="四、集群搭建">四、集群搭建</h2>
<h3 id="3-1-下载并解压">3.1 下载并解压</h3>
<p>下载 Hadoop。这里我下载的是 CDH 版本 Hadoop，下载地址为：<a href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">tar -zvxf hadoop-2.6.0-cdh5.15.2.tar.gz</span> </span><br></pre></td></tr></table></figure>
<h3 id="3-2-配置环境变量">3.2 配置环境变量</h3>
<p>编辑 <code>profile</code> 文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">vim /etc/profile</span></span><br></pre></td></tr></table></figure>
<p>增加如下配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2</span><br><span class="line">export  PATH=$&#123;HADOOP_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure>
<p>执行 <code>source</code> 命令，使得配置立即生效：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure>
<h3 id="3-3-修改配置">3.3 修改配置</h3>
<p>进入 <code>$&#123;HADOOP_HOME&#125;/etc/hadoop</code> 目录下，修改配置文件。各个配置文件内容如下：</p>
<h4 id="1-hadoop-env-sh">1. <a href="http://hadoop-env.sh">hadoop-env.sh</a></h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">指定JDK的安装位置</span></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_201/</span><br></pre></td></tr></table></figure>
<h4 id="2-core-site-xml">2.  core-site.xml</h4>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--指定 namenode 的 hdfs 协议文件系统的通信地址--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop001:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--指定 hadoop 集群存储临时文件的目录--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="3-hdfs-site-xml">3. hdfs-site.xml</h4>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--namenode 节点数据（即元数据）的存放位置，可以指定多个目录实现容错，多个目录用逗号分隔--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/namenode/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--datanode 节点数据（即数据块）的存放位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/datanode/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="4-yarn-site-xml">4. yarn-site.xml</h4>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--配置 NodeManager 上运行的附属服务。需要配置成 mapreduce_shuffle 后才可以在 Yarn 上运行 MapReduce 程序。--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--resourcemanager 的主机名--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="5-mapred-site-xml">5.  mapred-site.xml</h4>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--指定 mapreduce 作业运行在 yarn 上--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="5-slaves">5. slaves</h4>
<p>配置所有从属节点的主机名或 IP 地址，每行一个。所有从属节点上的 <code>DataNode</code> 服务和 <code>NodeManager</code> 服务都会被启动。</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">hadoop001</span></span><br><span class="line"><span class="attr">hadoop002</span></span><br><span class="line"><span class="attr">hadoop003</span></span><br></pre></td></tr></table></figure>
<h3 id="3-4-分发程序">3.4 分发程序</h3>
<p>将 Hadoop 安装包分发到其他两台服务器，分发后建议在这两台服务器上也配置一下 Hadoop 的环境变量。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">将安装包分发到hadoop002</span></span><br><span class="line">scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/  hadoop002:/usr/app/</span><br><span class="line"><span class="meta"># </span><span class="language-bash">将安装包分发到hadoop003</span></span><br><span class="line">scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/  hadoop003:/usr/app/</span><br></pre></td></tr></table></figure>
<h3 id="3-5-初始化">3.5  初始化</h3>
<p>在 <code>Hadoop001</code> 上执行 namenode 初始化命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>
<h3 id="3-6-启动集群">3.6 启动集群</h3>
<p>进入到 <code>Hadoop001</code> 的 <code>$&#123;HADOOP_HOME&#125;/sbin</code> 目录下，启动 Hadoop。此时 <code>hadoop002</code> 和 <code>hadoop003</code> 上的相关服务也会被启动：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">启动dfs服务</span></span><br><span class="line">start-dfs.sh</span><br><span class="line"><span class="meta"># </span><span class="language-bash">启动yarn服务</span></span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure>
<h3 id="3-7-查看集群">3.7 查看集群</h3>
<p>在每台服务器上使用 <code>jps</code> 命令查看服务进程，或直接进入 Web-UI 界面进行查看，端口为 <code>50070</code>。可以看到此时有三个可用的 <code>Datanode</code>：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hadoop-集群环境搭建.png"/> </div>
<BR/>
<p>点击 <code>Live Nodes</code> 进入，可以看到每个 <code>DataNode</code> 的详细情况：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hadoop-集群搭建2.png"/> </div>
<BR/>
<p>接着可以查看 Yarn 的情况，端口号为 <code>8088</code> ：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hadoop-集群搭建3.png"/> </div>
<h2 id="五、提交服务到集群">五、提交服务到集群</h2>
<p>提交作业到集群的方式和单机环境完全一致，这里以提交 Hadoop 内置的计算 Pi 的示例程序为例，在任何一个节点上执行都可以，命令如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar /usr/app/hadoop-2.6.0-cdh5.15.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.15.2.jar  pi  3  3</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase JAVA API使用</title>
    <url>/2021/10/25/Hbase%20JAVA%20API%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="一、简述">一、简述</h2>
<p>截至到目前 (2019.04)，HBase 有两个主要的版本，分别是 1.x 和 2.x ，两个版本的 Java API 有所不同，1.x 中某些方法在 2.x 中被标识为 <code>@deprecated</code> 过时。所以下面关于 API 的样例，我会分别给出 1.x 和 2.x 两个版本。完整的代码见本仓库：</p>
<blockquote>
<ul class="lvl-1">
<li class="lvl-2">
<p><a href="https://github.com/ihadyou/BigData-Notes/tree/master/code/Hbase/hbase-java-api-1.x">Java API 1.x Examples</a></p>
</li>
<li class="lvl-2">
<p><a href="https://github.com/ihadyou/BigData-Notes/tree/master/code/Hbase/hbase-java-api-2.x">Java API 2.x Examples</a></p>
</li>
</ul>
</blockquote>
<p>同时你使用的客户端的版本必须与服务端版本保持一致，如果用 2.x 版本的客户端代码去连接 1.x 版本的服务端，会抛出 <code>NoSuchColumnFamilyException</code> 等异常。</p>
<h2 id="二、Java-API-1-x-基本使用">二、Java API 1.x 基本使用</h2>
<h4 id="2-1-新建Maven工程，导入项目依赖">2.1 新建Maven工程，导入项目依赖</h4>
<p>要使用 Java API 操作 HBase，需要引入 <code>hbase-client</code>。这里选取的 <code>HBase Client</code> 的版本为 <code>1.2.0</code>。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="2-2-API-基本使用">2.2 API 基本使用</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HBaseUtils</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> HBaseConfiguration.create();</span><br><span class="line">        configuration.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>);</span><br><span class="line">        <span class="comment">// 如果是集群 则主机名用逗号分隔</span></span><br><span class="line">        configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建 HBase 表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName      表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilies 列族的数组</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">createTable</span><span class="params">(String tableName, List&lt;String&gt; columnFamilies)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">HBaseAdmin</span> <span class="variable">admin</span> <span class="operator">=</span> (HBaseAdmin) connection.getAdmin();</span><br><span class="line">            <span class="keyword">if</span> (admin.tableExists(tableName)) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">HTableDescriptor</span> <span class="variable">tableDescriptor</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HTableDescriptor</span>(TableName.valueOf(tableName));</span><br><span class="line">            columnFamilies.forEach(columnFamily -&gt; &#123;</span><br><span class="line">                <span class="type">HColumnDescriptor</span> <span class="variable">columnDescriptor</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HColumnDescriptor</span>(columnFamily);</span><br><span class="line">                columnDescriptor.setMaxVersions(<span class="number">1</span>);</span><br><span class="line">                tableDescriptor.addFamily(columnDescriptor);</span><br><span class="line">            &#125;);</span><br><span class="line">            admin.createTable(tableDescriptor);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除 hBase 表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">deleteTable</span><span class="params">(String tableName)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">HBaseAdmin</span> <span class="variable">admin</span> <span class="operator">=</span> (HBaseAdmin) connection.getAdmin();</span><br><span class="line">            <span class="comment">// 删除表前需要先禁用表</span></span><br><span class="line">            admin.disableTable(tableName);</span><br><span class="line">            admin.deleteTable(tableName);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 插入数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName        表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey           唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilyName 列族名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier        列标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value            数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">putRow</span><span class="params">(String tableName, String rowKey, String columnFamilyName, String qualifier,</span></span><br><span class="line"><span class="params">                                 String value)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Put</span> <span class="variable">put</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Put</span>(Bytes.toBytes(rowKey));</span><br><span class="line">            put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(qualifier), Bytes.toBytes(value));</span><br><span class="line">            table.put(put);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 插入数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName        表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey           唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilyName 列族名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> pairList         列标识和值的集合</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">putRow</span><span class="params">(String tableName, String rowKey, String columnFamilyName, List&lt;Pair&lt;String, String&gt;&gt; pairList)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Put</span> <span class="variable">put</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Put</span>(Bytes.toBytes(rowKey));</span><br><span class="line">            pairList.forEach(pair -&gt; put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(pair.getKey()), Bytes.toBytes(pair.getValue())));</span><br><span class="line">            table.put(put);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 根据 rowKey 获取指定行的数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey    唯一标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Result <span class="title function_">getRow</span><span class="params">(String tableName, String rowKey)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Get</span> <span class="variable">get</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Get</span>(Bytes.toBytes(rowKey));</span><br><span class="line">            <span class="keyword">return</span> table.get(get);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取指定行指定列 (cell) 的最新版本的数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName    表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey       唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamily 列族</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier    列标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String <span class="title function_">getCell</span><span class="params">(String tableName, String rowKey, String columnFamily, String qualifier)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Get</span> <span class="variable">get</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Get</span>(Bytes.toBytes(rowKey));</span><br><span class="line">            <span class="keyword">if</span> (!get.isCheckExistenceOnly()) &#123;</span><br><span class="line">                get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier));</span><br><span class="line">                <span class="type">Result</span> <span class="variable">result</span> <span class="operator">=</span> table.get(get);</span><br><span class="line">                <span class="type">byte</span>[] resultValue = result.getValue(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier));</span><br><span class="line">                <span class="keyword">return</span> Bytes.toString(resultValue);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索全表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title function_">getScanner</span><span class="params">(String tableName)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索表中指定数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName  表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> filterList 过滤器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title function_">getScanner</span><span class="params">(String tableName, FilterList filterList)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">            scan.setFilter(filterList);</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索表中指定数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName   表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> startRowKey 起始 RowKey</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> endRowKey   终止 RowKey</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> filterList  过滤器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title function_">getScanner</span><span class="params">(String tableName, String startRowKey, String endRowKey,</span></span><br><span class="line"><span class="params">                                           FilterList filterList)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">            scan.setStartRow(Bytes.toBytes(startRowKey));</span><br><span class="line">            scan.setStopRow(Bytes.toBytes(endRowKey));</span><br><span class="line">            scan.setFilter(filterList);</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除指定行记录</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey    唯一标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">deleteRow</span><span class="params">(String tableName, String rowKey)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Delete</span> <span class="variable">delete</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Delete</span>(Bytes.toBytes(rowKey));</span><br><span class="line">            table.delete(delete);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除指定行的指定列</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName  表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey     唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> familyName 列族</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier  列标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">deleteColumn</span><span class="params">(String tableName, String rowKey, String familyName,</span></span><br><span class="line"><span class="params">                                          String qualifier)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Delete</span> <span class="variable">delete</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Delete</span>(Bytes.toBytes(rowKey));</span><br><span class="line">            delete.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(qualifier));</span><br><span class="line">            table.delete(delete);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-单元测试">2.3 单元测试</h3>
<p>以单元测试的方式对上面封装的 API 进行测试。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HBaseUtilsTest</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">TABLE_NAME</span> <span class="operator">=</span> <span class="string">&quot;class&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">TEACHER</span> <span class="operator">=</span> <span class="string">&quot;teacher&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">STUDENT</span> <span class="operator">=</span> <span class="string">&quot;student&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">createTable</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 新建表</span></span><br><span class="line">        List&lt;String&gt; columnFamilies = Arrays.asList(TEACHER, STUDENT);</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">table</span> <span class="operator">=</span> HBaseUtils.createTable(TABLE_NAME, columnFamilies);</span><br><span class="line">        System.out.println(<span class="string">&quot;表创建结果:&quot;</span> + table);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">insertData</span><span class="params">()</span> &#123;</span><br><span class="line">        List&lt;Pair&lt;String, String&gt;&gt; pairs1 = Arrays.asList(<span class="keyword">new</span> <span class="title class_">Pair</span>&lt;&gt;(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;Tom&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Pair</span>&lt;&gt;(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;22&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Pair</span>&lt;&gt;(<span class="string">&quot;gender&quot;</span>, <span class="string">&quot;1&quot;</span>));</span><br><span class="line">        HBaseUtils.putRow(TABLE_NAME, <span class="string">&quot;rowKey1&quot;</span>, STUDENT, pairs1);</span><br><span class="line"></span><br><span class="line">        List&lt;Pair&lt;String, String&gt;&gt; pairs2 = Arrays.asList(<span class="keyword">new</span> <span class="title class_">Pair</span>&lt;&gt;(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;Jack&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Pair</span>&lt;&gt;(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;33&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Pair</span>&lt;&gt;(<span class="string">&quot;gender&quot;</span>, <span class="string">&quot;2&quot;</span>));</span><br><span class="line">        HBaseUtils.putRow(TABLE_NAME, <span class="string">&quot;rowKey2&quot;</span>, STUDENT, pairs2);</span><br><span class="line"></span><br><span class="line">        List&lt;Pair&lt;String, String&gt;&gt; pairs3 = Arrays.asList(<span class="keyword">new</span> <span class="title class_">Pair</span>&lt;&gt;(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;Mike&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Pair</span>&lt;&gt;(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;44&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">Pair</span>&lt;&gt;(<span class="string">&quot;gender&quot;</span>, <span class="string">&quot;1&quot;</span>));</span><br><span class="line">        HBaseUtils.putRow(TABLE_NAME, <span class="string">&quot;rowKey3&quot;</span>, STUDENT, pairs3);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">getRow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">Result</span> <span class="variable">result</span> <span class="operator">=</span> HBaseUtils.getRow(TABLE_NAME, <span class="string">&quot;rowKey1&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (result != <span class="literal">null</span>) &#123;</span><br><span class="line">            System.out.println(Bytes</span><br><span class="line">                    .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes(<span class="string">&quot;name&quot;</span>))));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">getCell</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">cell</span> <span class="operator">=</span> HBaseUtils.getCell(TABLE_NAME, <span class="string">&quot;rowKey2&quot;</span>, STUDENT, <span class="string">&quot;age&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;cell age :&quot;</span> + cell);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">getScanner</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> HBaseUtils.getScanner(TABLE_NAME);</span><br><span class="line">        <span class="keyword">if</span> (scanner != <span class="literal">null</span>) &#123;</span><br><span class="line">            scanner.forEach(result -&gt; System.out.println(Bytes.toString(result.getRow()) + <span class="string">&quot;-&gt;&quot;</span> + Bytes</span><br><span class="line">                    .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes(<span class="string">&quot;name&quot;</span>)))));</span><br><span class="line">            scanner.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">getScannerWithFilter</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">FilterList</span> <span class="variable">filterList</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FilterList</span>(FilterList.Operator.MUST_PASS_ALL);</span><br><span class="line">        <span class="type">SingleColumnValueFilter</span> <span class="variable">nameFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SingleColumnValueFilter</span>(Bytes.toBytes(STUDENT),</span><br><span class="line">                Bytes.toBytes(<span class="string">&quot;name&quot;</span>), CompareOperator.EQUAL, Bytes.toBytes(<span class="string">&quot;Jack&quot;</span>));</span><br><span class="line">        filterList.addFilter(nameFilter);</span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> HBaseUtils.getScanner(TABLE_NAME, filterList);</span><br><span class="line">        <span class="keyword">if</span> (scanner != <span class="literal">null</span>) &#123;</span><br><span class="line">            scanner.forEach(result -&gt; System.out.println(Bytes.toString(result.getRow()) + <span class="string">&quot;-&gt;&quot;</span> + Bytes</span><br><span class="line">                    .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes(<span class="string">&quot;name&quot;</span>)))));</span><br><span class="line">            scanner.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">deleteColumn</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> HBaseUtils.deleteColumn(TABLE_NAME, <span class="string">&quot;rowKey2&quot;</span>, STUDENT, <span class="string">&quot;age&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;删除结果: &quot;</span> + b);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">deleteRow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> HBaseUtils.deleteRow(TABLE_NAME, <span class="string">&quot;rowKey2&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;删除结果: &quot;</span> + b);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">deleteTable</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> HBaseUtils.deleteTable(TABLE_NAME);</span><br><span class="line">        System.out.println(<span class="string">&quot;删除结果: &quot;</span> + b);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="三、Java-API-2-x-基本使用">三、Java API 2.x 基本使用</h2>
<h4 id="3-1-新建Maven工程，导入项目依赖">3.1 新建Maven工程，导入项目依赖</h4>
<p>这里选取的 <code>HBase Client</code> 的版本为最新的 <code>2.1.4</code>。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="3-2-API-的基本使用">3.2 API 的基本使用</h4>
<p>2.x 版本相比于 1.x 废弃了一部分方法，关于废弃的方法在源码中都会指明新的替代方法，比如，在 2.x 中创建表时：<code>HTableDescriptor</code> 和 <code>HColumnDescriptor</code> 等类都标识为废弃，取而代之的是使用 <code>TableDescriptorBuilder</code> 和 <code>ColumnFamilyDescriptorBuilder</code> 来定义表和列族。</p>
<div align="center"> <img width="700px"  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/deprecated.png"/> </div>
<p>以下为 HBase  2.x 版本 Java API 的使用示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HBaseUtils</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> HBaseConfiguration.create();</span><br><span class="line">        configuration.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>);</span><br><span class="line">        <span class="comment">// 如果是集群 则主机名用逗号分隔</span></span><br><span class="line">        configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建 HBase 表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName      表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilies 列族的数组</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">createTable</span><span class="params">(String tableName, List&lt;String&gt; columnFamilies)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">HBaseAdmin</span> <span class="variable">admin</span> <span class="operator">=</span> (HBaseAdmin) connection.getAdmin();</span><br><span class="line">            <span class="keyword">if</span> (admin.tableExists(TableName.valueOf(tableName))) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">TableDescriptorBuilder</span> <span class="variable">tableDescriptor</span> <span class="operator">=</span> TableDescriptorBuilder.newBuilder(TableName.valueOf(tableName));</span><br><span class="line">            columnFamilies.forEach(columnFamily -&gt; &#123;</span><br><span class="line">                <span class="type">ColumnFamilyDescriptorBuilder</span> <span class="variable">cfDescriptorBuilder</span> <span class="operator">=</span> ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(columnFamily));</span><br><span class="line">                cfDescriptorBuilder.setMaxVersions(<span class="number">1</span>);</span><br><span class="line">                <span class="type">ColumnFamilyDescriptor</span> <span class="variable">familyDescriptor</span> <span class="operator">=</span> cfDescriptorBuilder.build();</span><br><span class="line">                tableDescriptor.setColumnFamily(familyDescriptor);</span><br><span class="line">            &#125;);</span><br><span class="line">            admin.createTable(tableDescriptor.build());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除 hBase 表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">deleteTable</span><span class="params">(String tableName)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">HBaseAdmin</span> <span class="variable">admin</span> <span class="operator">=</span> (HBaseAdmin) connection.getAdmin();</span><br><span class="line">            <span class="comment">// 删除表前需要先禁用表</span></span><br><span class="line">            admin.disableTable(TableName.valueOf(tableName));</span><br><span class="line">            admin.deleteTable(TableName.valueOf(tableName));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 插入数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName        表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey           唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilyName 列族名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier        列标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value            数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">putRow</span><span class="params">(String tableName, String rowKey, String columnFamilyName, String qualifier,</span></span><br><span class="line"><span class="params">                                 String value)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Put</span> <span class="variable">put</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Put</span>(Bytes.toBytes(rowKey));</span><br><span class="line">            put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(qualifier), Bytes.toBytes(value));</span><br><span class="line">            table.put(put);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 插入数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName        表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey           唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilyName 列族名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> pairList         列标识和值的集合</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">putRow</span><span class="params">(String tableName, String rowKey, String columnFamilyName, List&lt;Pair&lt;String, String&gt;&gt; pairList)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Put</span> <span class="variable">put</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Put</span>(Bytes.toBytes(rowKey));</span><br><span class="line">            pairList.forEach(pair -&gt; put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(pair.getKey()), Bytes.toBytes(pair.getValue())));</span><br><span class="line">            table.put(put);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 根据 rowKey 获取指定行的数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey    唯一标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Result <span class="title function_">getRow</span><span class="params">(String tableName, String rowKey)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Get</span> <span class="variable">get</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Get</span>(Bytes.toBytes(rowKey));</span><br><span class="line">            <span class="keyword">return</span> table.get(get);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取指定行指定列 (cell) 的最新版本的数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName    表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey       唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamily 列族</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier    列标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String <span class="title function_">getCell</span><span class="params">(String tableName, String rowKey, String columnFamily, String qualifier)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Get</span> <span class="variable">get</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Get</span>(Bytes.toBytes(rowKey));</span><br><span class="line">            <span class="keyword">if</span> (!get.isCheckExistenceOnly()) &#123;</span><br><span class="line">                get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier));</span><br><span class="line">                <span class="type">Result</span> <span class="variable">result</span> <span class="operator">=</span> table.get(get);</span><br><span class="line">                <span class="type">byte</span>[] resultValue = result.getValue(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier));</span><br><span class="line">                <span class="keyword">return</span> Bytes.toString(resultValue);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索全表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title function_">getScanner</span><span class="params">(String tableName)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索表中指定数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName  表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> filterList 过滤器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title function_">getScanner</span><span class="params">(String tableName, FilterList filterList)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">            scan.setFilter(filterList);</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索表中指定数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName   表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> startRowKey 起始 RowKey</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> endRowKey   终止 RowKey</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> filterList  过滤器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title function_">getScanner</span><span class="params">(String tableName, String startRowKey, String endRowKey,</span></span><br><span class="line"><span class="params">                                           FilterList filterList)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">            scan.withStartRow(Bytes.toBytes(startRowKey));</span><br><span class="line">            scan.withStopRow(Bytes.toBytes(endRowKey));</span><br><span class="line">            scan.setFilter(filterList);</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除指定行记录</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey    唯一标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">deleteRow</span><span class="params">(String tableName, String rowKey)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Delete</span> <span class="variable">delete</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Delete</span>(Bytes.toBytes(rowKey));</span><br><span class="line">            table.delete(delete);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除指定行指定列</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName  表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey     唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> familyName 列族</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier  列标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">deleteColumn</span><span class="params">(String tableName, String rowKey, String familyName,</span></span><br><span class="line"><span class="params">                                          String qualifier)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            <span class="type">Delete</span> <span class="variable">delete</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Delete</span>(Bytes.toBytes(rowKey));</span><br><span class="line">            delete.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(qualifier));</span><br><span class="line">            table.delete(delete);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="四、正确连接Hbase">四、正确连接Hbase</h2>
<p>在上面的代码中，在类加载时就初始化了 Connection 连接，并且之后的方法都是复用这个 Connection，这时我们可能会考虑是否可以使用自定义连接池来获取更好的性能表现？实际上这是没有必要的。</p>
<p>首先官方对于 <code>Connection</code> 的使用说明如下：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">Connection</span> <span class="string">Pooling For applications which require high-end multithreaded   </span></span><br><span class="line"><span class="attr">access</span> <span class="string">(e.g., web-servers or  application servers  that may serve many   </span></span><br><span class="line"><span class="attr">application</span> <span class="string">threads in a single JVM), you can pre-create a Connection,   </span></span><br><span class="line"><span class="attr">as</span> <span class="string">shown in the following example:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">对于高并发多线程访问的应用程序（例如，在单个</span> <span class="string">JVM 中存在的为多个线程服务的 Web 服务器或应用程序服务器），  </span></span><br><span class="line"><span class="attr">您只需要预先创建一个</span> <span class="string">Connection。例子如下：</span></span><br><span class="line"></span><br><span class="line"> <span class="attr">//Create</span> <span class="string">a connection to the cluster.</span></span><br><span class="line"><span class="attr">Configuration</span> <span class="string">conf = HBaseConfiguration.create();</span></span><br><span class="line"><span class="attr">try</span> <span class="string">(Connection connection = ConnectionFactory.createConnection(conf);</span></span><br><span class="line">     <span class="attr">Table</span> <span class="string">table = connection.getTable(TableName.valueOf(tablename))) &#123;</span></span><br><span class="line">   <span class="attr">//use</span> <span class="string">table as needed, the table returned is lightweight</span></span><br><span class="line"><span class="attr">&#125;</span></span><br></pre></td></tr></table></figure>
<p>之所以能这样使用，这是因为 Connection 并不是一个简单的 socket 连接，<a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Connection.html">接口文档</a> 中对 Connection 的表述是：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">A</span> <span class="string">cluster connection encapsulating lower level individual connections to actual servers and a  </span></span><br><span class="line"><span class="attr">connection</span> <span class="string">to zookeeper.  Connections are instantiated through the ConnectionFactory class.  </span></span><br><span class="line"><span class="attr">The</span> <span class="string">lifecycle of the connection is managed by the caller,  who has to close() the connection   </span></span><br><span class="line"><span class="attr">to</span> <span class="string">release the resources. </span></span><br><span class="line"></span><br><span class="line"><span class="attr">Connection</span> <span class="string">是一个集群连接，封装了与多台服务器（Matser/Region Server）的底层连接以及与 zookeeper 的连接。  </span></span><br><span class="line"><span class="attr">连接通过</span> <span class="string">ConnectionFactory  类实例化。连接的生命周期由调用者管理，调用者必须使用 close() 关闭连接以释放资源。</span></span><br></pre></td></tr></table></figure>
<p>之所以封装这些连接，是因为 HBase 客户端需要连接三个不同的服务角色：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>Zookeeper</strong> ：主要用于获取 <code>meta</code> 表的位置信息，Master 的信息；</p>
</li>
<li class="lvl-2">
<p><strong>HBase Master</strong> ：主要用于执行 HBaseAdmin 接口的一些操作，例如建表等；</p>
</li>
<li class="lvl-2">
<p><strong>HBase RegionServer</strong> ：用于读、写数据。</p>
</li>
</ul>
<div align="center"> <img width="700px"  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-arc.png"/> </div>
<p>Connection 对象和实际的 Socket 连接之间的对应关系如下图：</p>
<div align="center"> <img width="700px"   src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-connection.png"/> </div>
<blockquote>
<p>上面两张图片引用自博客：<a href="https://yq.aliyun.com/articles/581702?spm=a2c4e.11157919.spm-cont-list.1.146c27aeFxoMsN%20%E8%BF%9E%E6%8E%A5HBase%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF">连接 HBase 的正确姿势</a></p>
</blockquote>
<p>在 HBase 客户端代码中，真正对应 Socket 连接的是 <code>RpcConnection</code> 对象。HBase 使用 <code>PoolMap</code> 这种数据结构来存储客户端到 HBase 服务器之间的连接。<code>PoolMap</code> 的内部有一个 <code>ConcurrentHashMap</code> 实例，其 key 是 <code>ConnectionId</code>(封装了服务器地址和用户 ticket)，value 是一个 <code>RpcConnection</code> 对象的资源池。当 HBase 需要连接一个服务器时，首先会根据 <code>ConnectionId</code> 找到对应的连接池，然后从连接池中取出一个连接对象。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@InterfaceAudience</span>.Private</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">PoolMap</span>&lt;K, V&gt; <span class="keyword">implements</span> <span class="title class_">Map</span>&lt;K, V&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> PoolType poolType;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> poolMaxSize;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Map&lt;K, Pool&lt;V&gt;&gt; pools = <span class="keyword">new</span> <span class="title class_">ConcurrentHashMap</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">PoolMap</span><span class="params">(PoolType poolType)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.poolType = poolType;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>HBase 中提供了三种资源池的实现，分别是 <code>Reusable</code>，<code>RoundRobin</code> 和 <code>ThreadLocal</code>。具体实现可以通 <code>hbase.client.ipc.pool.type</code> 配置项指定，默认为 <code>Reusable</code>。连接池的大小也可以通过 <code>hbase.client.ipc.pool.size</code> 配置项指定，默认为 1，即每个 Server 1 个连接。也可以通过修改配置实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">config.set(<span class="string">&quot;hbase.client.ipc.pool.type&quot;</span>,...);</span><br><span class="line">config.set(<span class="string">&quot;hbase.client.ipc.pool.size&quot;</span>,...);</span><br><span class="line">connection = ConnectionFactory.createConnection(config);</span><br></pre></td></tr></table></figure>
<p>由此可以看出 HBase 中 Connection 类已经实现了对连接的管理功能，所以我们不必在 Connection 上在做额外的管理。</p>
<p>另外，Connection 是线程安全的，但 Table 和 Admin 却不是线程安全的，因此正确的做法是一个进程共用一个 Connection 对象，而在不同的线程中使用单独的 Table 和 Admin 对象。Table 和 Admin 的获取操作 <code>getTable()</code> 和 <code>getAdmin()</code> 都是轻量级，所以不必担心性能的消耗，同时建议在使用完成后显示的调用 <code>close()</code> 方法来关闭它们。</p>
<h2 id="参考资料-10">参考资料</h2>
<ol>
<li class="lvl-3">
<p><a href="https://yq.aliyun.com/articles/581702?spm=a2c4e.11157919.spm-cont-list.1.146c27aeFxoMsN%20%E8%BF%9E%E6%8E%A5HBase%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF">连接 HBase 的正确姿势</a></p>
</li>
<li class="lvl-3">
<p><a href="http://hbase.apache.org/book.htm">Apache HBase ™ Reference Guide</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop单机版环境搭建</title>
    <url>/2021/10/15/Hadoop%E5%8D%95%E6%9C%BA%E7%89%88%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h2 id="一、前置条件">一、前置条件</h2>
<p>Hadoop 的运行依赖 JDK，需要预先安装，安装步骤见：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a href="https://github.com/oicio/BigData-Notes/blob/master/notes/installation/Linux%E4%B8%8BJDK%E5%AE%89%E8%A3%85.md">Linux 下 JDK 的安装</a></p>
</li>
</ul>
<h2 id="二、配置免密登录">二、配置免密登录</h2>
<p>Hadoop 组件之间需要基于 SSH 进行通讯。</p>
<h4 id="2-1-配置映射">2.1 配置映射</h4>
<p>配置 ip 地址和主机名映射：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line"><span class="meta"># </span><span class="language-bash">文件末尾增加</span></span><br><span class="line">192.168.43.202  hadoop001</span><br></pre></td></tr></table></figure>
<h3 id="2-2-生成公私钥">2.2  生成公私钥</h3>
<p>执行下面命令行生成公匙和私匙：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>
<h3 id="3-3-授权">3.3 授权</h3>
<p>进入 <code>~/.ssh</code> 目录下，查看生成的公匙和私匙，并将公匙写入到授权文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@@hadoop001 sbin]#  cd ~/.ssh</span><br><span class="line">[root@@hadoop001 .ssh]# ll</span><br><span class="line">-rw-------. 1 root root 1675 3 月  15 09:48 id_rsa</span><br><span class="line">-rw-r--r--. 1 root root  388 3 月  15 09:48 id_rsa.pub</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">写入公匙到授权文件</span></span><br><span class="line">[root@hadoop001 .ssh]# cat id_rsa.pub &gt;&gt; authorized_keys</span><br><span class="line">[root@hadoop001 .ssh]# chmod 600 authorized_keys</span><br></pre></td></tr></table></figure>
<h2 id="三、Hadoop-HDFS-环境搭建">三、Hadoop(HDFS)环境搭建</h2>
<h3 id="3-1-下载并解压-2">3.1 下载并解压</h3>
<p>下载 Hadoop 安装包，这里我下载的是 CDH 版本的，下载地址为：<a href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">解压</span></span><br><span class="line">tar -zvxf hadoop-2.6.0-cdh5.15.2.tar.gz </span><br></pre></td></tr></table></figure>
<h3 id="3-2-配置环境变量-2">3.2 配置环境变量</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">vi /etc/profile</span></span><br></pre></td></tr></table></figure>
<p>配置环境变量：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2</span><br><span class="line">export  PATH=$&#123;HADOOP_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure>
<p>执行 <code>source</code> 命令，使得配置的环境变量立即生效：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure>
<h3 id="3-3-修改Hadoop配置">3.3 修改Hadoop配置</h3>
<p>进入 <code>$&#123;HADOOP_HOME&#125;/etc/hadoop/ </code> 目录下，修改以下配置：</p>
<h4 id="1-hadoop-env-sh-2">1. <a href="http://hadoop-env.sh">hadoop-env.sh</a></h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">JDK安装路径</span></span><br><span class="line">export  JAVA_HOME=/usr/java/jdk1.8.0_201/</span><br></pre></td></tr></table></figure>
<h4 id="2-core-site-xml-2">2. core-site.xml</h4>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--指定 namenode 的 hdfs 协议文件系统的通信地址--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop001:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--指定 hadoop 存储临时文件的目录--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="3-hdfs-site-xml-2">3. hdfs-site.xml</h4>
<p>指定副本系数和临时文件存储位置：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--由于我们这里搭建是单机版本，所以指定 dfs 的副本系数为 1--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="4-slaves">4. slaves</h4>
<p>配置所有从属节点的主机名或 IP 地址，由于是单机版本，所以指定本机即可：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop001</span><br></pre></td></tr></table></figure>
<h3 id="3-4-关闭防火墙">3.4 关闭防火墙</h3>
<p>不关闭防火墙可能导致无法访问 Hadoop 的 Web UI 界面：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">查看防火墙状态</span></span><br><span class="line">sudo firewall-cmd --state</span><br><span class="line"><span class="meta"># </span><span class="language-bash">关闭防火墙:</span></span><br><span class="line">sudo systemctl stop firewalld.service</span><br></pre></td></tr></table></figure>
<h3 id="3-5-初始化-2">3.5 初始化</h3>
<p>第一次启动 Hadoop 时需要进行初始化，进入 <code>$&#123;HADOOP_HOME&#125;/bin/</code> 目录下，执行以下命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 bin]# ./hdfs namenode -format</span><br></pre></td></tr></table></figure>
<h3 id="3-6-启动HDFS">3.6 启动HDFS</h3>
<p>进入 <code>$&#123;HADOOP_HOME&#125;/sbin/</code> 目录下，启动 HDFS：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 sbin]# ./start-dfs.sh</span><br></pre></td></tr></table></figure>
<h3 id="3-7-验证是否启动成功">3.7 验证是否启动成功</h3>
<p>方式一：执行 <code>jps</code> 查看 <code>NameNode</code> 和 <code>DataNode</code> 服务是否已经启动：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 hadoop-2.6.0-cdh5.15.2]# jps</span><br><span class="line">9137 DataNode</span><br><span class="line">9026 NameNode</span><br><span class="line">9390 SecondaryNameNode</span><br></pre></td></tr></table></figure>
<p>方式二：查看 Web UI 界面，端口为 <code>50070</code>：</p>
<div align="center"> <img width="700px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hadoop安装验证.png"/> </div>
<h2 id="四、Hadoop-YARN-环境搭建">四、Hadoop(YARN)环境搭建</h2>
<h3 id="4-1-修改配置">4.1 修改配置</h3>
<p>进入 <code>$&#123;HADOOP_HOME&#125;/etc/hadoop/ </code> 目录下，修改以下配置：</p>
<h4 id="1-mapred-site-xml">1. mapred-site.xml</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">如果没有mapred-site.xml，则拷贝一份样例文件后再修改</span></span><br><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="2-yarn-site-xml">2. yarn-site.xml</h4>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--配置 NodeManager 上运行的附属服务。需要配置成 mapreduce_shuffle 后才可以在 Yarn 上运行 MapReduce 程序。--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-2-启动服务">4.2 启动服务</h3>
<p>进入 <code>$&#123;HADOOP_HOME&#125;/sbin/</code> 目录下，启动 YARN：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./start-yarn.sh</span><br></pre></td></tr></table></figure>
<h4 id="4-3-验证是否启动成功">4.3 验证是否启动成功</h4>
<p>方式一：执行 <code>jps</code> 命令查看 <code>NodeManager</code> 和 <code>ResourceManager</code> 服务是否已经启动：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 hadoop-2.6.0-cdh5.15.2]# jps</span><br><span class="line">9137 DataNode</span><br><span class="line">9026 NameNode</span><br><span class="line">12294 NodeManager</span><br><span class="line">12185 ResourceManager</span><br><span class="line">9390 SecondaryNameNode</span><br></pre></td></tr></table></figure>
<p>方式二：查看 Web UI 界面，端口号为 <code>8088</code>：</p>
<div align="center"> <img width="700px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hadoop-yarn安装验证.png"/> </div>]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase协处理器</title>
    <url>/2021/10/25/Hbase%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8/</url>
    <content><![CDATA[<h2 id="一、简述-2">一、简述</h2>
<p>在使用 HBase 时，如果你的数据量达到了数十亿行或数百万列，此时能否在查询中返回大量数据将受制于网络的带宽，即便网络状况允许，但是客户端的计算处理也未必能够满足要求。在这种情况下，协处理器（Coprocessors）应运而生。它允许你将业务计算代码放入在 RegionServer 的协处理器中，将处理好的数据再返回给客户端，这可以极大地降低需要传输的数据量，从而获得性能上的提升。同时协处理器也允许用户扩展实现 HBase 目前所不具备的功能，如权限校验、二级索引、完整性约束等。</p>
<h2 id="二、协处理器类型">二、协处理器类型</h2>
<h3 id="2-1-Observer协处理器">2.1 Observer协处理器</h3>
<h4 id="1-功能">1. 功能</h4>
<p>Observer 协处理器类似于关系型数据库中的触发器，当发生某些事件的时候这类协处理器会被 Server 端调用。通常可以用来实现下面功能：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>权限校验</strong>：在执行 <code>Get</code> 或 <code>Put</code> 操作之前，您可以使用 <code>preGet</code> 或 <code>prePut</code> 方法检查权限；</p>
</li>
<li class="lvl-2">
<p><strong>完整性约束</strong>： HBase 不支持关系型数据库中的外键功能，可以通过触发器在插入或者删除数据的时候，对关联的数据进行检查；</p>
</li>
<li class="lvl-2">
<p><strong>二级索引</strong>： 可以使用协处理器来维护二级索引。</p>
</li>
</ul>
<h4 id="2-类型">2. 类型</h4>
<p>当前 Observer 协处理器有以下四种类型：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>RegionObserver</strong> :<br>
允许您观察 Region 上的事件，例如 Get 和 Put 操作。</p>
</li>
<li class="lvl-2">
<p><strong>RegionServerObserver</strong> :<br>
允许您观察与 RegionServer 操作相关的事件，例如启动，停止或执行合并，提交或回滚。</p>
</li>
<li class="lvl-2">
<p><strong>MasterObserver</strong> :<br>
允许您观察与 HBase Master 相关的事件，例如表创建，删除或 schema 修改。</p>
</li>
<li class="lvl-2">
<p><strong>WalObserver</strong> :<br>
允许您观察与预写日志（WAL）相关的事件。</p>
</li>
</ul>
<h4 id="3-接口">3. 接口</h4>
<p>以上四种类型的 Observer 协处理器均继承自 <code>Coprocessor</code> 接口，这四个接口中分别定义了所有可用的钩子方法，以便在对应方法前后执行特定的操作。通常情况下，我们并不会直接实现上面接口，而是继承其 Base 实现类，Base 实现类只是简单空实现了接口中的方法，这样我们在实现自定义的协处理器时，就不必实现所有方法，只需要重写必要方法即可。</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-coprocessor.png"/> </div>
<p>这里以 <code>RegionObservers </code> 为例，其接口类中定义了所有可用的钩子方法，下面截取了部分方法的定义，多数方法都是成对出现的，有 <code>pre</code> 就有 <code>post</code>：</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/RegionObserver.png"/> </div>
<h4 id="4-执行流程">4. 执行流程</h4>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/RegionObservers-works.png"/> </div>
<ul class="lvl-0">
<li class="lvl-2">
<p>客户端发出 put 请求</p>
</li>
<li class="lvl-2">
<p>该请求被分派给合适的 RegionServer 和 region</p>
</li>
<li class="lvl-2">
<p>coprocessorHost 拦截该请求，然后在该表的每个 RegionObserver 上调用 prePut()</p>
</li>
<li class="lvl-2">
<p>如果没有被 <code>prePut()</code> 拦截，该请求继续送到 region，然后进行处理</p>
</li>
<li class="lvl-2">
<p>region 产生的结果再次被 CoprocessorHost 拦截，调用 <code>postPut()</code></p>
</li>
<li class="lvl-2">
<p>假如没有 <code>postPut()</code> 拦截该响应，最终结果被返回给客户端</p>
</li>
</ul>
<p>如果大家了解 Spring，可以将这种执行方式类比于其 AOP 的执行原理即可，官方文档当中也是这样类比的：</p>
<blockquote>
<p>If you are familiar with Aspect Oriented Programming (AOP), you can think of a coprocessor as applying advice by intercepting a request and then running some custom code,before passing the request on to its final destination (or even changing the destination).</p>
<p>如果您熟悉面向切面编程（AOP），您可以将协处理器视为通过拦截请求然后运行一些自定义代码来使用 Advice，然后将请求传递到其最终目标（或者更改目标）。</p>
</blockquote>
<h3 id="2-2-Endpoint协处理器">2.2  Endpoint协处理器</h3>
<p>Endpoint 协处理器类似于关系型数据库中的存储过程。客户端可以调用 Endpoint 协处理器在服务端对数据进行处理，然后再返回。</p>
<p>以聚集操作为例，如果没有协处理器，当用户需要找出一张表中的最大数据，即 max 聚合操作，就必须进行全表扫描，然后在客户端上遍历扫描结果，这必然会加重了客户端处理数据的压力。利用 Coprocessor，用户可以将求最大值的代码部署到 HBase Server 端，HBase 将利用底层 cluster 的多个节点并发执行求最大值的操作。即在每个 Region 范围内执行求最大值的代码，将每个 Region 的最大值在 Region Server 端计算出来，仅仅将该 max 值返回给客户端。之后客户端只需要将每个 Region 的最大值进行比较而找到其中最大的值即可。</p>
<h2 id="三、协处理的加载方式">三、协处理的加载方式</h2>
<p>要使用我们自己开发的协处理器，必须通过静态（使用 HBase 配置）或动态（使用 HBase Shell 或 Java API）加载它。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>静态加载的协处理器称之为 <strong>System Coprocessor</strong>（系统级协处理器）,作用范围是整个 HBase 上的所有表，需要重启 HBase 服务；</p>
</li>
<li class="lvl-2">
<p>动态加载的协处理器称之为 <strong>Table Coprocessor</strong>（表处理器），作用于指定的表，不需要重启 HBase 服务。</p>
</li>
</ul>
<p>其加载和卸载方式分别介绍如下。</p>
<h2 id="四、静态加载与卸载">四、静态加载与卸载</h2>
<h3 id="4-1-静态加载">4.1 静态加载</h3>
<p>静态加载分以下三步：</p>
<ol>
<li class="lvl-3">
<p>在 <code>hbase-site.xml</code> 定义需要加载的协处理器。</p>
</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.coprocessor.region.classes<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.myname.hbase.coprocessor.endpoint.SumEndPoint<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><code> &lt;name&gt;</code> 标签的值必须是下面其中之一：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>RegionObservers 和 Endpoints 协处理器：<code>hbase.coprocessor.region.classes</code></p>
</li>
<li class="lvl-2">
<p>WALObservers 协处理器： <code>hbase.coprocessor.wal.classes</code></p>
</li>
<li class="lvl-2">
<p>MasterObservers 协处理器：<code>hbase.coprocessor.master.classes</code></p>
</li>
</ul>
<p><code>&lt;value&gt;</code> 必须是协处理器实现类的全限定类名。如果为加载指定了多个类，则类名必须以逗号分隔。</p>
<ol start="2">
<li class="lvl-3">
<p>将 jar(包含代码和所有依赖项) 放入 HBase 安装目录中的 <code>lib</code> 目录下；</p>
</li>
<li class="lvl-3">
<p>重启 HBase。</p>
</li>
</ol>
<h3 id="4-2-静态卸载">4.2 静态卸载</h3>
<ol>
<li class="lvl-3">
<p>从 hbase-site.xml 中删除配置的协处理器的&lt;property&gt;元素及其子元素；</p>
</li>
<li class="lvl-3">
<p>从类路径或 HBase 的 lib 目录中删除协处理器的 JAR 文件（可选）；</p>
</li>
<li class="lvl-3">
<p>重启 HBase。</p>
</li>
</ol>
<h2 id="五、动态加载与卸载">五、动态加载与卸载</h2>
<p>使用动态加载协处理器，不需要重新启动 HBase。但动态加载的协处理器是基于每个表加载的，只能用于所指定的表。此外，在使用动态加载必须使表脱机（disable）以加载协处理器。动态加载通常有两种方式：Shell 和 Java API 。</p>
<blockquote>
<p>以下示例基于两个前提：</p>
<ol>
<li class="lvl-3">coprocessor.jar 包含协处理器实现及其所有依赖项。</li>
<li class="lvl-3">JAR 包存放在 HDFS 上的路径为：hdfs：// &lt;namenode&gt;：&lt;port&gt; / user / &lt;hadoop-user&gt; /coprocessor.jar</li>
</ol>
</blockquote>
<h3 id="5-1-HBase-Shell动态加载">5.1 HBase Shell动态加载</h3>
<ol>
<li class="lvl-3">
<p>使用 HBase Shell 禁用表</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; disable &#x27;tableName&#x27;</span><br></pre></td></tr></table></figure>
<ol start="2">
<li class="lvl-3">
<p>使用如下命令加载协处理器</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; alter &#x27;tableName&#x27;, METHOD =&gt; &#x27;table_att&#x27;, &#x27;Coprocessor&#x27;=&gt;&#x27;hdfs://&lt;namenode&gt;:&lt;port&gt;/</span><br><span class="line">user/&lt;hadoop-user&gt;/coprocessor.jar| org.myname.hbase.Coprocessor.RegionObserverExample|1073741823|</span><br><span class="line">arg1=1,arg2=2&#x27;</span><br></pre></td></tr></table></figure>
<p><code>Coprocessor</code> 包含由管道（|）字符分隔的四个参数，按顺序解释如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>JAR 包路径</strong>：通常为 JAR 包在 HDFS 上的路径。关于路径以下两点需要注意：</p>
</li>
<li class="lvl-2">
<p>允许使用通配符，例如：<code>hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/*.jar</code> 来添加指定的 JAR 包；</p>
</li>
<li class="lvl-2">
<p>可以使指定目录，例如：<code>hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/</code> ，这会添加目录中的所有 JAR 包，但不会搜索子目录中的 JAR 包。</p>
</li>
<li class="lvl-2">
<p><strong>类名</strong>：协处理器的完整类名。</p>
</li>
<li class="lvl-2">
<p><strong>优先级</strong>：协处理器的优先级，遵循数字的自然序，即值越小优先级越高。可以为空，在这种情况下，将分配默认优先级值。</p>
</li>
<li class="lvl-2">
<p><strong>可选参数</strong> ：传递的协处理器的可选参数。</p>
</li>
</ul>
<ol start="3">
<li class="lvl-3">
<p>启用表</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; enable &#x27;tableName&#x27;</span><br></pre></td></tr></table></figure>
<ol start="4">
<li class="lvl-3">
<p>验证协处理器是否已加载</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; describe &#x27;tableName&#x27;</span><br></pre></td></tr></table></figure>
<p>协处理器出现在 <code>TABLE_ATTRIBUTES</code> 属性中则代表加载成功。</p>
<h3 id="5-2-HBase-Shell动态卸载">5.2 HBase Shell动态卸载</h3>
<ol>
<li class="lvl-3">
<p>禁用表</p>
</li>
</ol>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hbase&gt; </span><span class="language-bash"><span class="built_in">disable</span> <span class="string">&#x27;tableName&#x27;</span></span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li class="lvl-3">
<p>移除表协处理器</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hbase&gt; </span><span class="language-bash">alter <span class="string">&#x27;tableName&#x27;</span>, METHOD =&gt; <span class="string">&#x27;table_att_unset&#x27;</span>, NAME =&gt; <span class="string">&#x27;coprocessor$1&#x27;</span></span></span><br></pre></td></tr></table></figure>
<ol start="3">
<li class="lvl-3">
<p>启用表</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hbase&gt; </span><span class="language-bash"><span class="built_in">enable</span> <span class="string">&#x27;tableName&#x27;</span></span></span><br></pre></td></tr></table></figure>
<h3 id="5-3-Java-API-动态加载">5.3 Java API 动态加载</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">TableName</span> <span class="variable">tableName</span> <span class="operator">=</span> TableName.valueOf(<span class="string">&quot;users&quot;</span>);</span><br><span class="line"><span class="type">String</span> <span class="variable">path</span> <span class="operator">=</span> <span class="string">&quot;hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar&quot;</span>;</span><br><span class="line"><span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> HBaseConfiguration.create();</span><br><span class="line"><span class="type">Connection</span> <span class="variable">connection</span> <span class="operator">=</span> ConnectionFactory.createConnection(conf);</span><br><span class="line"><span class="type">Admin</span> <span class="variable">admin</span> <span class="operator">=</span> connection.getAdmin();</span><br><span class="line">admin.disableTable(tableName);</span><br><span class="line"><span class="type">HTableDescriptor</span> <span class="variable">hTableDescriptor</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HTableDescriptor</span>(tableName);</span><br><span class="line"><span class="type">HColumnDescriptor</span> <span class="variable">columnFamily1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HColumnDescriptor</span>(<span class="string">&quot;personalDet&quot;</span>);</span><br><span class="line">columnFamily1.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily1);</span><br><span class="line"><span class="type">HColumnDescriptor</span> <span class="variable">columnFamily2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HColumnDescriptor</span>(<span class="string">&quot;salaryDet&quot;</span>);</span><br><span class="line">columnFamily2.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily2);</span><br><span class="line">hTableDescriptor.setValue(<span class="string">&quot;COPROCESSOR$1&quot;</span>, path + <span class="string">&quot;|&quot;</span></span><br><span class="line">+ RegionObserverExample.class.getCanonicalName() + <span class="string">&quot;|&quot;</span></span><br><span class="line">+ Coprocessor.PRIORITY_USER);</span><br><span class="line">admin.modifyTable(tableName, hTableDescriptor);</span><br><span class="line">admin.enableTable(tableName);</span><br></pre></td></tr></table></figure>
<p>在 HBase 0.96 及其以后版本中，HTableDescriptor 的 addCoprocessor() 方法提供了一种更为简便的加载方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">TableName</span> <span class="variable">tableName</span> <span class="operator">=</span> TableName.valueOf(<span class="string">&quot;users&quot;</span>);</span><br><span class="line"><span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar&quot;</span>);</span><br><span class="line"><span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> HBaseConfiguration.create();</span><br><span class="line"><span class="type">Connection</span> <span class="variable">connection</span> <span class="operator">=</span> ConnectionFactory.createConnection(conf);</span><br><span class="line"><span class="type">Admin</span> <span class="variable">admin</span> <span class="operator">=</span> connection.getAdmin();</span><br><span class="line">admin.disableTable(tableName);</span><br><span class="line"><span class="type">HTableDescriptor</span> <span class="variable">hTableDescriptor</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HTableDescriptor</span>(tableName);</span><br><span class="line"><span class="type">HColumnDescriptor</span> <span class="variable">columnFamily1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HColumnDescriptor</span>(<span class="string">&quot;personalDet&quot;</span>);</span><br><span class="line">columnFamily1.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily1);</span><br><span class="line"><span class="type">HColumnDescriptor</span> <span class="variable">columnFamily2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HColumnDescriptor</span>(<span class="string">&quot;salaryDet&quot;</span>);</span><br><span class="line">columnFamily2.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily2);</span><br><span class="line">hTableDescriptor.addCoprocessor(RegionObserverExample.class.getCanonicalName(), path,</span><br><span class="line">Coprocessor.PRIORITY_USER, <span class="literal">null</span>);</span><br><span class="line">admin.modifyTable(tableName, hTableDescriptor);</span><br><span class="line">admin.enableTable(tableName);</span><br></pre></td></tr></table></figure>
<h3 id="5-4-Java-API-动态卸载">5.4 Java API 动态卸载</h3>
<p>卸载其实就是重新定义表但不设置协处理器。这会删除所有表上的协处理器。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">TableName</span> <span class="variable">tableName</span> <span class="operator">=</span> TableName.valueOf(<span class="string">&quot;users&quot;</span>);</span><br><span class="line"><span class="type">String</span> <span class="variable">path</span> <span class="operator">=</span> <span class="string">&quot;hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar&quot;</span>;</span><br><span class="line"><span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> HBaseConfiguration.create();</span><br><span class="line"><span class="type">Connection</span> <span class="variable">connection</span> <span class="operator">=</span> ConnectionFactory.createConnection(conf);</span><br><span class="line"><span class="type">Admin</span> <span class="variable">admin</span> <span class="operator">=</span> connection.getAdmin();</span><br><span class="line">admin.disableTable(tableName);</span><br><span class="line"><span class="type">HTableDescriptor</span> <span class="variable">hTableDescriptor</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HTableDescriptor</span>(tableName);</span><br><span class="line"><span class="type">HColumnDescriptor</span> <span class="variable">columnFamily1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HColumnDescriptor</span>(<span class="string">&quot;personalDet&quot;</span>);</span><br><span class="line">columnFamily1.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily1);</span><br><span class="line"><span class="type">HColumnDescriptor</span> <span class="variable">columnFamily2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HColumnDescriptor</span>(<span class="string">&quot;salaryDet&quot;</span>);</span><br><span class="line">columnFamily2.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily2);</span><br><span class="line">admin.modifyTable(tableName, hTableDescriptor);</span><br><span class="line">admin.enableTable(tableName);</span><br></pre></td></tr></table></figure>
<h2 id="六、协处理器案例">六、协处理器案例</h2>
<p>这里给出一个简单的案例，实现一个类似于 Redis 中 <code>append</code> 命令的协处理器，当我们对已有列执行 put 操作时候，HBase 默认执行的是 update 操作，这里我们修改为执行 append 操作。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">redis append 命令示例</span></span><br><span class="line"><span class="meta">redis&gt; </span><span class="language-bash"> EXISTS mykey</span></span><br><span class="line">(integer) 0</span><br><span class="line"><span class="meta">redis&gt; </span><span class="language-bash"> APPEND mykey <span class="string">&quot;Hello&quot;</span></span></span><br><span class="line">(integer) 5</span><br><span class="line"><span class="meta">redis&gt; </span><span class="language-bash"> APPEND mykey <span class="string">&quot; World&quot;</span></span></span><br><span class="line">(integer) 11</span><br><span class="line"><span class="meta">redis&gt; </span><span class="language-bash"> GET mykey</span> </span><br><span class="line">&quot;Hello World&quot;</span><br></pre></td></tr></table></figure>
<h3 id="6-1-创建测试表">6.1 创建测试表</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">创建一张杂志表 有文章和图片两个列族</span></span><br><span class="line">hbase &gt;  create &#x27;magazine&#x27;,&#x27;article&#x27;,&#x27;picture&#x27;</span><br></pre></td></tr></table></figure>
<h3 id="6-2-协处理器编程">6.2 协处理器编程</h3>
<blockquote>
<p>完整代码可见本仓库：<a href="https://github.com/ihadyou/BigData-Notes/tree/master/code/Hbase%5Chbase-observer-coprocessor">hbase-observer-coprocessor</a></p>
</blockquote>
<p>新建 Maven 工程，导入下面依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>继承 <code>BaseRegionObserver</code> 实现我们自定义的 <code>RegionObserver</code>,对相同的 <code>article:content</code> 执行 put 命令时，将新插入的内容添加到原有内容的末尾，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AppendRegionObserver</span> <span class="keyword">extends</span> <span class="title class_">BaseRegionObserver</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">byte</span>[] columnFamily = Bytes.toBytes(<span class="string">&quot;article&quot;</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="type">byte</span>[] qualifier = Bytes.toBytes(<span class="string">&quot;content&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">prePut</span><span class="params">(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit,</span></span><br><span class="line"><span class="params">                       Durability durability)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="keyword">if</span> (put.has(columnFamily, qualifier)) &#123;</span><br><span class="line">            <span class="comment">// 遍历查询结果，获取指定列的原值</span></span><br><span class="line">            <span class="type">Result</span> <span class="variable">rs</span> <span class="operator">=</span> e.getEnvironment().getRegion().get(<span class="keyword">new</span> <span class="title class_">Get</span>(put.getRow()));</span><br><span class="line">            <span class="type">String</span> <span class="variable">oldValue</span> <span class="operator">=</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">            <span class="keyword">for</span> (Cell cell : rs.rawCells())</span><br><span class="line">                <span class="keyword">if</span> (CellUtil.matchingColumn(cell, columnFamily, qualifier)) &#123;</span><br><span class="line">                    oldValue = Bytes.toString(CellUtil.cloneValue(cell));</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取指定列新插入的值</span></span><br><span class="line">            List&lt;Cell&gt; cells = put.get(columnFamily, qualifier);</span><br><span class="line">            <span class="type">String</span> <span class="variable">newValue</span> <span class="operator">=</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">            <span class="keyword">for</span> (Cell cell : cells) &#123;</span><br><span class="line">                <span class="keyword">if</span> (CellUtil.matchingColumn(cell, columnFamily, qualifier)) &#123;</span><br><span class="line">                    newValue = Bytes.toString(CellUtil.cloneValue(cell));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Append 操作</span></span><br><span class="line">            put.addColumn(columnFamily, qualifier, Bytes.toBytes(oldValue + newValue));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="6-3-打包项目">6.3 打包项目</h3>
<p>使用 maven 命令进行打包，打包后的文件名为 <code>hbase-observer-coprocessor-1.0-SNAPSHOT.jar</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">mvn clean package</span></span><br></pre></td></tr></table></figure>
<h3 id="6-4-上传JAR包到HDFS">6.4 上传JAR包到HDFS</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">上传项目到HDFS上的hbase目录</span></span><br><span class="line">hadoop fs -put /usr/app/hbase-observer-coprocessor-1.0-SNAPSHOT.jar /hbase</span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看上传是否成功</span></span><br><span class="line">hadoop fs -ls /hbase</span><br></pre></td></tr></table></figure>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-cp-hdfs.png"/> </div>
<h3 id="6-5-加载协处理器">6.5 加载协处理器</h3>
<ol>
<li class="lvl-3">
<p>加载协处理器前需要先禁用表</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt;  disable &#x27;magazine&#x27;</span><br></pre></td></tr></table></figure>
<ol start="2">
<li class="lvl-3">
<p>加载协处理器</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt;   alter &#x27;magazine&#x27;, METHOD =&gt; &#x27;table_att&#x27;, &#x27;Coprocessor&#x27;=&gt;&#x27;hdfs://hadoop001:8020/hbase/hbase-observer-coprocessor-1.0-SNAPSHOT.jar|com.ihadyou.AppendRegionObserver|1001|&#x27;</span><br></pre></td></tr></table></figure>
<ol start="3">
<li class="lvl-3">
<p>启用表</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt;  enable &#x27;magazine&#x27;</span><br></pre></td></tr></table></figure>
<ol start="4">
<li class="lvl-3">
<p>查看协处理器是否加载成功</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt;  desc &#x27;magazine&#x27;</span><br></pre></td></tr></table></figure>
<p>协处理器出现在 <code>TABLE_ATTRIBUTES</code> 属性中则代表加载成功，如下图：</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-cp-load.png"/> </div>
<h3 id="6-6-测试加载结果">6.6 测试加载结果</h3>
<p>插入一组测试数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:content&#x27;,&#x27;Hello&#x27;</span><br><span class="line">hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:content&#x27;</span><br><span class="line">hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:content&#x27;,&#x27;World&#x27;</span><br><span class="line">hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:content&#x27;</span><br></pre></td></tr></table></figure>
<p>可以看到对于指定列的值已经执行了 append 操作：</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-cp-helloworld.png"/> </div>
<p>插入一组对照数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:author&#x27;,&#x27;zhangsan&#x27;</span><br><span class="line">hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:author&#x27;</span><br><span class="line">hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:author&#x27;,&#x27;lisi&#x27;</span><br><span class="line">hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:author&#x27;</span><br></pre></td></tr></table></figure>
<p>可以看到对于正常的列还是执行 update 操作:</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-cp-lisi.png"/> </div>
<h3 id="6-7-卸载协处理器">6.7 卸载协处理器</h3>
<ol>
<li class="lvl-3">
<p>卸载协处理器前需要先禁用表</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt;  disable &#x27;magazine&#x27;</span><br></pre></td></tr></table></figure>
<ol start="2">
<li class="lvl-3">
<p>卸载协处理器</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; alter &#x27;magazine&#x27;, METHOD =&gt; &#x27;table_att_unset&#x27;, NAME =&gt; &#x27;coprocessor$1&#x27;</span><br></pre></td></tr></table></figure>
<ol start="3">
<li class="lvl-3">
<p>启用表</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt;  enable &#x27;magazine&#x27;</span><br></pre></td></tr></table></figure>
<ol start="4">
<li class="lvl-3">
<p>查看协处理器是否卸载成功</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt;  desc &#x27;magazine&#x27;</span><br></pre></td></tr></table></figure>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-co-unload.png"/> </div>
<h3 id="6-8-测试卸载结果">6.8 测试卸载结果</h3>
<p>依次执行下面命令可以测试卸载是否成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:content&#x27;</span><br><span class="line">hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:content&#x27;,&#x27;Hello&#x27;</span><br><span class="line">hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:content&#x27;</span><br></pre></td></tr></table></figure>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-unload-test.png"/> </div>
<h2 id="参考资料-11">参考资料</h2>
<ol>
<li class="lvl-3">
<p><a href="http://hbase.apache.org/book.html#cp">Apache HBase Coprocessors</a></p>
</li>
<li class="lvl-3">
<p><a href="https://blogs.apache.org/hbase/entry/coprocessor_introduction">Apache HBase Coprocessor Introduction</a></p>
</li>
<li class="lvl-3">
<p><a href="https://www.itread01.com/content/1546245908.html">HBase 高階知識</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase容灾与备份</title>
    <url>/2021/10/25/Hbase%E5%AE%B9%E7%81%BE%E4%B8%8E%E5%A4%87%E4%BB%BD/</url>
    <content><![CDATA[<h2 id="一、前言">一、前言</h2>
<p>本文主要介绍 Hbase 常用的三种简单的容灾备份方案，即<strong>CopyTable</strong>、<strong>Export</strong>/<strong>Import</strong>、<strong>Snapshot</strong>。分别介绍如下：</p>
<h2 id="二、CopyTable">二、CopyTable</h2>
<h3 id="2-1-简介">2.1 简介</h3>
<p><strong>CopyTable</strong>可以将现有表的数据复制到新表中，具有以下特点：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>支持时间区间 、row 区间 、改变表名称 、改变列族名称 、以及是否 Copy 已被删除的数据等功能；</p>
</li>
<li class="lvl-2">
<p>执行命令前，需先创建与原表结构相同的新表；</p>
</li>
<li class="lvl-2">
<p><code>CopyTable</code> 的操作是基于 HBase Client API 进行的，即采用 <code>scan</code> 进行查询, 采用 <code>put</code> 进行写入。</p>
</li>
</ul>
<h3 id="2-2-命令格式">2.2 命令格式</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Usage: CopyTable [general options] [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] &lt;tablename&gt;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-常用命令">2.3 常用命令</h3>
<ol>
<li class="lvl-3">
<p>同集群下 CopyTable</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name=tableCopy  tableOrig</span><br></pre></td></tr></table></figure>
<ol start="2">
<li class="lvl-3">
<p>不同集群下 CopyTable</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">两表名称相同的情况</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.CopyTable \</span><br><span class="line">--peer.adr=dstClusterZK:2181:/hbase tableOrig</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">也可以指新的表名</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.CopyTable \</span><br><span class="line">--peer.adr=dstClusterZK:2181:/hbase \</span><br><span class="line">--new.name=tableCopy tableOrig</span><br></pre></td></tr></table></figure>
<ol start="3">
<li class="lvl-3">
<p>下面是一个官方给的比较完整的例子，指定开始和结束时间，集群地址，以及只复制指定的列族：</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.CopyTable \</span><br><span class="line">--starttime=1265875194289 \</span><br><span class="line">--endtime=1265878794289 \</span><br><span class="line">--peer.adr=server1,server2,server3:2181:/hbase \</span><br><span class="line">--families=myOldCf:myNewCf,cf2,cf3 TestTable</span><br></pre></td></tr></table></figure>
<h3 id="2-4-更多参数">2.4 更多参数</h3>
<p>可以通过 <code>--help</code> 查看更多支持的参数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">hbase org.apache.hadoop.hbase.mapreduce.CopyTable --<span class="built_in">help</span></span></span><br></pre></td></tr></table></figure>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-copy-table.png"/> </div>
<h2 id="三、Export-Import">三、Export/Import</h2>
<h3 id="3-1-简介">3.1 简介</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><code>Export</code> 支持导出数据到 HDFS, <code>Import</code> 支持从 HDFS 导入数据。<code>Export</code> 还支持指定导出数据的开始时间和结束时间，因此可以用于增量备份。</p>
</li>
<li class="lvl-2">
<p><code>Export</code> 导出与 <code>CopyTable</code> 一样，依赖 HBase 的 <code>scan</code> 操作</p>
</li>
</ul>
<h3 id="3-2-命令格式">3.2 命令格式</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">Export</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">Inport</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;</span><br></pre></td></tr></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>导出的 <code>outputdir</code> 目录可以不用预先创建，程序会自动创建。导出完成后，导出文件的所有权将由执行导出命令的用户所拥有。</p>
</li>
<li class="lvl-2">
<p>默认情况下，仅导出给定 <code>Cell</code> 的最新版本，而不管历史版本。要导出多个版本，需要将 <code>&lt;versions&gt;</code> 参数替换为所需的版本数。</p>
</li>
</ul>
<h3 id="3-3-常用命令">3.3 常用命令</h3>
<ol>
<li class="lvl-3">
<p>导出命令</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Export tableName  hdfs 路径/tableName.db</span><br></pre></td></tr></table></figure>
<ol start="2">
<li class="lvl-3">
<p>导入命令</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Import tableName  hdfs 路径/tableName.db</span><br></pre></td></tr></table></figure>
<h2 id="四、Snapshot">四、Snapshot</h2>
<h3 id="4-1-简介">4.1 简介</h3>
<p>HBase 的快照 (Snapshot) 功能允许您获取表的副本 (包括内容和元数据)，并且性能开销很小。因为快照存储的仅仅是表的元数据和 HFiles 的信息。快照的 <code>clone</code> 操作会从该快照创建新表，快照的 <code>restore</code> 操作会将表的内容还原到快照节点。<code>clone</code> 和 <code>restore</code> 操作不需要复制任何数据，因为底层 HFiles(包含 HBase 表数据的文件) 不会被修改，修改的只是表的元数据信息。</p>
<h3 id="4-2-配置">4.2 配置</h3>
<p>HBase 快照功能默认没有开启，如果要开启快照，需要在 <code>hbase-site.xml</code> 文件中添加如下配置项：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.snapshot.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-3-常用命令">4.3 常用命令</h3>
<p>快照的所有命令都需要在 Hbase Shell 交互式命令行中执行。</p>
<h4 id="1-Take-a-Snapshot">1. Take a Snapshot</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">拍摄快照</span></span><br><span class="line"><span class="meta">hbase&gt; </span><span class="language-bash">snapshot <span class="string">&#x27;表名&#x27;</span>, <span class="string">&#x27;快照名&#x27;</span></span></span><br></pre></td></tr></table></figure>
<p>默认情况下拍摄快照之前会在内存中执行数据刷新。以保证内存中的数据包含在快照中。但是如果你不希望包含内存中的数据，则可以使用 <code>SKIP_FLUSH</code> 选项禁止刷新。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">禁止内存刷新</span></span><br><span class="line"><span class="meta">hbase&gt; </span><span class="language-bash">snapshot  <span class="string">&#x27;表名&#x27;</span>, <span class="string">&#x27;快照名&#x27;</span>, &#123;SKIP_FLUSH =&gt; <span class="literal">true</span>&#125;</span></span><br></pre></td></tr></table></figure>
<h4 id="2-Listing-Snapshots">2. Listing Snapshots</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">获取快照列表</span></span><br><span class="line"><span class="meta">hbase&gt; </span><span class="language-bash">list_snapshots</span></span><br></pre></td></tr></table></figure>
<h4 id="3-Deleting-Snapshots">3. Deleting Snapshots</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">删除快照</span></span><br><span class="line"><span class="meta">hbase&gt; </span><span class="language-bash">delete_snapshot <span class="string">&#x27;快照名&#x27;</span></span></span><br></pre></td></tr></table></figure>
<h4 id="4-Clone-a-table-from-snapshot">4. Clone a table from snapshot</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">从现有的快照创建一张新表</span></span><br><span class="line"><span class="meta">hbase&gt; </span><span class="language-bash"> clone_snapshot <span class="string">&#x27;快照名&#x27;</span>, <span class="string">&#x27;新表名&#x27;</span></span></span><br></pre></td></tr></table></figure>
<h4 id="5-Restore-a-snapshot">5. Restore a snapshot</h4>
<p>将表恢复到快照节点，恢复操作需要先禁用表</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hbase&gt; </span><span class="language-bash"><span class="built_in">disable</span> <span class="string">&#x27;表名&#x27;</span></span></span><br><span class="line"><span class="meta">hbase&gt; </span><span class="language-bash">restore_snapshot <span class="string">&#x27;快照名&#x27;</span></span></span><br></pre></td></tr></table></figure>
<p>这里需要注意的是：是如果 HBase 配置了基于 Replication 的主从复制，由于 Replication 在日志级别工作，而快照在文件系统级别工作，因此在还原之后，会出现副本与主服务器处于不同的状态的情况。这时候可以先停止同步，所有服务器还原到一致的数据点后再重新建立同步。</p>
<h2 id="参考资料-12">参考资料</h2>
<ol>
<li class="lvl-3">
<p><a href="https://blog.cloudera.com/blog/2012/06/online-hbase-backups-with-copytable-2/">Online Apache HBase Backups with CopyTable</a></p>
</li>
<li class="lvl-3">
<p><a href="http://hbase.apache.org/book.htm">Apache HBase ™ Reference Guide</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase架构及数据结构</title>
    <url>/2021/10/25/Hbase%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<h2 id="一、基本概念">一、基本概念</h2>
<p>一个典型的 Hbase Table 表如下：</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-webtable.png"/> </div>
<h3 id="1-1-Row-Key-行键">1.1 Row Key (行键)</h3>
<p><code>Row Key</code> 是用来检索记录的主键。想要访问 HBase Table 中的数据，只有以下三种方式：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>通过指定的 <code>Row Key</code> 进行访问；</p>
</li>
<li class="lvl-2">
<p>通过 Row Key 的 range 进行访问，即访问指定范围内的行；</p>
</li>
<li class="lvl-2">
<p>进行全表扫描。</p>
</li>
</ul>
<p><code>Row Key</code> 可以是任意字符串，存储时数据按照 <code>Row Key</code> 的字典序进行排序。这里需要注意以下两点：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>因为字典序对 Int 排序的结果是 1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99。如果你使用整型的字符串作为行键，那么为了保持整型的自然序，行键必须用 0 作左填充。</p>
</li>
<li class="lvl-2">
<p>行的一次读写操作时原子性的 (不论一次读写多少列)。</p>
</li>
</ul>
<h3 id="1-2-Column-Family（列族）">1.2 Column Family（列族）</h3>
<p>HBase 表中的每个列，都归属于某个列族。列族是表的 Schema 的一部分，所以列族需要在创建表时进行定义。列族的所有列都以列族名作为前缀，例如 <code>courses:history</code>，<code>courses:math</code> 都属于 <code>courses</code> 这个列族。</p>
<h3 id="1-3-Column-Qualifier-列限定符">1.3 Column Qualifier (列限定符)</h3>
<p>列限定符，你可以理解为是具体的列名，例如 <code>courses:history</code>，<code>courses:math</code> 都属于 <code>courses</code> 这个列族，它们的列限定符分别是 <code>history</code> 和 <code>math</code>。需要注意的是列限定符不是表 Schema 的一部分，你可以在插入数据的过程中动态创建列。</p>
<h3 id="1-4-Column-列">1.4 Column(列)</h3>
<p>HBase 中的列由列族和列限定符组成，它们由 <code>:</code>(冒号) 进行分隔，即一个完整的列名应该表述为 <code>列族名 ：列限定符</code>。</p>
<h3 id="1-5-Cell">1.5 Cell</h3>
<p><code>Cell</code> 是行，列族和列限定符的组合，并包含值和时间戳。你可以等价理解为关系型数据库中由指定行和指定列确定的一个单元格，但不同的是 HBase 中的一个单元格是由多个版本的数据组成的，每个版本的数据用时间戳进行区分。</p>
<h3 id="1-6-Timestamp-时间戳">1.6 Timestamp(时间戳)</h3>
<p>HBase 中通过 <code>row key</code> 和 <code>column</code> 确定的为一个存储单元称为 <code>Cell</code>。每个 <code>Cell</code> 都保存着同一份数据的多个版本。版本通过时间戳来索引，时间戳的类型是 64 位整型，时间戳可以由 HBase 在数据写入时自动赋值，也可以由客户显式指定。每个 <code>Cell</code> 中，不同版本的数据按照时间戳倒序排列，即最新的数据排在最前面。</p>
<h2 id="二、存储结构">二、存储结构</h2>
<h3 id="2-1-Regions">2.1 Regions</h3>
<p>HBase Table 中的所有行按照 <code>Row Key</code> 的字典序排列。HBase Tables 通过行键的范围 (row key range) 被水平切分成多个 <code>Region</code>, 一个 <code>Region</code> 包含了在 start key 和 end key 之间的所有行。</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBaseArchitecture-Blog-Fig2.png"/> </div>
<p>每个表一开始只有一个 <code>Region</code>，随着数据不断增加，<code>Region</code> 会不断增大，当增大到一个阀值的时候，<code>Region</code> 就会等分为两个新的 <code>Region</code>。当 Table 中的行不断增多，就会有越来越多的 <code>Region</code>。</p>
<div align="center"> <img width="600px" src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-region-splite.png"/> </div>
<p><code>Region</code> 是 HBase 中<strong>分布式存储和负载均衡的最小单元</strong>。这意味着不同的 <code>Region</code> 可以分布在不同的 <code>Region Server</code> 上。但一个 <code>Region</code> 是不会拆分到多个 Server 上的。</p>
<div align="center"> <img width="600px" src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-region-dis.png"/> </div>
<h3 id="2-2-Region-Server">2.2 Region Server</h3>
<p><code>Region Server</code> 运行在 HDFS 的 DataNode 上。它具有以下组件：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>WAL(Write Ahead Log，预写日志)</strong>：用于存储尚未进持久化存储的数据记录，以便在发生故障时进行恢复。</p>
</li>
<li class="lvl-2">
<p><strong>BlockCache</strong>：读缓存。它将频繁读取的数据存储在内存中，如果存储不足，它将按照 <code>最近最少使用原则</code> 清除多余的数据。</p>
</li>
<li class="lvl-2">
<p><strong>MemStore</strong>：写缓存。它存储尚未写入磁盘的新数据，并会在数据写入磁盘之前对其进行排序。每个 Region 上的每个列族都有一个 MemStore。</p>
</li>
<li class="lvl-2">
<p><strong>HFile</strong> ：将行数据按照 Key\Values 的形式存储在文件系统上。</p>
</li>
</ul>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-Region-Server.png"/> </div>
<p>Region Server 存取一个子表时，会创建一个 Region 对象，然后对表的每个列族创建一个 <code>Store</code> 实例，每个 <code>Store</code> 会有 0 个或多个 <code>StoreFile</code> 与之对应，每个 <code>StoreFile</code> 则对应一个 <code>HFile</code>，HFile 就是实际存储在 HDFS 上的文件。</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-hadoop.png"/> </div>
<h2 id="三、Hbase系统架构">三、Hbase系统架构</h2>
<h3 id="3-1-系统架构">3.1 系统架构</h3>
<p>HBase 系统遵循 Master/Salve 架构，由三种不同类型的组件组成：</p>
<p><strong>Zookeeper</strong></p>
<ol>
<li class="lvl-3">
<p>保证任何时候，集群中只有一个 Master；</p>
</li>
<li class="lvl-3">
<p>存贮所有 Region 的寻址入口；</p>
</li>
<li class="lvl-3">
<p>实时监控 Region Server 的状态，将 Region Server 的上线和下线信息实时通知给 Master；</p>
</li>
<li class="lvl-3">
<p>存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 Column Family 等信息。</p>
</li>
</ol>
<p><strong>Master</strong></p>
<ol>
<li class="lvl-3">
<p>为 Region Server 分配 Region ；</p>
</li>
<li class="lvl-3">
<p>负责 Region Server 的负载均衡 ；</p>
</li>
<li class="lvl-3">
<p>发现失效的 Region Server 并重新分配其上的 Region；</p>
</li>
<li class="lvl-3">
<p>GFS 上的垃圾文件回收；</p>
</li>
<li class="lvl-3">
<p>处理 Schema 的更新请求。</p>
</li>
</ol>
<p><strong>Region Server</strong></p>
<ol>
<li class="lvl-3">
<p>Region Server 负责维护 Master 分配给它的 Region ，并处理发送到 Region 上的 IO 请求；</p>
</li>
<li class="lvl-3">
<p>Region Server 负责切分在运行过程中变得过大的 Region。</p>
</li>
</ol>
<div align="center"> <img width="600px" src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBaseArchitecture-Blog-Fig1.png"/> </div>
<h3 id="3-2-组件间的协作">3.2 组件间的协作</h3>
<p>HBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务器状态。 Zookeeper 负责维护可用服务列表，并提供服务故障通知等服务：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>每个 Region Server 都会在 ZooKeeper 上创建一个临时节点，Master 通过 Zookeeper 的 Watcher 机制对节点进行监控，从而可以发现新加入的 Region Server 或故障退出的 Region Server；</p>
</li>
<li class="lvl-2">
<p>所有 Masters 会竞争性地在 Zookeeper 上创建同一个临时节点，由于 Zookeeper 只能有一个同名节点，所以必然只有一个 Master 能够创建成功，此时该 Master 就是主 Master，主 Master 会定期向 Zookeeper 发送心跳。备用 Masters 则通过 Watcher 机制对主 HMaster 所在节点进行监听；</p>
</li>
<li class="lvl-2">
<p>如果主 Master 未能定时发送心跳，则其持有的 Zookeeper 会话会过期，相应的临时节点也会被删除，这会触发定义在该节点上的 Watcher 事件，使得备用的 Master Servers 得到通知。所有备用的 Master Servers 在接到通知后，会再次去竞争性地创建临时节点，完成主 Master 的选举。</p>
</li>
</ul>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBaseArchitecture-Blog-Fig5.png"/> </div>
<h2 id="四、数据的读写流程简述">四、数据的读写流程简述</h2>
<h3 id="4-1-写入数据的流程">4.1 写入数据的流程</h3>
<ol>
<li class="lvl-3">
<p>Client 向 Region Server 提交写请求；</p>
</li>
<li class="lvl-3">
<p>Region Server 找到目标 Region；</p>
</li>
<li class="lvl-3">
<p>Region 检查数据是否与 Schema 一致；</p>
</li>
<li class="lvl-3">
<p>如果客户端没有指定版本，则获取当前系统时间作为数据版本；</p>
</li>
<li class="lvl-3">
<p>将更新写入 WAL Log；</p>
</li>
<li class="lvl-3">
<p>将更新写入 Memstore；</p>
</li>
<li class="lvl-3">
<p>判断 Memstore 存储是否已满，如果存储已满则需要 flush 为 Store Hfile 文件。</p>
</li>
</ol>
<blockquote>
<p>更为详细写入流程可以参考：<a href="http://hbasefly.com/2016/03/23/hbase_writer/">HBase － 数据写入流程解析</a></p>
</blockquote>
<h3 id="4-2-读取数据的流程">4.2 读取数据的流程</h3>
<p>以下是客户端首次读写 HBase 上数据的流程：</p>
<ol>
<li class="lvl-3">
<p>客户端从 Zookeeper 获取 <code>META</code> 表所在的 Region Server；</p>
</li>
<li class="lvl-3">
<p>客户端访问 <code>META</code> 表所在的 Region Server，从 <code>META</code> 表中查询到访问行键所在的 Region Server，之后客户端将缓存这些信息以及 <code>META</code> 表的位置；</p>
</li>
<li class="lvl-3">
<p>客户端从行键所在的 Region Server 上获取数据。</p>
</li>
</ol>
<p>如果再次读取，客户端将从缓存中获取行键所在的 Region Server。这样客户端就不需要再次查询 <code>META</code> 表，除非 Region 移动导致缓存失效，这样的话，则将会重新查询并更新缓存。</p>
<p>注：<code>META</code> 表是 HBase 中一张特殊的表，它保存了所有 Region 的位置信息，META 表自己的位置信息则存储在 ZooKeeper 上。</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBaseArchitecture-Blog-Fig7.png"/> </div>
<blockquote>
<p>更为详细读取数据流程参考：</p>
<p><a href="http://hbasefly.com/2016/12/21/hbase-getorscan/">HBase 原理－数据读取流程解析</a></p>
<p><a href="http://hbasefly.com/2017/06/11/hbase-scan-2/">HBase 原理－迟到的‘数据读取流程部分细节</a></p>
</blockquote>
<h2 id="参考资料-13">参考资料</h2>
<p>本篇文章内容主要参考自官方文档和以下两篇博客，图片也主要引用自以下两篇博客：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a href="https://mapr.com/blog/in-depth-look-hbase-architecture/#.VdMxvWSqqko">HBase Architectural Components</a></p>
</li>
<li class="lvl-2">
<p><a href="https://www.open-open.com/lib/view/open1346821084631.html">Hbase 系统架构及数据结构</a></p>
</li>
</ul>
<p>官方文档：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a href="https://hbase.apache.org/2.1/book.html">Apache HBase ™ Reference Guide</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase常用shell命令</title>
    <url>/2021/10/25/Hbase%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h2 id="一、基本命令">一、基本命令</h2>
<p>打开 Hbase Shell：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">hbase shell</span></span><br></pre></td></tr></table></figure>
<h4 id="1-1-获取帮助">1.1 获取帮助</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">获取帮助</span></span><br><span class="line">help</span><br><span class="line"><span class="meta"># </span><span class="language-bash">获取命令的详细信息</span></span><br><span class="line">help &#x27;status&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="1-2-查看服务器状态">1.2 查看服务器状态</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">status</span><br></pre></td></tr></table></figure>
<h4 id="1-3-查看版本信息">1.3 查看版本信息</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">version</span><br></pre></td></tr></table></figure>
<h2 id="二、关于表的操作">二、关于表的操作</h2>
<h4 id="2-1-查看所有表">2.1 查看所有表</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">list</span><br></pre></td></tr></table></figure>
<h4 id="2-2-创建表">2.2 创建表</h4>
<p><strong>命令格式</strong>： create ‘表名称’, ‘列族名称 1’,‘列族名称 2’,‘列名称 N’</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">创建一张名为Student的表,包含基本信息（baseInfo）、学校信息（schoolInfo）两个列族</span></span><br><span class="line">create &#x27;Student&#x27;,&#x27;baseInfo&#x27;,&#x27;schoolInfo&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="2-3-查看表的基本信息">2.3 查看表的基本信息</h4>
<p><strong>命令格式</strong>：desc ‘表名’</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">describe &#x27;Student&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="2-4-表的启用-禁用">2.4 表的启用/禁用</h4>
<p>enable 和 disable 可以启用/禁用这个表,is_enabled 和 is_disabled 来检查表是否被禁用</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">禁用表</span></span><br><span class="line">disable &#x27;Student&#x27;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">检查表是否被禁用</span></span><br><span class="line">is_disabled &#x27;Student&#x27;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">启用表</span></span><br><span class="line">enable &#x27;Student&#x27;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">检查表是否被启用</span></span><br><span class="line">is_enabled &#x27;Student&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="2-5-检查表是否存在">2.5 检查表是否存在</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">exists &#x27;Student&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="2-6-删除表">2.6 删除表</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">删除表前需要先禁用表</span></span><br><span class="line">disable &#x27;Student&#x27;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">删除表</span></span><br><span class="line">drop &#x27;Student&#x27;</span><br></pre></td></tr></table></figure>
<h2 id="三、增删改">三、增删改</h2>
<h4 id="3-1-添加列族">3.1 添加列族</h4>
<p><strong>命令格式</strong>： alter ‘表名’, ‘列族名’</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alter &#x27;Student&#x27;, &#x27;teacherInfo&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="3-2-删除列族">3.2 删除列族</h4>
<p><strong>命令格式</strong>：alter ‘表名’, {NAME =&gt; ‘列族名’, METHOD =&gt; ‘delete’}</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alter &#x27;Student&#x27;, &#123;NAME =&gt; &#x27;teacherInfo&#x27;, METHOD =&gt; &#x27;delete&#x27;&#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-3-更改列族存储版本的限制">3.3 更改列族存储版本的限制</h4>
<p>默认情况下，列族只存储一个版本的数据，如果需要存储多个版本的数据，则需要修改列族的属性。修改后可通过 <code>desc</code> 命令查看。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alter &#x27;Student&#x27;,&#123;NAME=&gt;&#x27;baseInfo&#x27;,VERSIONS=&gt;3&#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-4-插入数据">3.4 插入数据</h4>
<p><strong>命令格式</strong>：put ‘表名’, ‘行键’,‘列族:列’,‘值’</p>
<p><strong>注意：如果新增数据的行键值、列族名、列名与原有数据完全相同，则相当于更新操作</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;baseInfo:name&#x27;,&#x27;tom&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;baseInfo:birthday&#x27;,&#x27;1990-01-09&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;baseInfo:age&#x27;,&#x27;29&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;schoolInfo:name&#x27;,&#x27;Havard&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;schoolInfo:localtion&#x27;,&#x27;Boston&#x27;</span><br><span class="line"></span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;baseInfo:name&#x27;,&#x27;jack&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;baseInfo:birthday&#x27;,&#x27;1998-08-22&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;baseInfo:age&#x27;,&#x27;21&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;schoolInfo:name&#x27;,&#x27;yale&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;schoolInfo:localtion&#x27;,&#x27;New Haven&#x27;</span><br><span class="line"></span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;baseInfo:name&#x27;,&#x27;maike&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;baseInfo:birthday&#x27;,&#x27;1995-01-22&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;baseInfo:age&#x27;,&#x27;24&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;schoolInfo:name&#x27;,&#x27;yale&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;schoolInfo:localtion&#x27;,&#x27;New Haven&#x27;</span><br><span class="line"></span><br><span class="line">put &#x27;Student&#x27;, &#x27;wrowkey4&#x27;,&#x27;baseInfo:name&#x27;,&#x27;maike-jack&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="3-5-获取指定行、指定行中的列族、列的信息">3.5 获取指定行、指定行中的列族、列的信息</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">获取指定行中所有列的数据信息</span></span><br><span class="line">get &#x27;Student&#x27;,&#x27;rowkey3&#x27;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">获取指定行中指定列族下所有列的数据信息</span></span><br><span class="line">get &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo&#x27;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">获取指定行中指定列的数据信息</span></span><br><span class="line">get &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo:name&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="3-6-删除指定行、指定行中的列">3.6 删除指定行、指定行中的列</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">删除指定行</span></span><br><span class="line">delete &#x27;Student&#x27;,&#x27;rowkey3&#x27;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">删除指定行中指定列的数据</span></span><br><span class="line">delete &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo:name&#x27;</span><br></pre></td></tr></table></figure>
<h2 id="四、查询">四、查询</h2>
<p>hbase 中访问数据有两种基本的方式：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>按指定 rowkey 获取数据：get 方法；</p>
</li>
<li class="lvl-2">
<p>按指定条件获取数据：scan 方法。</p>
</li>
</ul>
<p><code>scan</code> 可以设置 begin 和 end 参数来访问一个范围内所有的数据。get 本质上就是 begin 和 end 相等的一种特殊的 scan。</p>
<h4 id="4-1Get查询">4.1Get查询</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">获取指定行中所有列的数据信息</span></span><br><span class="line">get &#x27;Student&#x27;,&#x27;rowkey3&#x27;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">获取指定行中指定列族下所有列的数据信息</span></span><br><span class="line">get &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo&#x27;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">获取指定行中指定列的数据信息</span></span><br><span class="line">get &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo:name&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="4-2-查询整表数据">4.2 查询整表数据</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan &#x27;Student&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="4-3-查询指定列簇的数据">4.3 查询指定列簇的数据</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan &#x27;Student&#x27;, &#123;COLUMN=&gt;&#x27;baseInfo&#x27;&#125;</span><br></pre></td></tr></table></figure>
<h4 id="4-4-条件查询">4.4  条件查询</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">查询指定列的数据</span></span><br><span class="line">scan &#x27;Student&#x27;, &#123;COLUMNS=&gt; &#x27;baseInfo:birthday&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p>除了列 <code>（COLUMNS）</code> 修饰词外，HBase 还支持 <code>Limit</code>（限制查询结果行数），<code>STARTROW</code>（<code>ROWKEY</code> 起始行，会先根据这个 <code>key</code> 定位到 <code>region</code>，再向后扫描）、<code>STOPROW</code>(结束行)、<code>TIMERANGE</code>（限定时间戳范围）、<code>VERSIONS</code>（版本数）、和 <code>FILTER</code>（按条件过滤行）等。</p>
<p>如下代表从 <code>rowkey2</code> 这个 <code>rowkey</code> 开始，查找下两个行的最新 3 个版本的 name 列的数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan &#x27;Student&#x27;, &#123;COLUMNS=&gt; &#x27;baseInfo:name&#x27;,STARTROW =&gt; &#x27;rowkey2&#x27;,STOPROW =&gt; &#x27;wrowkey4&#x27;,LIMIT=&gt;2, VERSIONS=&gt;3&#125;</span><br></pre></td></tr></table></figure>
<h4 id="4-5-条件过滤">4.5  条件过滤</h4>
<p>Filter 可以设定一系列条件来进行过滤。如我们要查询值等于 24 的所有数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan &#x27;Student&#x27;, FILTER=&gt;&quot;ValueFilter(=,&#x27;binary:24&#x27;)&quot;</span><br></pre></td></tr></table></figure>
<p>值包含 yale 的所有数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan &#x27;Student&#x27;, FILTER=&gt;&quot;ValueFilter(=,&#x27;substring:yale&#x27;)&quot;</span><br></pre></td></tr></table></figure>
<p>列名中的前缀为 birth 的：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan &#x27;Student&#x27;, FILTER=&gt;&quot;ColumnPrefixFilter(&#x27;birth&#x27;)&quot;</span><br></pre></td></tr></table></figure>
<p>FILTER 中支持多个过滤条件通过括号、AND 和 OR 进行组合：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">列名中的前缀为birth且列值中包含1998的数据</span></span><br><span class="line">scan &#x27;Student&#x27;, FILTER=&gt;&quot;ColumnPrefixFilter(&#x27;birth&#x27;) AND ValueFilter ValueFilter(=,&#x27;substring:1998&#x27;)&quot;</span><br></pre></td></tr></table></figure>
<p><code>PrefixFilter</code> 用于对 Rowkey 的前缀进行判断：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan &#x27;Student&#x27;, FILTER=&gt;&quot;PrefixFilter(&#x27;wr&#x27;)&quot;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase环境搭建</title>
    <url>/2021/10/25/Hbase%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h2 id="一、安装前置条件说明">一、安装前置条件说明</h2>
<h3 id="1-1-JDK版本说明">1.1 JDK版本说明</h3>
<p>HBase 需要依赖 JDK 环境，同时 HBase 2.0+ 以上版本不再支持 JDK 1.7 ，需要安装 JDK 1.8+ 。JDK 安装方式见本仓库：</p>
<blockquote>
<p><a href="https://github.com/ihadyou/BigData-Notes/blob/master/notes/installation/Linux%E4%B8%8BJDK%E5%AE%89%E8%A3%85.md">Linux 环境下 JDK 安装</a></p>
</blockquote>
<h3 id="1-2-Standalone模式和伪集群模式的区别">1.2 Standalone模式和伪集群模式的区别</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>在 <code>Standalone</code> 模式下，所有守护进程都运行在一个 <code>jvm</code> 进程/实例中；</p>
</li>
<li class="lvl-2">
<p>在伪分布模式下，HBase 仍然在单个主机上运行，但是每个守护进程 (HMaster，HRegionServer 和 ZooKeeper) 则分别作为一个单独的进程运行。</p>
</li>
</ul>
<p><strong>说明：两种模式任选其一进行部署即可，对于开发测试来说区别不大。</strong></p>
<h2 id="二、Standalone-模式">二、Standalone 模式</h2>
<h3 id="2-1-下载并解压">2.1 下载并解压</h3>
<p>从<a href="https://hbase.apache.org/downloads.html">官方网站</a> 下载所需要版本的二进制安装包，并进行解压：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">tar -zxvf hbase-2.1.4-bin.tar.gz</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-配置环境变量">2.2 配置环境变量</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">vim /etc/profile</span></span><br></pre></td></tr></table></figure>
<p>添加环境变量：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HBASE_HOME=/usr/app/hbase-2.1.4</span><br><span class="line">export PATH=$HBASE_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>
<p>使得配置的环境变量生效：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-进行HBase相关配置">2.3 进行HBase相关配置</h3>
<p>修改安装目录下的 <code>conf/hbase-env.sh</code>,指定 JDK 的安装路径：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">The java implementation to use.  Java 1.8+ required.</span></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_201</span><br></pre></td></tr></table></figure>
<p>修改安装目录下的 <code>conf/hbase-site.xml</code>，增加如下配置：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///home/hbase/rootdir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/zookeeper/dataDir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.unsafe.stream.capability.enforce<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><code>hbase.rootdir</code>: 配置 hbase 数据的存储路径；</p>
<p><code>hbase.zookeeper.property.dataDir</code>: 配置 zookeeper 数据的存储路径；</p>
<p><code>hbase.unsafe.stream.capability.enforce</code>: 使用本地文件系统存储，不使用 HDFS 的情况下需要禁用此配置，设置为 false。</p>
<h3 id="2-4-启动HBase">2.4 启动HBase</h3>
<p>由于已经将 HBase 的 bin 目录配置到环境变量，直接使用以下命令启动：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">start-hbase.sh</span></span><br></pre></td></tr></table></figure>
<h3 id="2-5-验证启动是否成功">2.5 验证启动是否成功</h3>
<p>验证方式一 ：使用 <code>jps</code> 命令查看 HMaster 进程是否启动。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 hbase-2.1.4]# jps</span><br><span class="line">16336 Jps</span><br><span class="line">15500 HMaster</span><br></pre></td></tr></table></figure>
<p>验证方式二 ：访问 HBaseWeb UI 页面，默认端口为 <code>16010</code> 。</p>
<div align="center"> <img src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-web-ui.png"/> </div>
<h2 id="三、伪集群模式安装（Pseudo-Distributed）">三、伪集群模式安装（Pseudo-Distributed）</h2>
<h3 id="3-1-Hadoop单机伪集群安装">3.1 Hadoop单机伪集群安装</h3>
<p>这里我们采用 HDFS 作为 HBase 的存储方案，需要预先安装 Hadoop。Hadoop 的安装方式单独整理至：</p>
<blockquote>
<p><a href="https://github.com/ihadyou/BigData-Notes/blob/master/notes/installation/Hadoop%E5%8D%95%E6%9C%BA%E7%89%88%E6%9C%AC%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.md">Hadoop 单机伪集群搭建</a></p>
</blockquote>
<h3 id="3-2-Hbase版本选择">3.2 Hbase版本选择</h3>
<p>HBase 的版本必须要与 Hadoop 的版本兼容，不然会出现各种 Jar 包冲突。这里我 Hadoop 安装的版本为 <code>hadoop-2.6.0-cdh5.15.2</code>，为保持版本一致，选择的 HBase 版本为 <code>hbase-1.2.0-cdh5.15.2</code> 。所有软件版本如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>Hadoop 版本： hadoop-2.6.0-cdh5.15.2</p>
</li>
<li class="lvl-2">
<p>HBase 版本： hbase-1.2.0-cdh5.15.2</p>
</li>
<li class="lvl-2">
<p>JDK 版本：JDK 1.8</p>
</li>
</ul>
<h3 id="3-3-软件下载解压">3.3 软件下载解压</h3>
<p>下载后进行解压，下载地址：<a href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">tar -zxvf hbase-1.2.0-cdh5.15.2.tar.gz</span></span><br></pre></td></tr></table></figure>
<h3 id="3-4-配置环境变量">3.4 配置环境变量</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">vim /etc/profile</span></span><br></pre></td></tr></table></figure>
<p>添加环境变量：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HBASE_HOME=/usr/app/hbase-1.2.0-cdh5.15.2</span><br><span class="line">export PATH=$HBASE_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>
<p>使得配置的环境变量生效：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure>
<h3 id="3-5-进行HBase相关配置">3.5 进行HBase相关配置</h3>
<p>1.修改安装目录下的 <code>conf/hbase-env.sh</code>,指定 JDK 的安装路径：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">The java implementation to use.  Java 1.7+ required.</span></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_201</span><br></pre></td></tr></table></figure>
<p>2.修改安装目录下的 <code>conf/hbase-site.xml</code>，增加如下配置 (hadoop001 为主机名)：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!--指定 HBase 以分布式模式运行--&gt;</span>   </span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!--指定 HBase 数据存储路径为 HDFS 上的 hbase 目录,hbase 目录不需要预先创建，程序会自动创建--&gt;</span>   </span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop001:8020/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定 zookeeper 数据的存储位置--&gt;</span>   </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/zookeeper/dataDir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>3.修改安装目录下的 <code>conf/regionservers</code>，指定 region  servers 的地址，修改后其内容如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop001</span><br></pre></td></tr></table></figure>
<h3 id="3-6-启动">3.6 启动</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">bin/start-hbase.sh</span></span><br></pre></td></tr></table></figure>
<h3 id="3-7-验证启动是否成功">3.7 验证启动是否成功</h3>
<p>验证方式一 ：使用 <code>jps</code> 命令查看进程。其中 <code>HMaster</code>，<code>HRegionServer</code> 是 HBase 的进程，<code>HQuorumPeer</code> 是 HBase 内置的 Zookeeper 的进程，其余的为 HDFS 和 YARN 的进程。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 conf]# jps</span><br><span class="line">28688 NodeManager</span><br><span class="line">25824 GradleDaemon</span><br><span class="line">10177 Jps</span><br><span class="line">22083 HRegionServer</span><br><span class="line">20534 DataNode</span><br><span class="line">20807 SecondaryNameNode</span><br><span class="line">18744 Main</span><br><span class="line">20411 NameNode</span><br><span class="line">21851 HQuorumPeer</span><br><span class="line">28573 ResourceManager</span><br><span class="line">21933 HMaster</span><br></pre></td></tr></table></figure>
<p>验证方式二 ：访问 HBase Web UI 界面，需要注意的是 1.2 版本的 HBase 的访问端口为 <code>60010</code></p>
<div align="center"> <img src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-60010.png"/> </div>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase简介</title>
    <url>/2021/10/25/Hbase%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h2 id="一、Hadoop的局限">一、Hadoop的局限</h2>
<p>HBase 是一个构建在 Hadoop 文件系统之上的面向列的数据库管理系统。</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase.jpg"/> </div>
<p>要想明白为什么产生 HBase，就需要先了解一下 Hadoop 存在的限制？Hadoop 可以通过 HDFS 来存储结构化、半结构甚至非结构化的数据，它是传统数据库的补充，是海量数据存储的最佳方法，它针对大文件的存储，批量访问和流式访问都做了优化，同时也通过多副本解决了容灾问题。</p>
<p>但是 Hadoop 的缺陷在于它只能执行批处理，并且只能以顺序方式访问数据，这意味着即使是最简单的工作，也必须搜索整个数据集，无法实现对数据的随机访问。实现数据的随机访问是传统的关系型数据库所擅长的，但它们却不能用于海量数据的存储。在这种情况下，必须有一种新的方案来解决海量数据存储和随机访问的问题，HBase 就是其中之一 (HBase，Cassandra，couchDB，Dynamo 和 MongoDB 都能存储海量数据并支持随机访问)。</p>
<blockquote>
<p>注：数据结构分类：</p>
<ul class="lvl-1">
<li class="lvl-2">结构化数据：即以关系型数据库表形式管理的数据；</li>
<li class="lvl-2">半结构化数据：非关系模型的，有基本固定结构模式的数据，例如日志文件、XML 文档、JSON 文档、Email 等；</li>
<li class="lvl-2">非结构化数据：没有固定模式的数据，如 WORD、PDF、PPT、EXL，各种格式的图片、视频等。</li>
</ul>
</blockquote>
<h2 id="二、HBase简介">二、HBase简介</h2>
<p>HBase 是一个构建在 Hadoop 文件系统之上的面向列的数据库管理系统。</p>
<p>HBase 是一种类似于 <code>Google’s Big Table</code> 的数据模型，它是 Hadoop 生态系统的一部分，它将数据存储在 HDFS 上，客户端可以通过 HBase 实现对 HDFS 上数据的随机访问。它具有以下特性：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>不支持复杂的事务，只支持行级事务，即单行数据的读写都是原子性的；</p>
</li>
<li class="lvl-2">
<p>由于是采用 HDFS 作为底层存储，所以和 HDFS 一样，支持结构化、半结构化和非结构化的存储；</p>
</li>
<li class="lvl-2">
<p>支持通过增加机器进行横向扩展；</p>
</li>
<li class="lvl-2">
<p>支持数据分片；</p>
</li>
<li class="lvl-2">
<p>支持 RegionServers 之间的自动故障转移；</p>
</li>
<li class="lvl-2">
<p>易于使用的 Java 客户端 API；</p>
</li>
<li class="lvl-2">
<p>支持 BlockCache 和布隆过滤器；</p>
</li>
<li class="lvl-2">
<p>过滤器支持谓词下推。</p>
</li>
</ul>
<h2 id="三、HBase-Table">三、HBase Table</h2>
<p>HBase 是一个面向 <code>列</code> 的数据库管理系统，这里更为确切的而说，HBase 是一个面向 <code>列族</code> 的数据库管理系统。表 schema 仅定义列族，表具有多个列族，每个列族可以包含任意数量的列，列由多个单元格（cell ）组成，单元格可以存储多个版本的数据，多个版本数据以时间戳进行区分。</p>
<p>下图为 HBase 中一张表的：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>RowKey 为行的唯一标识，所有行按照 RowKey 的字典序进行排序；</p>
</li>
<li class="lvl-2">
<p>该表具有两个列族，分别是 personal 和 office;</p>
</li>
<li class="lvl-2">
<p>其中列族 personal 拥有 name、city、phone 三个列，列族 office 拥有 tel、addres 两个列。</p>
</li>
</ul>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBase_table-iteblog.png"/> </div>
<blockquote>
<p><em>图片引用自 : HBase 是列式存储数据库吗</em> <em><a href="https://www.iteblog.com/archives/2498.html">https://www.iteblog.com/archives/2498.html</a></em></p>
</blockquote>
<p>Hbase 的表具有以下特点：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>容量大：一个表可以有数十亿行，上百万列；</p>
</li>
<li class="lvl-2">
<p>面向列：数据是按照列存储，每一列都单独存放，数据即索引，在查询时可以只访问指定列的数据，有效地降低了系统的 I/O 负担；</p>
</li>
<li class="lvl-2">
<p>稀疏性：空 (null) 列并不占用存储空间，表可以设计的非常稀疏  ；</p>
</li>
<li class="lvl-2">
<p>数据多版本：每个单元中的数据可以有多个版本，按照时间戳排序，新的数据在最上面；</p>
</li>
<li class="lvl-2">
<p>存储类型：所有数据的底层存储格式都是字节数组 (byte[])。</p>
</li>
</ul>
<h2 id="四、Phoenix">四、Phoenix</h2>
<p><code>Phoenix</code> 是 HBase 的开源 SQL 中间层，它允许你使用标准 JDBC 的方式来操作 HBase 上的数据。在 <code>Phoenix</code> 之前，如果你要访问 HBase，只能调用它的 Java API，但相比于使用一行 SQL 就能实现数据查询，HBase 的 API 还是过于复杂。<code>Phoenix</code> 的理念是 <code>we put sql SQL back in NOSQL</code>，即你可以使用标准的 SQL 就能完成对 HBase 上数据的操作。同时这也意味着你可以通过集成 <code>Spring Data  JPA</code> 或 <code>Mybatis</code> 等常用的持久层框架来操作 HBase。</p>
<p>其次 <code>Phoenix</code> 的性能表现也非常优异，<code>Phoenix</code> 查询引擎会将 SQL 查询转换为一个或多个 HBase Scan，通过并行执行来生成标准的 JDBC 结果集。它通过直接使用 HBase API 以及协处理器和自定义过滤器，可以为小型数据查询提供毫秒级的性能，为千万行数据的查询提供秒级的性能。同时 Phoenix 还拥有二级索引等 HBase 不具备的特性，因为以上的优点，所以 <code>Phoenix</code> 成为了 HBase 最优秀的 SQL 中间层。</p>
<h2 id="参考资料-14">参考资料</h2>
<ol>
<li class="lvl-3">
<p><a href="https://www.tutorialspoint.com/hbase/hbase_overview.htm">HBase - Overview</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase的SQL中间层—Phoenix</title>
    <url>/2021/10/25/Hbase%E7%9A%84SQL%E4%B8%AD%E9%97%B4%E5%B1%82%E2%80%94Phoenix/</url>
    <content><![CDATA[<h2 id="一、Phoenix简介">一、Phoenix简介</h2>
<p><code>Phoenix</code> 是 HBase 的开源 SQL 中间层，它允许你使用标准 JDBC 的方式来操作 HBase 上的数据。在 <code>Phoenix</code> 之前，如果你要访问 HBase，只能调用它的 Java API，但相比于使用一行 SQL 就能实现数据查询，HBase 的 API 还是过于复杂。<code>Phoenix</code> 的理念是 <code>we put sql SQL back in NOSQL</code>，即你可以使用标准的 SQL 就能完成对 HBase 上数据的操作。同时这也意味着你可以通过集成 <code>Spring Data  JPA</code> 或 <code>Mybatis</code> 等常用的持久层框架来操作 HBase。</p>
<p>其次 <code>Phoenix</code> 的性能表现也非常优异，<code>Phoenix</code> 查询引擎会将 SQL 查询转换为一个或多个 HBase Scan，通过并行执行来生成标准的 JDBC 结果集。它通过直接使用 HBase API 以及协处理器和自定义过滤器，可以为小型数据查询提供毫秒级的性能，为千万行数据的查询提供秒级的性能。同时 Phoenix 还拥有二级索引等 HBase 不具备的特性，因为以上的优点，所以 <code>Phoenix</code> 成为了 HBase 最优秀的 SQL 中间层。</p>
<div align="center"> <img width="600px"  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-hadoop.png"/> </div>
<h2 id="二、Phoenix安装">二、Phoenix安装</h2>
<blockquote>
<p>我们可以按照官方安装说明进行安装，官方说明如下：</p>
<ul class="lvl-1">
<li class="lvl-2">download and expand our installation tar</li>
<li class="lvl-2">copy the phoenix server jar that is compatible with your HBase installation into the lib directory of every region server</li>
<li class="lvl-2">restart the region servers</li>
<li class="lvl-2">add the phoenix client jar to the classpath of your HBase client</li>
<li class="lvl-2">download and setup SQuirrel as your SQL client so you can issue adhoc SQL against your HBase cluster</li>
</ul>
</blockquote>
<h3 id="2-1-下载并解压-2">2.1 下载并解压</h3>
<p>官方针对 Apache 版本和 CDH 版本的 HBase 均提供了安装包，按需下载即可。官方下载地址: <a href="http://phoenix.apache.org/download.html">http://phoenix.apache.org/download.html</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">下载</span></span><br><span class="line">wget http://mirror.bit.edu.cn/apache/phoenix/apache-phoenix-4.14.0-cdh5.14.2/bin/apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz</span><br><span class="line"><span class="meta"># </span><span class="language-bash">解压</span></span><br><span class="line">tar tar apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz</span><br></pre></td></tr></table></figure>
<h3 id="2-2-拷贝Jar包">2.2 拷贝Jar包</h3>
<p>按照官方文档的说明，需要将 <code>phoenix server jar</code> 添加到所有 <code>Region Servers</code> 的安装目录的 <code>lib</code> 目录下。</p>
<p>这里由于我搭建的是 HBase 伪集群，所以只需要拷贝到当前机器的 HBase 的 lib 目录下。如果是真实集群，则使用 scp 命令分发到所有 <code>Region Servers</code> 机器上。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp /usr/app/apache-phoenix-4.14.0-cdh5.14.2-bin/phoenix-4.14.0-cdh5.14.2-server.jar /usr/app/hbase-1.2.0-cdh5.15.2/lib</span><br></pre></td></tr></table></figure>
<h3 id="2-3-重启-Region-Servers">2.3 重启 Region Servers</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">停止Hbase</span></span><br><span class="line">stop-hbase.sh</span><br><span class="line"><span class="meta"># </span><span class="language-bash">启动Hbase</span></span><br><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure>
<h3 id="2-4-启动Phoenix">2.4 启动Phoenix</h3>
<p>在 Phoenix 解压目录下的 <code>bin</code> 目录下执行如下命令，需要指定 Zookeeper 的地址：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>如果 HBase 采用 Standalone 模式或者伪集群模式搭建，则默认采用内置的 Zookeeper 服务，端口为 2181；</p>
</li>
<li class="lvl-2">
<p>如果是 HBase 是集群模式并采用外置的 Zookeeper 集群，则按照自己的实际情况进行指定。</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">./sqlline.py hadoop001:2181</span></span><br></pre></td></tr></table></figure>
<h3 id="2-5-启动结果">2.5 启动结果</h3>
<p>启动后则进入了 Phoenix 交互式 SQL 命令行，可以使用 <code>!table</code> 或 <code>!tables</code> 查看当前所有表的信息</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/phoenix-shell.png"/> </div>
<h2 id="三、Phoenix-简单使用">三、Phoenix 简单使用</h2>
<h3 id="3-1-创建表">3.1 创建表</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> us_population (</span><br><span class="line">      state <span class="type">CHAR</span>(<span class="number">2</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">      city <span class="type">VARCHAR</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">      population <span class="type">BIGINT</span></span><br><span class="line">      <span class="keyword">CONSTRAINT</span> my_pk <span class="keyword">PRIMARY</span> KEY (state, city));</span><br></pre></td></tr></table></figure>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-create-table.png"/> </div>
新建的表会按照特定的规则转换为 HBase 上的表，关于表的信息，可以通过 Hbase Web UI 进行查看：
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-web-ui-phoenix.png"/> </div>
### 3.2 插入数据
<p>Phoenix 中插入数据采用的是 <code>UPSERT</code> 而不是 <code>INSERT</code>,因为 Phoenix 并没有更新操作，插入相同主键的数据就视为更新，所以 <code>UPSERT</code> 就相当于 <code>UPDATE</code>+<code>INSERT</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">UPSERT INTO us_population VALUES(&#x27;NY&#x27;,&#x27;New York&#x27;,8143197);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;CA&#x27;,&#x27;Los Angeles&#x27;,3844829);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;IL&#x27;,&#x27;Chicago&#x27;,2842518);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;TX&#x27;,&#x27;Houston&#x27;,2016582);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;PA&#x27;,&#x27;Philadelphia&#x27;,1463281);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;AZ&#x27;,&#x27;Phoenix&#x27;,1461575);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;TX&#x27;,&#x27;San Antonio&#x27;,1256509);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;CA&#x27;,&#x27;San Diego&#x27;,1255540);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;TX&#x27;,&#x27;Dallas&#x27;,1213825);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;CA&#x27;,&#x27;San Jose&#x27;,912332);</span><br></pre></td></tr></table></figure>
<h3 id="3-3-修改数据">3.3 修改数据</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 插入主键相同的数据就视为更新</span></span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;NY&#x27;</span>,<span class="string">&#x27;New York&#x27;</span>,<span class="number">999999</span>);</span><br></pre></td></tr></table></figure>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-update.png"/> </div>
### 3.4 删除数据
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> us_population <span class="keyword">WHERE</span> city<span class="operator">=</span><span class="string">&#x27;Dallas&#x27;</span>;</span><br></pre></td></tr></table></figure>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-delete.png"/> </div>
### 3.5 查询数据
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> state <span class="keyword">as</span> &quot;州&quot;,<span class="built_in">count</span>(city) <span class="keyword">as</span> &quot;市&quot;,<span class="built_in">sum</span>(population) <span class="keyword">as</span> &quot;热度&quot;</span><br><span class="line"><span class="keyword">FROM</span> us_population</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> state</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="built_in">sum</span>(population) <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-select.png"/> </div>
<h3 id="3-6-退出命令">3.6 退出命令</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">!</span>quit</span><br></pre></td></tr></table></figure>
<h3 id="3-7-扩展">3.7 扩展</h3>
<p>从上面的操作中可以看出，Phoenix 支持大多数标准的 SQL 语法。关于 Phoenix 支持的语法、数据类型、函数、序列等详细信息，因为涉及内容很多，可以参考其官方文档，官方文档上有详细的说明：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>语法 (Grammar)</strong> ：<a href="https://phoenix.apache.org/language/index.html">https://phoenix.apache.org/language/index.html</a></p>
</li>
<li class="lvl-2">
<p><strong>函数 (Functions)</strong> ：<a href="http://phoenix.apache.org/language/functions.html">http://phoenix.apache.org/language/functions.html</a></p>
</li>
<li class="lvl-2">
<p><strong>数据类型 (Datatypes)</strong> ：<a href="http://phoenix.apache.org/language/datatypes.html">http://phoenix.apache.org/language/datatypes.html</a></p>
</li>
<li class="lvl-2">
<p><strong>序列 (Sequences)</strong> :<a href="http://phoenix.apache.org/sequences.html">http://phoenix.apache.org/sequences.html</a></p>
</li>
<li class="lvl-2">
<p><strong>联结查询 (Joins)</strong> ：<a href="http://phoenix.apache.org/joins.html">http://phoenix.apache.org/joins.html</a></p>
</li>
</ul>
<h2 id="四、Phoenix-Java-API">四、Phoenix Java API</h2>
<p>因为 Phoenix 遵循 JDBC 规范，并提供了对应的数据库驱动 <code>PhoenixDriver</code>，这使得采用 Java 语言对其进行操作的时候，就如同对其他关系型数据库一样，下面给出基本的使用示例。</p>
<h3 id="4-1-引入Phoenix-core-JAR包">4.1 引入Phoenix core JAR包</h3>
<p>如果是 maven 项目，直接在 maven 中央仓库找到对应的版本，导入依赖即可：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-core --&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>如果是普通项目，则可以从 Phoenix 解压目录下找到对应的 JAR 包，然后手动引入：</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/phoenix-core-jar.png"/> </div>
### 4.2 简单的Java API实例
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">PhoenixJavaApi</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 加载数据库驱动</span></span><br><span class="line">        Class.forName(<span class="string">&quot;org.apache.phoenix.jdbc.PhoenixDriver&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 指定数据库地址,格式为 jdbc:phoenix:Zookeeper 地址</span></span><br><span class="line"><span class="comment">         * 如果 HBase 采用 Standalone 模式或者伪集群模式搭建，则 HBase 默认使用内置的 Zookeeper，默认端口为 2181</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Connection</span> <span class="variable">connection</span> <span class="operator">=</span> DriverManager.getConnection(<span class="string">&quot;jdbc:phoenix:192.168.200.226:2181&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">PreparedStatement</span> <span class="variable">statement</span> <span class="operator">=</span> connection.prepareStatement(<span class="string">&quot;SELECT * FROM us_population&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">ResultSet</span> <span class="variable">resultSet</span> <span class="operator">=</span> statement.executeQuery();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            System.out.println(resultSet.getString(<span class="string">&quot;city&quot;</span>) + <span class="string">&quot; &quot;</span></span><br><span class="line">                    + resultSet.getInt(<span class="string">&quot;population&quot;</span>));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        statement.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-java-api-result.png"/> </div>
<p>实际的开发中我们通常都是采用第三方框架来操作数据库，如 <code>mybatis</code>，<code>Hibernate</code>，<code>Spring Data</code> 等。关于 Phoenix 与这些框架的整合步骤参见下一篇文章：<a href="https://github.com/ihadyou/BigData-Notes/blob/master/notes/Spring+Mybtais+Phoenix%E6%95%B4%E5%90%88.md">Spring/Spring Boot + Mybatis + Phoenix</a></p>
<h1>参考资料</h1>
<ol>
<li class="lvl-3">
<p><a href="http://phoenix.apache.org/">http://phoenix.apache.org/</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive分区表和分桶表</title>
    <url>/2021/10/15/Hive%E5%88%86%E5%8C%BA%E8%A1%A8%E5%92%8C%E5%88%86%E6%A1%B6%E8%A1%A8/</url>
    <content><![CDATA[<h2 id="一、分区表">一、分区表</h2>
<h3 id="1-1-概念">1.1 概念</h3>
<p>Hive 中的表对应为 HDFS 上的指定目录，在查询数据时候，默认会对全表进行扫描，这样时间和性能的消耗都非常大。</p>
<p><strong>分区为 HDFS 上表目录的子目录</strong>，数据按照分区存储在子目录中。如果查询的 <code>where</code> 字句的中包含分区条件，则直接从该分区去查找，而不是扫描整个表目录，合理的分区设计可以极大提高查询速度和性能。</p>
<blockquote>
<p>这里说明一下分区表并 Hive 独有的概念，实际上这个概念非常常见。比如在我们常用的 Oracle 数据库中，当表中的数据量不断增大，查询数据的速度就会下降，这时也可以对表进行分区。表进行分区后，逻辑上表仍然是一张完整的表，只是将表中的数据存放到多个表空间（物理文件上），这样查询数据时，就不必要每次都扫描整张表，从而提升查询性能。</p>
</blockquote>
<h3 id="1-2-使用场景">1.2  使用场景</h3>
<p>通常，在管理大规模数据集的时候都需要进行分区，比如将日志文件按天进行分区，从而保证数据细粒度的划分，使得查询性能得到提升。</p>
<h3 id="1-3-创建分区表">1.3 创建分区表</h3>
<p>在 Hive 中可以使用 <code>PARTITIONED BY</code> 子句创建分区表。表可以包含一个或多个分区列，程序会为分区列中的每个不同值组合创建单独的数据目录。下面的我们创建一张雇员表作为测试：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE emp_partition(</span><br><span class="line">   empno INT,</span><br><span class="line">   ename STRING,</span><br><span class="line">   job STRING,</span><br><span class="line">   mgr INT,</span><br><span class="line">   hiredate TIMESTAMP,</span><br><span class="line">   sal DECIMAL(7,2),</span><br><span class="line">   comm DECIMAL(7,2)</span><br><span class="line">   )</span><br><span class="line">   PARTITIONED BY (deptno INT)   -- 按照部门编号进行分区</span><br><span class="line">   ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">   LOCATION &#x27;/hive/emp_partition&#x27;;</span><br></pre></td></tr></table></figure>
<h3 id="1-4-加载数据到分区表">1.4 加载数据到分区表</h3>
<p>加载数据到分区表时候必须要指定数据所处的分区：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">加载部门编号为20的数据到表中</span></span><br><span class="line">LOAD DATA LOCAL INPATH &quot;/usr/file/emp20.txt&quot; OVERWRITE INTO TABLE emp_partition PARTITION (deptno=20)</span><br><span class="line"><span class="meta"># </span><span class="language-bash">加载部门编号为30的数据到表中</span></span><br><span class="line">LOAD DATA LOCAL INPATH &quot;/usr/file/emp30.txt&quot; OVERWRITE INTO TABLE emp_partition PARTITION (deptno=30)</span><br></pre></td></tr></table></figure>
<h3 id="1-5-查看分区目录">1.5 查看分区目录</h3>
<p>这时候我们直接查看表目录，可以看到表目录下存在两个子目录，分别是 <code>deptno=20</code> 和 <code>deptno=30</code>,这就是分区目录，分区目录下才是我们加载的数据文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">hadoop fs -<span class="built_in">ls</span>  hdfs://hadoop001:8020/hive/emp_partition/</span></span><br></pre></td></tr></table></figure>
<p>这时候当你的查询语句的 <code>where</code> 包含 <code>deptno=20</code>，则就去对应的分区目录下进行查找，而不用扫描全表。</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-hadoop-partitation.png"/> </div>
<h2 id="二、分桶表">二、分桶表</h2>
<h3 id="1-1-简介">1.1 简介</h3>
<p>分区提供了一个隔离数据和优化查询的可行方案，但是并非所有的数据集都可以形成合理的分区，分区的数量也不是越多越好，过多的分区条件可能会导致很多分区上没有数据。同时 Hive 会限制动态分区可以创建的最大分区数，用来避免过多分区文件对文件系统产生负担。鉴于以上原因，Hive 还提供了一种更加细粒度的数据拆分方案：分桶表 (bucket Table)。</p>
<p>分桶表会将指定列的值进行哈希散列，并对 bucket（桶数量）取余，然后存储到对应的 bucket（桶）中。</p>
<h3 id="1-2-理解分桶表">1.2 理解分桶表</h3>
<p>单从概念上理解分桶表可能会比较晦涩，其实和分区一样，分桶这个概念同样不是 Hive 独有的，对于 Java 开发人员而言，这可能是一个每天都会用到的概念，因为 Hive 中的分桶概念和 Java 数据结构中的 HashMap 的分桶概念是一致的。</p>
<p>当调用 HashMap 的 put() 方法存储数据时，程序会先对 key 值调用 hashCode() 方法计算出 hashcode，然后对数组长度取模计算出 index，最后将数据存储在数组 index 位置的链表上，链表达到一定阈值后会转换为红黑树 (JDK1.8+)。下图为 HashMap 的数据结构图：</p>
<div align="center"> <img width="600px"  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/HashMap-HashTable.png"/> </div>
<blockquote>
<p>图片引用自：<a href="http://www.itcuties.com/java/hashmap-hashtable/">HashMap vs. Hashtable</a></p>
</blockquote>
<h3 id="1-3-创建分桶表">1.3 创建分桶表</h3>
<p>在 Hive 中，我们可以通过 <code>CLUSTERED BY</code> 指定分桶列，并通过 <code>SORTED BY</code> 指定桶中数据的排序参考列。下面为分桶表建表语句示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_bucket(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  deptno <span class="type">INT</span>)</span><br><span class="line">  CLUSTERED <span class="keyword">BY</span>(empno) SORTED <span class="keyword">BY</span>(empno <span class="keyword">ASC</span>) <span class="keyword">INTO</span> <span class="number">4</span> BUCKETS  <span class="comment">--按照员工编号散列到四个 bucket 中</span></span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;</span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_bucket&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h3 id="1-4-加载数据到分桶表">1.4 加载数据到分桶表</h3>
<p>这里直接使用 <code>Load</code> 语句向分桶表加载数据，数据时可以加载成功的，但是数据并不会分桶。</p>
<p>这是由于分桶的实质是对指定字段做了 hash 散列然后存放到对应文件中，这意味着向分桶表中插入数据是必然要通过 MapReduce，且 Reducer 的数量必须等于分桶的数量。由于以上原因，分桶表的数据通常只能使用 CTAS(CREATE TABLE AS SELECT) 方式插入，因为 CTAS 操作会触发 MapReduce。加载数据步骤如下：</p>
<h4 id="1-设置强制分桶">1. 设置强制分桶</h4>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing <span class="operator">=</span> <span class="literal">true</span>; <span class="comment">--Hive 2.x 不需要这一步</span></span><br></pre></td></tr></table></figure>
<p>在 Hive 0.x and 1.x 版本，必须使用设置 <code>hive.enforce.bucketing = true</code>，表示强制分桶，允许程序根据表结构自动选择正确数量的 Reducer 和 cluster by  column 来进行分桶。</p>
<h4 id="2-CTAS导入数据">2. CTAS导入数据</h4>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_bucket <span class="keyword">SELECT</span> <span class="operator">*</span>  <span class="keyword">FROM</span> emp;  <span class="comment">--这里的 emp 表就是一张普通的雇员表</span></span><br></pre></td></tr></table></figure>
<p>可以从执行日志看到 CTAS 触发 MapReduce 操作，且 Reducer 数量和建表时候指定 bucket 数量一致：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-hadoop-mapreducer.png"/> </div>
<h3 id="1-5-查看分桶文件">1.5 查看分桶文件</h3>
<p>bucket(桶) 本质上就是表目录下的具体文件：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-hadoop-bucket.png"/> </div>
<h2 id="三、分区表和分桶表结合使用">三、分区表和分桶表结合使用</h2>
<p>分区表和分桶表的本质都是将数据按照不同粒度进行拆分，从而使得在查询时候不必扫描全表，只需要扫描对应的分区或分桶，从而提升查询效率。两者可以结合起来使用，从而保证表数据在不同粒度上都能得到合理的拆分。下面是 Hive 官方给出的示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view_bucketed(</span><br><span class="line">	viewTime <span class="type">INT</span>, </span><br><span class="line">    userid <span class="type">BIGINT</span>,</span><br><span class="line">    page_url STRING, </span><br><span class="line">    referrer_url STRING,</span><br><span class="line">    ip STRING )</span><br><span class="line"> PARTITIONED <span class="keyword">BY</span>(dt STRING)</span><br><span class="line"> CLUSTERED <span class="keyword">BY</span>(userid) SORTED <span class="keyword">BY</span>(viewTime) <span class="keyword">INTO</span> <span class="number">32</span> BUCKETS</span><br><span class="line"> <span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">   FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\001&#x27;</span></span><br><span class="line">   COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\002&#x27;</span></span><br><span class="line">   MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\003&#x27;</span></span><br><span class="line"> STORED <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure>
<p>此时导入数据时需要指定分区：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">INSERT OVERWRITE page_view_bucketed</span><br><span class="line">PARTITION (dt=&#x27;2009-02-25&#x27;)</span><br><span class="line">SELECT * FROM page_view WHERE dt=&#x27;2009-02-25&#x27;;</span><br></pre></td></tr></table></figure>
<h2 id="参考资料-15">参考资料</h2>
<ol>
<li class="lvl-3">
<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL+BucketedTables">LanguageManual DDL BucketedTables</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase过滤器</title>
    <url>/2021/10/25/Hbase%E8%BF%87%E6%BB%A4%E5%99%A8/</url>
    <content><![CDATA[<h2 id="一、HBase过滤器简介">一、HBase过滤器简介</h2>
<p>Hbase 提供了种类丰富的过滤器（filter）来提高数据处理的效率，用户可以通过内置或自定义的过滤器来对数据进行过滤，所有的过滤器都在服务端生效，即谓词下推（predicate push down）。这样可以保证过滤掉的数据不会被传送到客户端，从而减轻网络传输和客户端处理的压力。</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-fliter.png"/> </div>
<h2 id="二、过滤器基础">二、过滤器基础</h2>
<h3 id="2-1-Filter接口和FilterBase抽象类">2.1  Filter接口和FilterBase抽象类</h3>
<p>Filter 接口中定义了过滤器的基本方法，FilterBase 抽象类实现了 Filter 接口。所有内置的过滤器则直接或者间接继承自 FilterBase 抽象类。用户只需要将定义好的过滤器通过 <code>setFilter</code> 方法传递给 <code>Scan</code> 或 <code>put</code> 的实例即可。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">setFilter(Filter filter)</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Scan 中定义的 setFilter</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"> <span class="keyword">public</span> Scan <span class="title function_">setFilter</span><span class="params">(Filter filter)</span> &#123;</span><br><span class="line">   <span class="built_in">super</span>.setFilter(filter);</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">this</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">// Get 中定义的 setFilter</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"> <span class="keyword">public</span> Get <span class="title function_">setFilter</span><span class="params">(Filter filter)</span> &#123;</span><br><span class="line">   <span class="built_in">super</span>.setFilter(filter);</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">this</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>FilterBase 的所有子类过滤器如下：<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-filterbase-subclass.png"/> </div></p>
<blockquote>
<p>说明：上图基于当前时间点（2019.4）最新的 Hbase-2.1.4 ，下文所有说明均基于此版本。</p>
</blockquote>
<h3 id="2-2-过滤器分类">2.2 过滤器分类</h3>
<p>HBase 内置过滤器可以分为三类：分别是比较过滤器，专用过滤器和包装过滤器。分别在下面的三个小节中做详细的介绍。</p>
<h2 id="三、比较过滤器">三、比较过滤器</h2>
<p>所有比较过滤器均继承自 <code>CompareFilter</code>。创建一个比较过滤器需要两个参数，分别是<strong>比较运算符</strong>和<strong>比较器实例</strong>。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">CompareFilter</span><span class="params">(<span class="keyword">final</span> CompareOp compareOp,<span class="keyword">final</span> ByteArrayComparable comparator)</span> &#123;</span><br><span class="line">   <span class="built_in">this</span>.compareOp = compareOp;</span><br><span class="line">   <span class="built_in">this</span>.comparator = comparator;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-1-比较运算符">3.1 比较运算符</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>LESS (&lt;)</p>
</li>
<li class="lvl-2">
<p>LESS_OR_EQUAL (&lt;=)</p>
</li>
<li class="lvl-2">
<p>EQUAL (=)</p>
</li>
<li class="lvl-2">
<p>NOT_EQUAL (!=)</p>
</li>
<li class="lvl-2">
<p>GREATER_OR_EQUAL (&gt;=)</p>
</li>
<li class="lvl-2">
<p>GREATER (&gt;)</p>
</li>
<li class="lvl-2">
<p>NO_OP (排除所有符合条件的值)</p>
</li>
</ul>
<p>比较运算符均定义在枚举类 <code>CompareOperator</code> 中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@InterfaceAudience</span>.Public</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> <span class="title class_">CompareOperator</span> &#123;</span><br><span class="line">  LESS,</span><br><span class="line">  LESS_OR_EQUAL,</span><br><span class="line">  EQUAL,</span><br><span class="line">  NOT_EQUAL,</span><br><span class="line">  GREATER_OR_EQUAL,</span><br><span class="line">  GREATER,</span><br><span class="line">  NO_OP,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：在 1.x 版本的 HBase 中，比较运算符定义在 <code>CompareFilter.CompareOp</code> 枚举类中，但在 2.0 之后这个类就被标识为 @deprecated ，并会在 3.0 移除。所以 2.0 之后版本的 HBase 需要使用 <code>CompareOperator</code> 这个枚举类。</p>
</blockquote>
<h3 id="3-2-比较器">3.2 比较器</h3>
<p>所有比较器均继承自 <code>ByteArrayComparable</code> 抽象类，常用的有以下几种：</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-bytearraycomparable.png"/> </div>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>BinaryComparator</strong>  : 使用 <code>Bytes.compareTo(byte []，byte [])</code> 按字典序比较指定的字节数组。</p>
</li>
<li class="lvl-2">
<p><strong>BinaryPrefixComparator</strong> : 按字典序与指定的字节数组进行比较，但只比较到这个字节数组的长度。</p>
</li>
<li class="lvl-2">
<p><strong>RegexStringComparator</strong> :  使用给定的正则表达式与指定的字节数组进行比较。仅支持 <code>EQUAL</code> 和 <code>NOT_EQUAL</code> 操作。</p>
</li>
<li class="lvl-2">
<p><strong>SubStringComparator</strong> : 测试给定的子字符串是否出现在指定的字节数组中，比较不区分大小写。仅支持 <code>EQUAL</code> 和 <code>NOT_EQUAL</code> 操作。</p>
</li>
<li class="lvl-2">
<p><strong>NullComparator</strong> ：判断给定的值是否为空。</p>
</li>
<li class="lvl-2">
<p><strong>BitComparator</strong> ：按位进行比较。</p>
</li>
</ul>
<p><code>BinaryPrefixComparator</code> 和 <code>BinaryComparator</code> 的区别不是很好理解，这里举例说明一下：</p>
<p>在进行 <code>EQUAL</code> 的比较时，如果比较器传入的是 <code>abcd</code> 的字节数组，但是待比较数据是 <code>abcdefgh</code>：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>如果使用的是 <code>BinaryPrefixComparator</code> 比较器，则比较以 <code>abcd</code> 字节数组的长度为准，即 <code>efgh</code> 不会参与比较，这时候认为 <code>abcd</code> 与 <code>abcdefgh</code> 是满足 <code>EQUAL</code> 条件的；</p>
</li>
<li class="lvl-2">
<p>如果使用的是 <code>BinaryComparator</code> 比较器，则认为其是不相等的。</p>
</li>
</ul>
<h3 id="3-3-比较过滤器种类">3.3 比较过滤器种类</h3>
<p>比较过滤器共有五个（Hbase 1.x 版本和 2.x 版本相同），见下图：</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-compareFilter.png"/> </div>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>RowFilter</strong> ：基于行键来过滤数据；</p>
</li>
<li class="lvl-2">
<p><strong>FamilyFilterr</strong> ：基于列族来过滤数据；</p>
</li>
<li class="lvl-2">
<p><strong>QualifierFilterr</strong> ：基于列限定符（列名）来过滤数据；</p>
</li>
<li class="lvl-2">
<p><strong>ValueFilterr</strong> ：基于单元格 (cell) 的值来过滤数据；</p>
</li>
<li class="lvl-2">
<p><strong>DependentColumnFilter</strong> ：指定一个参考列来过滤其他列的过滤器，过滤的原则是基于参考列的时间戳来进行筛选 。</p>
</li>
</ul>
<p>前四种过滤器的使用方法相同，均只要传递比较运算符和运算器实例即可构建，然后通过 <code>setFilter</code> 方法传递给 <code>scan</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">Filter</span> <span class="variable">filter</span>  <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RowFilter</span>(CompareOperator.LESS_OR_EQUAL,</span><br><span class="line">                               <span class="keyword">new</span> <span class="title class_">BinaryComparator</span>(Bytes.toBytes(<span class="string">&quot;xxx&quot;</span>)));</span><br><span class="line"> scan.setFilter(filter);    </span><br></pre></td></tr></table></figure>
<p><code>DependentColumnFilter</code> 的使用稍微复杂一点，这里单独做下说明。</p>
<h3 id="3-4-DependentColumnFilter">3.4 DependentColumnFilter</h3>
<p>可以把 <code>DependentColumnFilter</code> 理解为<strong>一个 valueFilter 和一个时间戳过滤器的组合</strong>。<code>DependentColumnFilter</code> 有三个带参构造器，这里选择一个参数最全的进行说明：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DependentColumnFilter(<span class="keyword">final</span> <span class="type">byte</span> [] family, <span class="keyword">final</span> <span class="type">byte</span>[] qualifier,</span><br><span class="line">                               <span class="keyword">final</span> <span class="type">boolean</span> dropDependentColumn, <span class="keyword">final</span> CompareOperator op,</span><br><span class="line">                               <span class="keyword">final</span> ByteArrayComparable valueComparator)</span><br></pre></td></tr></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>family</strong>  ：列族</p>
</li>
<li class="lvl-2">
<p><strong>qualifier</strong> ：列限定符（列名）</p>
</li>
<li class="lvl-2">
<p><strong>dropDependentColumn</strong> ：决定参考列是否被包含在返回结果内，为 true 时表示参考列被返回，为 false 时表示被丢弃</p>
</li>
<li class="lvl-2">
<p><strong>op</strong> ：比较运算符</p>
</li>
<li class="lvl-2">
<p><strong>valueComparator</strong> ：比较器</p>
</li>
</ul>
<p>这里举例进行说明：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">DependentColumnFilter</span> <span class="variable">dependentColumnFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DependentColumnFilter</span>( </span><br><span class="line">    Bytes.toBytes(<span class="string">&quot;student&quot;</span>),</span><br><span class="line">    Bytes.toBytes(<span class="string">&quot;name&quot;</span>),</span><br><span class="line">    <span class="literal">false</span>,</span><br><span class="line">    CompareOperator.EQUAL, </span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">BinaryPrefixComparator</span>(Bytes.toBytes(<span class="string">&quot;xiaolan&quot;</span>)));</span><br></pre></td></tr></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>首先会去查找 <code>student:name</code> 中值以 <code>xiaolan</code> 开头的所有数据获得 <code>参考数据集</code>，这一步等同于 valueFilter 过滤器；</p>
</li>
<li class="lvl-2">
<p>其次再用参考数据集中所有数据的时间戳去检索其他列，获得时间戳相同的其他列的数据作为 <code>结果数据集</code>，这一步等同于时间戳过滤器；</p>
</li>
<li class="lvl-2">
<p>最后如果 <code>dropDependentColumn</code> 为 true，则返回 <code>参考数据集</code>+<code>结果数据集</code>，若为 false，则抛弃参考数据集，只返回 <code>结果数据集</code>。</p>
</li>
</ul>
<h2 id="四、专用过滤器">四、专用过滤器</h2>
<p>专用过滤器通常直接继承自 <code>FilterBase</code>，适用于范围更小的筛选规则。</p>
<h3 id="4-1-单列列值过滤器-SingleColumnValueFilter">4.1 单列列值过滤器 (SingleColumnValueFilter)</h3>
<p>基于某列（参考列）的值决定某行数据是否被过滤。其实例有以下方法：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>setFilterIfMissing(boolean filterIfMissing)</strong> ：默认值为 false，即如果该行数据不包含参考列，其依然被包含在最后的结果中；设置为 true 时，则不包含；</p>
</li>
<li class="lvl-2">
<p><strong>setLatestVersionOnly(boolean latestVersionOnly)</strong> ：默认为 true，即只检索参考列的最新版本数据；设置为 false，则检索所有版本数据。</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter(</span><br><span class="line">                &quot;student&quot;.getBytes(), </span><br><span class="line">                &quot;name&quot;.getBytes(), </span><br><span class="line">                CompareOperator.EQUAL, </span><br><span class="line">                new SubstringComparator(&quot;xiaolan&quot;));</span><br><span class="line">singleColumnValueFilter.setFilterIfMissing(true);</span><br><span class="line">scan.setFilter(singleColumnValueFilter);</span><br></pre></td></tr></table></figure>
<h3 id="4-2-单列列值排除器-SingleColumnValueExcludeFilter">4.2 单列列值排除器 (SingleColumnValueExcludeFilter)</h3>
<p><code>SingleColumnValueExcludeFilter</code> 继承自上面的 <code>SingleColumnValueFilter</code>，过滤行为与其相反。</p>
<h3 id="4-3-行键前缀过滤器-PrefixFilter">4.3 行键前缀过滤器 (PrefixFilter)</h3>
<p>基于 RowKey 值决定某行数据是否被过滤。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">PrefixFilter</span> <span class="variable">prefixFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PrefixFilter</span>(Bytes.toBytes(<span class="string">&quot;xxx&quot;</span>));</span><br><span class="line">scan.setFilter(prefixFilter);</span><br></pre></td></tr></table></figure>
<h3 id="4-4-列名前缀过滤器-ColumnPrefixFilter">4.4 列名前缀过滤器 (ColumnPrefixFilter)</h3>
<p>基于列限定符（列名）决定某行数据是否被过滤。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">ColumnPrefixFilter</span> <span class="variable">columnPrefixFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ColumnPrefixFilter</span>(Bytes.toBytes(<span class="string">&quot;xxx&quot;</span>));</span><br><span class="line"> scan.setFilter(columnPrefixFilter);</span><br></pre></td></tr></table></figure>
<h3 id="4-5-分页过滤器-PageFilter">4.5 分页过滤器 (PageFilter)</h3>
<p>可以使用这个过滤器实现对结果按行进行分页，创建 PageFilter 实例的时候需要传入每页的行数。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">PageFilter</span><span class="params">(<span class="keyword">final</span> <span class="type">long</span> pageSize)</span> &#123;</span><br><span class="line">    Preconditions.checkArgument(pageSize &gt;= <span class="number">0</span>, <span class="string">&quot;must be positive %s&quot;</span>, pageSize);</span><br><span class="line">    <span class="built_in">this</span>.pageSize = pageSize;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>下面的代码体现了客户端实现分页查询的主要逻辑，这里对其进行一下解释说明：</p>
<p>客户端进行分页查询，需要传递 <code>startRow</code>(起始 RowKey)，知道起始 <code>startRow</code> 后，就可以返回对应的 pageSize 行数据。这里唯一的问题就是，对于第一次查询，显然 <code>startRow</code> 就是表格的第一行数据，但是之后第二次、第三次查询我们并不知道 <code>startRow</code>，只能知道上一次查询的最后一条数据的 RowKey（简单称之为 <code>lastRow</code>）。</p>
<p>我们不能将 <code>lastRow</code> 作为新一次查询的 <code>startRow</code> 传入，因为 scan 的查询区间是[startRow，endRow) ，即前开后闭区间，这样 <code>startRow</code> 在新的查询也会被返回，这条数据就重复了。</p>
<p>同时在不使用第三方数据库存储 RowKey 的情况下，我们是无法通过知道 <code>lastRow</code> 的下一个 RowKey 的，因为 RowKey 的设计可能是连续的也有可能是不连续的。</p>
<p>由于 Hbase 的 RowKey 是按照字典序进行排序的。这种情况下，就可以在 <code>lastRow</code> 后面加上 <code>0</code> ，作为 <code>startRow</code> 传入，因为按照字典序的规则，某个值加上 <code>0</code> 后的新值，在字典序上一定是这个值的下一个值，对于 HBase 来说下一个 RowKey 在字典序上一定也是等于或者大于这个新值的。</p>
<p>所以最后传入 <code>lastRow</code>+<code>0</code>，如果等于这个值的 RowKey 存在就从这个值开始 scan,否则从字典序的下一个 RowKey 开始 scan。</p>
<blockquote>
<p>25 个字母以及数字字符，字典排序如下:</p>
<p><code>'0' &lt; '1' &lt; '2' &lt; ... &lt; '9' &lt; 'a' &lt; 'b' &lt; ... &lt; 'z'</code></p>
</blockquote>
<p>分页查询主要实现逻辑：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">byte</span>[] POSTFIX = <span class="keyword">new</span> <span class="title class_">byte</span>[] &#123; <span class="number">0x00</span> &#125;;</span><br><span class="line"><span class="type">Filter</span> <span class="variable">filter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PageFilter</span>(<span class="number">15</span>);</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="variable">totalRows</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"><span class="type">byte</span>[] lastRow = <span class="literal">null</span>;</span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">    scan.setFilter(filter);</span><br><span class="line">    <span class="keyword">if</span> (lastRow != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="comment">// 如果不是首行 则 lastRow + 0</span></span><br><span class="line">        <span class="type">byte</span>[] startRow = Bytes.add(lastRow, POSTFIX);</span><br><span class="line">        System.out.println(<span class="string">&quot;start row: &quot;</span> +</span><br><span class="line">                           Bytes.toStringBinary(startRow));</span><br><span class="line">        scan.withStartRow(startRow);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> table.getScanner(scan);</span><br><span class="line">    <span class="type">int</span> <span class="variable">localRows</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    Result result;</span><br><span class="line">    <span class="keyword">while</span> ((result = scanner.next()) != <span class="literal">null</span>) &#123;</span><br><span class="line">        System.out.println(localRows++ + <span class="string">&quot;: &quot;</span> + result);</span><br><span class="line">        totalRows++;</span><br><span class="line">        lastRow = result.getRow();</span><br><span class="line">    &#125;</span><br><span class="line">    scanner.close();</span><br><span class="line">    <span class="comment">//最后一页，查询结束  </span></span><br><span class="line">    <span class="keyword">if</span> (localRows == <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(<span class="string">&quot;total rows: &quot;</span> + totalRows);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>需要注意的是在多台 Regin Services 上执行分页过滤的时候，由于并行执行的过滤器不能共享它们的状态和边界，所以有可能每个过滤器都会在完成扫描前获取了 PageCount 行的结果，这种情况下会返回比分页条数更多的数据，分页过滤器就有失效的可能。</p>
</blockquote>
<h3 id="4-6-时间戳过滤器-TimestampsFilter">4.6 时间戳过滤器 (TimestampsFilter)</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;Long&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">list.add(<span class="number">1554975573000L</span>);</span><br><span class="line"><span class="type">TimestampsFilter</span> <span class="variable">timestampsFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TimestampsFilter</span>(list);</span><br><span class="line">scan.setFilter(timestampsFilter);</span><br></pre></td></tr></table></figure>
<h3 id="4-7-首次行键过滤器-FirstKeyOnlyFilter">4.7 首次行键过滤器 (FirstKeyOnlyFilter)</h3>
<p><code>FirstKeyOnlyFilter</code> 只扫描每行的第一列，扫描完第一列后就结束对当前行的扫描，并跳转到下一行。相比于全表扫描，其性能更好，通常用于行数统计的场景，因为如果某一行存在，则行中必然至少有一列。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">FirstKeyOnlyFilter</span> <span class="variable">firstKeyOnlyFilter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FirstKeyOnlyFilter</span>();</span><br><span class="line">scan.set(firstKeyOnlyFilter);</span><br></pre></td></tr></table></figure>
<h2 id="五、包装过滤器">五、包装过滤器</h2>
<p>包装过滤器就是通过包装其他过滤器以实现某些拓展的功能。</p>
<h3 id="5-1-SkipFilter过滤器">5.1 SkipFilter过滤器</h3>
<p><code>SkipFilter</code> 包装一个过滤器，当被包装的过滤器遇到一个需要过滤的 KeyValue 实例时，则拓展过滤整行数据。下面是一个使用示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义 ValueFilter 过滤器</span></span><br><span class="line"><span class="type">Filter</span> <span class="variable">filter1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ValueFilter</span>(CompareOperator.NOT_EQUAL,</span><br><span class="line">      <span class="keyword">new</span> <span class="title class_">BinaryComparator</span>(Bytes.toBytes(<span class="string">&quot;xxx&quot;</span>)));</span><br><span class="line"><span class="comment">// 使用 SkipFilter 进行包装</span></span><br><span class="line"><span class="type">Filter</span> <span class="variable">filter2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SkipFilter</span>(filter1);</span><br></pre></td></tr></table></figure>
<h3 id="5-2-WhileMatchFilter过滤器">5.2 WhileMatchFilter过滤器</h3>
<p><code>WhileMatchFilter</code> 包装一个过滤器，当被包装的过滤器遇到一个需要过滤的 KeyValue 实例时，<code>WhileMatchFilter</code> 则结束本次扫描，返回已经扫描到的结果。下面是其使用示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">Filter</span> <span class="variable">filter1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RowFilter</span>(CompareOperator.NOT_EQUAL,</span><br><span class="line">                               <span class="keyword">new</span> <span class="title class_">BinaryComparator</span>(Bytes.toBytes(<span class="string">&quot;rowKey4&quot;</span>)));</span><br><span class="line"></span><br><span class="line"><span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">scan.setFilter(filter1);</span><br><span class="line"><span class="type">ResultScanner</span> <span class="variable">scanner1</span> <span class="operator">=</span> table.getScanner(scan);</span><br><span class="line"><span class="keyword">for</span> (Result result : scanner1) &#123;</span><br><span class="line">    <span class="keyword">for</span> (Cell cell : result.listCells()) &#123;</span><br><span class="line">        System.out.println(cell);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">scanner1.close();</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">&quot;--------------------&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 WhileMatchFilter 进行包装</span></span><br><span class="line"><span class="type">Filter</span> <span class="variable">filter2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">WhileMatchFilter</span>(filter1);</span><br><span class="line"></span><br><span class="line">scan.setFilter(filter2);</span><br><span class="line"><span class="type">ResultScanner</span> <span class="variable">scanner2</span> <span class="operator">=</span> table.getScanner(scan);</span><br><span class="line"><span class="keyword">for</span> (Result result : scanner1) &#123;</span><br><span class="line">    <span class="keyword">for</span> (Cell cell : result.listCells()) &#123;</span><br><span class="line">        System.out.println(cell);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">scanner2.close();</span><br></pre></td></tr></table></figure>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">rowKey0/student</span>:<span class="string">name/1555035006994/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="attr">rowKey1/student</span>:<span class="string">name/1555035007019/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="attr">rowKey2/student</span>:<span class="string">name/1555035007025/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="attr">rowKey3/student</span>:<span class="string">name/1555035007037/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="attr">rowKey5/student</span>:<span class="string">name/1555035007051/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="attr">rowKey6/student</span>:<span class="string">name/1555035007057/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="attr">rowKey7/student</span>:<span class="string">name/1555035007062/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="attr">rowKey8/student</span>:<span class="string">name/1555035007068/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="attr">rowKey9/student</span>:<span class="string">name/1555035007073/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="attr">--------------------</span></span><br><span class="line"><span class="attr">rowKey0/student</span>:<span class="string">name/1555035006994/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="attr">rowKey1/student</span>:<span class="string">name/1555035007019/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="attr">rowKey2/student</span>:<span class="string">name/1555035007025/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="attr">rowKey3/student</span>:<span class="string">name/1555035007037/Put/vlen=8/seqid=0</span></span><br></pre></td></tr></table></figure>
<p>可以看到被包装后，只返回了 <code>rowKey4</code> 之前的数据。</p>
<h2 id="六、FilterList">六、FilterList</h2>
<p>以上都是讲解单个过滤器的作用，当需要多个过滤器共同作用于一次查询的时候，就需要使用 <code>FilterList</code>。<code>FilterList</code> 支持通过构造器或者 <code>addFilter</code> 方法传入多个过滤器。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 构造器传入</span></span><br><span class="line"><span class="keyword">public</span> <span class="title function_">FilterList</span><span class="params">(<span class="keyword">final</span> Operator operator, <span class="keyword">final</span> List&lt;Filter&gt; filters)</span></span><br><span class="line"><span class="keyword">public</span> <span class="title function_">FilterList</span><span class="params">(<span class="keyword">final</span> List&lt;Filter&gt; filters)</span></span><br><span class="line"><span class="keyword">public</span> <span class="title function_">FilterList</span><span class="params">(<span class="keyword">final</span> Filter... filters)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法传入</span></span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addFilter</span><span class="params">(List&lt;Filter&gt; filters)</span></span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addFilter</span><span class="params">(Filter filter)</span></span><br></pre></td></tr></table></figure>
<p>多个过滤器组合的结果由 <code>operator</code> 参数定义 ，其可选参数定义在 <code>Operator</code> 枚举类中。只有 <code>MUST_PASS_ALL</code> 和 <code>MUST_PASS_ONE</code> 两个可选的值：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>MUST_PASS_ALL</strong> ：相当于 AND，必须所有的过滤器都通过才认为通过；</p>
</li>
<li class="lvl-2">
<p><strong>MUST_PASS_ONE</strong> ：相当于 OR，只有要一个过滤器通过则认为通过。</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@InterfaceAudience</span>.Public</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">enum</span> <span class="title class_">Operator</span> &#123;</span><br><span class="line">    <span class="comment">/** !AND */</span></span><br><span class="line">    MUST_PASS_ALL,</span><br><span class="line">    <span class="comment">/** !OR */</span></span><br><span class="line">    MUST_PASS_ONE</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>使用示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;Filter&gt; filters = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;Filter&gt;();</span><br><span class="line"></span><br><span class="line"><span class="type">Filter</span> <span class="variable">filter1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RowFilter</span>(CompareOperator.GREATER_OR_EQUAL,</span><br><span class="line">                               <span class="keyword">new</span> <span class="title class_">BinaryComparator</span>(Bytes.toBytes(<span class="string">&quot;XXX&quot;</span>)));</span><br><span class="line">filters.add(filter1);</span><br><span class="line"></span><br><span class="line"><span class="type">Filter</span> <span class="variable">filter2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RowFilter</span>(CompareOperator.LESS_OR_EQUAL,</span><br><span class="line">                               <span class="keyword">new</span> <span class="title class_">BinaryComparator</span>(Bytes.toBytes(<span class="string">&quot;YYY&quot;</span>)));</span><br><span class="line">filters.add(filter2);</span><br><span class="line"></span><br><span class="line"><span class="type">Filter</span> <span class="variable">filter3</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">QualifierFilter</span>(CompareOperator.EQUAL,</span><br><span class="line">                                     <span class="keyword">new</span> <span class="title class_">RegexStringComparator</span>(<span class="string">&quot;ZZZ&quot;</span>));</span><br><span class="line">filters.add(filter3);</span><br><span class="line"></span><br><span class="line"><span class="type">FilterList</span> <span class="variable">filterList</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FilterList</span>(filters);</span><br><span class="line"></span><br><span class="line"><span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">scan.setFilter(filterList);</span><br></pre></td></tr></table></figure>
<h2 id="参考资料-16">参考资料</h2>
<p><a href="https://www.oreilly.com/library/view/hbase-the-definitive/9781449314682/ch04.html">HBase: The Definitive Guide _&gt;  Chapter 4. Client API: Advanced Features</a></p>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive常用DDL操作</title>
    <url>/2021/10/15/Hive%E5%B8%B8%E7%94%A8DDL%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h2 id="一、Database">一、Database</h2>
<h3 id="1-1-查看数据列表">1.1 查看数据列表</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> databases;</span><br></pre></td></tr></table></figure>
<div align="center"> <img width='700px' src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-show-database.png"/> </div>
<h3 id="1-2-使用数据库">1.2 使用数据库</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">USE database_name;</span><br></pre></td></tr></table></figure>
<h3 id="1-3-新建数据库">1.3 新建数据库</h3>
<p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> (DATABASE<span class="operator">|</span>SCHEMA) [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name   <span class="comment">--DATABASE|SCHEMA 是等价的</span></span><br><span class="line">  [COMMENT database_comment] <span class="comment">--数据库注释</span></span><br><span class="line">  [LOCATION hdfs_path] <span class="comment">--存储在 HDFS 上的位置</span></span><br><span class="line">  [<span class="keyword">WITH</span> DBPROPERTIES (property_name<span class="operator">=</span>property_value, ...)]; <span class="comment">--指定额外属性</span></span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> hive_test</span><br><span class="line">  COMMENT <span class="string">&#x27;hive database for test&#x27;</span></span><br><span class="line">  <span class="keyword">WITH</span> DBPROPERTIES (<span class="string">&#x27;create&#x27;</span><span class="operator">=</span><span class="string">&#x27;oicio&#x27;</span>);</span><br></pre></td></tr></table></figure>
<h3 id="1-4-查看数据库信息">1.4 查看数据库信息</h3>
<p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DESC</span> DATABASE [EXTENDED] db_name; <span class="comment">--EXTENDED 表示是否显示额外属性</span></span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DESC</span> DATABASE  EXTENDED hive_test;</span><br></pre></td></tr></table></figure>
<h3 id="1-5-删除数据库">1.5 删除数据库</h3>
<p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> (DATABASE<span class="operator">|</span>SCHEMA) [IF <span class="keyword">EXISTS</span>] database_name [RESTRICT<span class="operator">|</span>CASCADE];</span><br></pre></td></tr></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>默认行为是 RESTRICT，如果数据库中存在表则删除失败。要想删除库及其中的表，可以使用 CASCADE 级联删除。</p>
</li>
</ul>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> DATABASE IF <span class="keyword">EXISTS</span> hive_test CASCADE;</span><br></pre></td></tr></table></figure>
<h2 id="二、创建表">二、创建表</h2>
<h3 id="2-1-建表语法">2.1 建表语法</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [TEMPORARY] [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name     <span class="comment">--表名</span></span><br><span class="line">  [(col_name data_type [COMMENT col_comment],</span><br><span class="line">    ... [constraint_specification])]  <span class="comment">--列名 列数据类型</span></span><br><span class="line">  [COMMENT table_comment]   <span class="comment">--表描述</span></span><br><span class="line">  [PARTITIONED <span class="keyword">BY</span> (col_name data_type [COMMENT col_comment], ...)]  <span class="comment">--分区表分区规则</span></span><br><span class="line">  [</span><br><span class="line">    CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">   [SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span><span class="operator">|</span><span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS</span><br><span class="line">  ]  <span class="comment">--分桶表分桶规则</span></span><br><span class="line">  [SKEWED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="keyword">ON</span> ((col_value, col_value, ...), (col_value, col_value, ...), ...)  </span><br><span class="line">   [STORED <span class="keyword">AS</span> DIRECTORIES] </span><br><span class="line">  ]  <span class="comment">--指定倾斜列和值</span></span><br><span class="line">  [</span><br><span class="line">   [<span class="type">ROW</span> FORMAT row_format]    </span><br><span class="line">   [STORED <span class="keyword">AS</span> file_format]</span><br><span class="line">     <span class="operator">|</span> STORED <span class="keyword">BY</span> <span class="string">&#x27;storage.handler.class.name&#x27;</span> [<span class="keyword">WITH</span> SERDEPROPERTIES (...)]  </span><br><span class="line">  ]  <span class="comment">-- 指定行分隔符、存储文件格式或采用自定义存储格式</span></span><br><span class="line">  [LOCATION hdfs_path]  <span class="comment">-- 指定表的存储位置</span></span><br><span class="line">  [TBLPROPERTIES (property_name<span class="operator">=</span>property_value, ...)]  <span class="comment">--指定表的属性</span></span><br><span class="line">  [<span class="keyword">AS</span> select_statement];   <span class="comment">--从查询结果创建表</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-内部表">2.2 内部表</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  deptno <span class="type">INT</span>)</span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-外部表">2.3 外部表</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_external(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  deptno <span class="type">INT</span>)</span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;</span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_external&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p>使用 <code>desc format  emp_external</code> 命令可以查看表的详细信息如下：</p>
<div align="center"> <img width='700px' src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-external-table.png"/> </div>
<h3 id="2-4-分区表">2.4 分区表</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_partition(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line">  PARTITIONED <span class="keyword">BY</span> (deptno <span class="type">INT</span>)   <span class="comment">-- 按照部门编号进行分区</span></span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;</span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_partition&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-5-分桶表">2.5 分桶表</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_bucket(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  deptno <span class="type">INT</span>)</span><br><span class="line">  CLUSTERED <span class="keyword">BY</span>(empno) SORTED <span class="keyword">BY</span>(empno <span class="keyword">ASC</span>) <span class="keyword">INTO</span> <span class="number">4</span> BUCKETS  <span class="comment">--按照员工编号散列到四个 bucket 中</span></span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;</span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_bucket&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-6-倾斜表">2.6 倾斜表</h3>
<p>通过指定一个或者多个列经常出现的值（严重偏斜），Hive 会自动将涉及到这些值的数据拆分为单独的文件。在查询时，如果涉及到倾斜值，它就直接从独立文件中获取数据，而不是扫描所有文件，这使得性能得到提升。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_skewed(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line">  SKEWED <span class="keyword">BY</span> (empno) <span class="keyword">ON</span> (<span class="number">66</span>,<span class="number">88</span>,<span class="number">100</span>)  <span class="comment">--指定 empno 的倾斜值 66,88,100</span></span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;</span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_skewed&#x27;</span>;   </span><br></pre></td></tr></table></figure>
<h3 id="2-7-临时表">2.7 临时表</h3>
<p>临时表仅对当前 session 可见，临时表的数据将存储在用户的暂存目录中，并在会话结束后删除。如果临时表与永久表表名相同，则对该表名的任何引用都将解析为临时表，而不是永久表。临时表还具有以下两个限制：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>不支持分区列；</p>
</li>
<li class="lvl-2">
<p>不支持创建索引。</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">TABLE</span> emp_temp(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br></pre></td></tr></table></figure>
<h3 id="2-8-CTAS创建表">2.8 CTAS创建表</h3>
<p>支持从查询语句的结果创建表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp_copy <span class="keyword">AS</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno<span class="operator">=</span><span class="string">&#x27;20&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-9-复制表结构">2.9 复制表结构</h3>
<p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [TEMPORARY] [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name  <span class="comment">--创建表表名</span></span><br><span class="line">   <span class="keyword">LIKE</span> existing_table_or_view_name  <span class="comment">--被复制表的表名</span></span><br><span class="line">   [LOCATION hdfs_path]; <span class="comment">--存储位置</span></span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span>  IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>  emp_co  <span class="keyword">LIKE</span> emp</span><br></pre></td></tr></table></figure>
<h3 id="2-10-加载数据到表">2.10 加载数据到表</h3>
<p>加载数据到表中属于 DML 操作，这里为了方便大家测试，先简单介绍一下加载本地数据到表中：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 加载数据到 emp 表中</span></span><br><span class="line">load data <span class="keyword">local</span> inpath &quot;/usr/file/emp.txt&quot; <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure>
<p>其中 emp.txt 的内容如下，你可以直接复制使用，也可以到本仓库的<a href="https://github.com/oicio/BigData-Notes/tree/master/resources">resources</a> 目录下载：</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">7369	SMITH	CLERK	7902	1980-12-17 00:00:00	800.00		20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-02-20 00:00:00	1600.00	300.00	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-02-22 00:00:00	1250.00	500.00	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-04-02 00:00:00	2975.00		20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-09-28 00:00:00	1250.00	1400.00	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-05-01 00:00:00	2850.00		30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-06-09 00:00:00	2450.00		10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-04-19 00:00:00	1500.00		20</span><br><span class="line">7839	KING	PRESIDENT		1981-11-17 00:00:00	5000.00		10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-09-08 00:00:00	1500.00	0.00	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-05-23 00:00:00	1100.00		20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-03 00:00:00	950.00		30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-03 00:00:00	3000.00		20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-01-23 00:00:00	1300.00		10</span><br></pre></td></tr></table></figure>
<p>加载后可查询表中数据：</p>
<div align="center"> <img width='700px' src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-select-emp.png"/> </div>
<h2 id="三、修改表">三、修改表</h2>
<h3 id="3-1-重命名表">3.1 重命名表</h3>
<p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name RENAME <span class="keyword">TO</span> new_table_name;</span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> emp_temp RENAME <span class="keyword">TO</span> new_emp; <span class="comment">--把 emp_temp 表重命名为 new_emp</span></span><br></pre></td></tr></table></figure>
<h3 id="3-2-修改列">3.2 修改列</h3>
<p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name [<span class="keyword">PARTITION</span> partition_spec] CHANGE [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type</span><br><span class="line">  [COMMENT col_comment] [<span class="keyword">FIRST</span><span class="operator">|</span>AFTER column_name] [CASCADE<span class="operator">|</span>RESTRICT];</span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 修改字段名和类型</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> emp_temp CHANGE empno empno_new <span class="type">INT</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment">-- 修改字段 sal 的名称 并将其放置到 empno 字段后</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> emp_temp CHANGE sal sal_new <span class="type">decimal</span>(<span class="number">7</span>,<span class="number">2</span>)  AFTER ename;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 为字段增加注释</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> emp_temp CHANGE mgr mgr_new <span class="type">INT</span> COMMENT <span class="string">&#x27;this is column mgr&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-新增列">3.3 新增列</h3>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> emp_temp <span class="keyword">ADD</span> COLUMNS (address STRING COMMENT <span class="string">&#x27;home address&#x27;</span>);</span><br></pre></td></tr></table></figure>
<h2 id="四、清空表-删除表">四、清空表/删除表</h2>
<h3 id="4-1-清空表">4.1 清空表</h3>
<p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 清空整个表或表指定分区中的数据</span></span><br><span class="line"><span class="keyword">TRUNCATE</span> <span class="keyword">TABLE</span> table_name [<span class="keyword">PARTITION</span> (partition_column <span class="operator">=</span> partition_col_value,  ...)];</span><br></pre></td></tr></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>目前只有内部表才能执行 TRUNCATE 操作，外部表执行时会抛出异常 <code>Cannot truncate non-managed table XXXX</code>。</p>
</li>
</ul>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">TRUNCATE</span> <span class="keyword">TABLE</span> emp_mgt_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">20</span>);</span><br></pre></td></tr></table></figure>
<h3 id="4-2-删除表">4.2 删除表</h3>
<p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> [IF <span class="keyword">EXISTS</span>] table_name [PURGE]; </span><br></pre></td></tr></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>内部表：不仅会删除表的元数据，同时会删除 HDFS 上的数据；</p>
</li>
<li class="lvl-2">
<p>外部表：只会删除表的元数据，不会删除 HDFS 上的数据；</p>
</li>
<li class="lvl-2">
<p>删除视图引用的表时，不会给出警告（但视图已经无效了，必须由用户删除或重新创建）。</p>
</li>
</ul>
<h2 id="五、其他命令">五、其他命令</h2>
<h3 id="5-1-Describe">5.1 Describe</h3>
<p>查看数据库：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DESCRIBE</span><span class="operator">|</span><span class="keyword">Desc</span> DATABASE [EXTENDED] db_name;  <span class="comment">--EXTENDED 是否显示额外属性</span></span><br></pre></td></tr></table></figure>
<p>查看表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DESCRIBE</span><span class="operator">|</span><span class="keyword">Desc</span> [EXTENDED<span class="operator">|</span>FORMATTED] table_name <span class="comment">--FORMATTED 以友好的展现方式查看表详情</span></span><br></pre></td></tr></table></figure>
<h3 id="5-2-Show">5.2 Show</h3>
<p><strong>1. 查看数据库列表</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 语法</span></span><br><span class="line"><span class="keyword">SHOW</span> (DATABASES<span class="operator">|</span>SCHEMAS) [<span class="keyword">LIKE</span> <span class="string">&#x27;identifier_with_wildcards&#x27;</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 示例：</span></span><br><span class="line"><span class="keyword">SHOW</span> DATABASES <span class="keyword">like</span> <span class="string">&#x27;hive*&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p>LIKE 子句允许使用正则表达式进行过滤，但是 SHOW 语句当中的 LIKE 子句只支持 <code>*</code>（通配符）和 <code>|</code>（条件或）两个符号。例如 <code>employees</code>，<code>emp *</code>，<code>emp * | * ees</code>，所有这些都将匹配名为 <code>employees</code> 的数据库。</p>
<p><strong>2. 查看表的列表</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 语法</span></span><br><span class="line"><span class="keyword">SHOW</span> TABLES [<span class="keyword">IN</span> database_name] [<span class="string">&#x27;identifier_with_wildcards&#x27;</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 示例</span></span><br><span class="line"><span class="keyword">SHOW</span> TABLES <span class="keyword">IN</span> <span class="keyword">default</span>;</span><br></pre></td></tr></table></figure>
<p><strong>3. 查看视图列表</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> VIEWS [<span class="keyword">IN</span><span class="operator">/</span><span class="keyword">FROM</span> database_name] [<span class="keyword">LIKE</span> <span class="string">&#x27;pattern_with_wildcards&#x27;</span>];   <span class="comment">--仅支持 Hive 2.2.0 +</span></span><br></pre></td></tr></table></figure>
<p><strong>4. 查看表的分区列表</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> PARTITIONS table_name;</span><br></pre></td></tr></table></figure>
<p><strong>5. 查看表/视图的创建语句</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> ([db_name.]table_name<span class="operator">|</span>view_name);</span><br></pre></td></tr></table></figure>
<h2 id="参考资料-17">参考资料</h2>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL">LanguageManual DDL</a></p>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase集群搭建</title>
    <url>/2021/10/25/Hbase%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h2 id="一、集群规划-2">一、集群规划</h2>
<p>这里搭建一个 3 节点的 HBase 集群，其中三台主机上均为 <code>Regin Server</code>。同时为了保证高可用，除了在 hadoop001 上部署主 <code>Master</code> 服务外，还在 hadoop002 上部署备用的 <code>Master</code> 服务。Master 服务由 Zookeeper 集群进行协调管理，如果主 <code>Master</code> 不可用，则备用 <code>Master</code> 会成为新的主 <code>Master</code>。</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase集群规划.png"/> </div>
<h2 id="二、前置条件-2">二、前置条件</h2>
<p>HBase 的运行需要依赖 Hadoop 和 JDK(<code>HBase 2.0+</code> 对应 <code>JDK 1.8+</code>) 。同时为了保证高可用，这里我们不采用 HBase 内置的 Zookeeper 服务，而采用外置的 Zookeeper 集群。相关搭建步骤可以参阅：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a href="https://github.com/ihadyou/BigData-Notes/blob/master/notes/installation/Linux%E4%B8%8BJDK%E5%AE%89%E8%A3%85.md">Linux 环境下 JDK 安装</a></p>
</li>
<li class="lvl-2">
<p><a href="https://github.com/ihadyou/BigData-Notes/blob/master/notes/installation/Zookeeper%E5%8D%95%E6%9C%BA%E7%8E%AF%E5%A2%83%E5%92%8C%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.md">Zookeeper 单机环境和集群环境搭建</a></p>
</li>
<li class="lvl-2">
<p><a href="https://github.com/ihadyou/BigData-Notes/blob/master/notes/installation/Hadoop%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.md">Hadoop 集群环境搭建</a></p>
</li>
</ul>
<h2 id="三、集群搭建">三、集群搭建</h2>
<h3 id="3-1-下载并解压-3">3.1 下载并解压</h3>
<p>下载并解压，这里我下载的是 CDH 版本 HBase，下载地址为：<a href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">tar -zxvf hbase-1.2.0-cdh5.15.2.tar.gz</span></span><br></pre></td></tr></table></figure>
<h3 id="3-2-配置环境变量-3">3.2 配置环境变量</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">vim /etc/profile</span></span><br></pre></td></tr></table></figure>
<p>添加环境变量：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HBASE_HOME=usr/app/hbase-1.2.0-cdh5.15.2</span><br><span class="line">export PATH=$HBASE_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>
<p>使得配置的环境变量立即生效：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure>
<h3 id="3-3-集群配置">3.3 集群配置</h3>
<p>进入 <code>$&#123;HBASE_HOME&#125;/conf</code> 目录下，修改配置：</p>
<h4 id="1-hbase-env-sh">1. <a href="http://hbase-env.sh">hbase-env.sh</a></h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">配置JDK安装位置</span></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_201</span><br><span class="line"><span class="meta"># </span><span class="language-bash">不使用内置的zookeeper服务</span></span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure>
<h4 id="2-hbase-site-xml">2. hbase-site.xml</h4>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 指定 hbase 以分布式集群的方式运行 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 指定 hbase 在 HDFS 上的存储位置 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop001:8020/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 指定 zookeeper 的地址--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:2181,hadoop002:2181,hadoop003:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="3-regionservers">3. regionservers</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop001</span><br><span class="line">hadoop002</span><br><span class="line">hadoop003</span><br></pre></td></tr></table></figure>
<h4 id="4-backup-masters">4. backup-masters</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop002</span><br></pre></td></tr></table></figure>
<p><code> backup-masters</code> 这个文件是不存在的，需要新建，主要用来指明备用的 master 节点，可以是多个，这里我们以 1 个为例。</p>
<h3 id="3-4-HDFS客户端配置">3.4 HDFS客户端配置</h3>
<p>这里有一个可选的配置：如果您在 Hadoop 集群上进行了 HDFS 客户端配置的更改，比如将副本系数 <code>dfs.replication</code> 设置成 5，则必须使用以下方法之一来使 HBase 知道，否则 HBase 将依旧使用默认的副本系数 3 来创建文件：</p>
<blockquote>
<ol>
<li class="lvl-3">
<p>Add a pointer to your <code>HADOOP_CONF_DIR</code> to the <code>HBASE_CLASSPATH</code> environment variable in <em><a href="http://hbase-env.sh">hbase-env.sh</a></em>.</p>
</li>
<li class="lvl-3">
<p>Add a copy of <em>hdfs-site.xml</em> (or <em>hadoop-site.xml</em>) or, better, symlinks, under <em>${HBASE_HOME}/conf</em>, or</p>
</li>
<li class="lvl-3">
<p>if only a small set of HDFS client configurations, add them to <em>hbase-site.xml</em>.</p>
</li>
</ol>
</blockquote>
<p>以上是官方文档的说明，这里解释一下：</p>
<p><strong>第一种</strong> ：将 Hadoop 配置文件的位置信息添加到 <code>hbase-env.sh</code> 的 <code>HBASE_CLASSPATH</code> 属性，示例如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HBASE_CLASSPATH=usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop</span><br></pre></td></tr></table></figure>
<p><strong>第二种</strong> ：将 Hadoop 的 <code> hdfs-site.xml</code> 或 <code>hadoop-site.xml</code> 拷贝到  <code>$&#123;HBASE_HOME&#125;/conf </code> 目录下，或者通过符号链接的方式。如果采用这种方式的话，建议将两者都拷贝或建立符号链接，示例如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">拷贝</span></span><br><span class="line">cp core-site.xml hdfs-site.xml /usr/app/hbase-1.2.0-cdh5.15.2/conf/</span><br><span class="line"><span class="meta"># </span><span class="language-bash">使用符号链接</span></span><br><span class="line">ln -s   /usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop/core-site.xml</span><br><span class="line">ln -s   /usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop/hdfs-site.xml</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：<code>hadoop-site.xml</code> 这个配置文件现在叫做 <code>core-site.xml</code></p>
</blockquote>
<p><strong>第三种</strong> ：如果你只有少量更改，那么直接配置到 <code>hbase-site.xml</code> 中即可。</p>
<h3 id="3-5-安装包分发">3.5 安装包分发</h3>
<p>将 HBase 的安装包分发到其他服务器，分发后建议在这两台服务器上也配置一下 HBase 的环境变量。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scp -r /usr/app/hbase-1.2.0-cdh5.15.2/  hadoop002:usr/app/</span><br><span class="line">scp -r /usr/app/hbase-1.2.0-cdh5.15.2/  hadoop003:usr/app/</span><br></pre></td></tr></table></figure>
<h2 id="四、启动集群">四、启动集群</h2>
<h3 id="4-1-启动ZooKeeper集群">4.1 启动ZooKeeper集群</h3>
<p>分别到三台服务器上启动 ZooKeeper 服务：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure>
<h3 id="4-2-启动Hadoop集群">4.2 启动Hadoop集群</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">启动dfs服务</span></span><br><span class="line">start-dfs.sh</span><br><span class="line"><span class="meta"># </span><span class="language-bash">启动yarn服务</span></span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure>
<h3 id="4-3-启动HBase集群">4.3 启动HBase集群</h3>
<p>进入 hadoop001 的 <code>$&#123;HBASE_HOME&#125;/bin</code>，使用以下命令启动 HBase 集群。执行此命令后，会在 hadoop001 上启动 <code>Master</code> 服务，在 hadoop002 上启动备用 <code>Master</code> 服务，在 <code>regionservers</code> 文件中配置的所有节点启动 <code>region server</code> 服务。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure>
<h3 id="4-5-查看服务">4.5 查看服务</h3>
<p>访问 HBase 的 Web-UI 界面，这里我安装的 HBase 版本为 1.2，访问端口为 <code>60010</code>，如果你安装的是 2.0 以上的版本，则访问端口号为 <code>16010</code>。可以看到 <code>Master</code> 在 hadoop001 上，三个 <code>Regin Servers</code> 分别在 hadoop001，hadoop002，和 hadoop003 上，并且还有一个 <code>Backup Matser</code> 服务在 hadoop002 上。</p>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-集群搭建1.png"/> </div>
<br/>
<p>hadoop002 上的 HBase 出于备用状态：</p>
<br/>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-集群搭建2.png"/> </div>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive 常用DML操作</title>
    <url>/2021/10/15/Hive%E5%B8%B8%E7%94%A8DML%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h2 id="一、加载文件数据到表">一、加载文件数据到表</h2>
<h3 id="1-1-语法">1.1 语法</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">LOAD DATA [LOCAL] INPATH &#x27;filepath&#x27; [OVERWRITE] </span><br><span class="line">INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</span><br></pre></td></tr></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p><code>LOCAL</code> 关键字代表从本地文件系统加载文件，省略则代表从 HDFS 上加载文件：</p>
</li>
</ul>
<ul class="lvl-0">
<li class="lvl-2">
<p>从本地文件系统加载文件时， <code>filepath</code> 可以是绝对路径也可以是相对路径 (建议使用绝对路径)；</p>
</li>
<li class="lvl-2">
<p>从 HDFS 加载文件时候，<code>filepath</code> 为文件完整的 URL 地址：如 <code>hdfs://namenode:port/user/hive/project/ data1</code></p>
</li>
</ul>
<ul class="lvl-0">
<li class="lvl-2">
<p><code>filepath</code> 可以是文件路径 (在这种情况下 Hive 会将文件移动到表中)，也可以目录路径 (在这种情况下，Hive 会将该目录中的所有文件移动到表中)；</p>
</li>
<li class="lvl-2">
<p>如果使用 OVERWRITE 关键字，则将删除目标表（或分区）的内容，使用新的数据填充；不使用此关键字，则数据以追加的方式加入；</p>
</li>
<li class="lvl-2">
<p>加载的目标可以是表或分区。如果是分区表，则必须指定加载数据的分区；</p>
</li>
<li class="lvl-2">
<p>加载文件的格式必须与建表时使用 <code> STORED AS</code> 指定的存储格式相同。</p>
</li>
</ul>
<blockquote>
<p>使用建议：</p>
<p><strong>不论是本地路径还是 URL 都建议使用完整的</strong>。虽然可以使用不完整的 URL 地址，此时 Hive 将使用 hadoop 中的 <a href="http://fs.default.name">fs.default.name</a> 配置来推断地址，但是为避免不必要的错误，建议使用完整的本地路径或 URL 地址；</p>
<p><strong>加载对象是分区表时建议显示指定分区</strong>。在 Hive 3.0 之后，内部将加载 (LOAD) 重写为 INSERT AS SELECT，此时如果不指定分区，INSERT AS SELECT 将假设最后一组列是分区列，如果该列不是表定义的分区，它将抛出错误。为避免错误，还是建议显示指定分区。</p>
</blockquote>
<h3 id="1-2-示例">1.2 示例</h3>
<p>新建分区表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp_ptn(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line">  PARTITIONED <span class="keyword">BY</span> (deptno <span class="type">INT</span>)   <span class="comment">-- 按照部门编号进行分区</span></span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br></pre></td></tr></table></figure>
<p>从 HDFS 上加载数据到分区表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">LOAD DATA  INPATH &quot;hdfs://hadoop001:8020/mydir/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">20</span>);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>emp.txt 文件可在本仓库的 resources 目录中下载</p>
</blockquote>
<p>加载后表中数据如下,分区列 deptno 全部赋值成 20：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-emp-ptn.png"/> </div>
<h2 id="二、查询结果插入到表">二、查询结果插入到表</h2>
<h3 id="2-1-语法">2.1 语法</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1<span class="operator">=</span>val1, partcol2<span class="operator">=</span>val2 ...) [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]]   </span><br><span class="line">select_statement1 <span class="keyword">FROM</span> from_statement;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1<span class="operator">=</span>val1, partcol2<span class="operator">=</span>val2 ...)] </span><br><span class="line">select_statement1 <span class="keyword">FROM</span> from_statement;</span><br></pre></td></tr></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>Hive 0.13.0 开始，建表时可以通过使用 TBLPROPERTIES（“immutable”=“true”）来创建不可变表 (immutable table) ，如果不可以变表中存在数据，则 INSERT INTO 失败。（注：INSERT OVERWRITE 的语句不受 <code>immutable</code> 属性的影响）;</p>
</li>
<li class="lvl-2">
<p>可以对表或分区执行插入操作。如果表已分区，则必须通过指定所有分区列的值来指定表的特定分区；</p>
</li>
<li class="lvl-2">
<p>从 Hive 1.1.0 开始，TABLE 关键字是可选的；</p>
</li>
<li class="lvl-2">
<p>从 Hive 1.2.0 开始 ，可以采用 INSERT INTO tablename(z，x，c1) 指明插入列；</p>
</li>
<li class="lvl-2">
<p>可以将 SELECT 语句的查询结果插入多个表（或分区），称为多表插入。语法如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FROM</span> from_statement</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1 </span><br><span class="line">[<span class="keyword">PARTITION</span> (partcol1<span class="operator">=</span>val1, partcol2<span class="operator">=</span>val2 ...) [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]] select_statement1</span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ... [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]] select_statement2]</span><br><span class="line">[<span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ...] select_statement2] ...;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-2-动态插入分区">2.2 动态插入分区</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename <span class="keyword">PARTITION</span> (partcol1[<span class="operator">=</span>val1], partcol2[<span class="operator">=</span>val2] ...) </span><br><span class="line">select_statement <span class="keyword">FROM</span> from_statement;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename <span class="keyword">PARTITION</span> (partcol1[<span class="operator">=</span>val1], partcol2[<span class="operator">=</span>val2] ...) </span><br><span class="line">select_statement <span class="keyword">FROM</span> from_statement;</span><br></pre></td></tr></table></figure>
<p>在向分区表插入数据时候，分区列名是必须的，但是列值是可选的。如果给出了分区列值，我们将其称为静态分区，否则它是动态分区。动态分区列必须在 SELECT 语句的列中最后指定，并且与它们在 PARTITION() 子句中出现的顺序相同。</p>
<p>注意：Hive 0.9.0 之前的版本动态分区插入是默认禁用的，而 0.9.0 之后的版本则默认启用。以下是动态分区的相关配置：</p>
<table>
<thead>
<tr>
<th>配置</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>hive.exec.dynamic.partition</code></td>
<td><code>true</code></td>
<td>需要设置为 true 才能启用动态分区插入</td>
</tr>
<tr>
<td><code>hive.exec.dynamic.partition.mode</code></td>
<td><code>strict</code></td>
<td>在严格模式 (strict) 下，用户必须至少指定一个静态分区，以防用户意外覆盖所有分区，在非严格模式下，允许所有分区都是动态的</td>
</tr>
<tr>
<td><code>hive.exec.max.dynamic.partitions.pernode</code></td>
<td>100</td>
<td>允许在每个 mapper/reducer 节点中创建的最大动态分区数</td>
</tr>
<tr>
<td><code>hive.exec.max.dynamic.partitions</code></td>
<td>1000</td>
<td>允许总共创建的最大动态分区数</td>
</tr>
<tr>
<td><code>hive.exec.max.created.files</code></td>
<td>100000</td>
<td>作业中所有 mapper/reducer 创建的 HDFS 文件的最大数量</td>
</tr>
<tr>
<td><code>hive.error.on.empty.partition</code></td>
<td><code>false</code></td>
<td>如果动态分区插入生成空结果，是否抛出异常</td>
</tr>
</tbody>
</table>
<h3 id="2-3-示例">2.3 示例</h3>
<ol>
<li class="lvl-3">
<p>新建 emp 表，作为查询对象表</p>
</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp(</span><br><span class="line">    empno <span class="type">INT</span>,</span><br><span class="line">    ename STRING,</span><br><span class="line">    job STRING,</span><br><span class="line">    mgr <span class="type">INT</span>,</span><br><span class="line">    hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">    sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">    comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">    deptno <span class="type">INT</span>)</span><br><span class="line">    <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br><span class="line">    </span><br><span class="line"> <span class="comment">-- 加载数据到 emp 表中 这里直接从本地加载</span></span><br><span class="line">load data <span class="keyword">local</span> inpath &quot;/usr/file/emp.txt&quot; <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure>
<p>​	完成后 <code>emp</code> 表中数据如下：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-emp.png"/> </div>
<ol start="2">
<li class="lvl-3">
<p>为清晰演示，先清空 <code>emp_ptn</code> 表中加载的数据：</p>
</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">TRUNCATE</span> <span class="keyword">TABLE</span> emp_ptn;</span><br></pre></td></tr></table></figure>
<ol start="3">
<li class="lvl-3">
<p>静态分区演示：从 <code>emp</code> 表中查询部门编号为 20 的员工数据，并插入 <code>emp_ptn</code> 表中，语句如下：</p>
</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">20</span>) </span><br><span class="line"><span class="keyword">SELECT</span> empno,ename,job,mgr,hiredate,sal,comm <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno<span class="operator">=</span><span class="number">20</span>;</span><br></pre></td></tr></table></figure>
<p>​	完成后 <code>emp_ptn</code> 表中数据如下：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-emp-deptno-20.png"/> </div>
<ol start="4">
<li class="lvl-3">
<p>接着演示动态分区：</p>
</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 由于我们只有一个分区，且还是动态分区，所以需要关闭严格默认。因为在严格模式下，用户必须至少指定一个静态分区</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode<span class="operator">=</span>nonstrict;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 动态分区   此时查询语句的最后一列为动态分区列，即 deptno</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno) </span><br><span class="line"><span class="keyword">SELECT</span> empno,ename,job,mgr,hiredate,sal,comm,deptno <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno<span class="operator">=</span><span class="number">30</span>;</span><br></pre></td></tr></table></figure>
<p>​	完成后 <code>emp_ptn</code> 表中数据如下：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-emp-deptno-20-30.png"/> </div>
<h2 id="三、使用SQL语句插入值">三、使用SQL语句插入值</h2>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span> (partcol1[<span class="operator">=</span>val1], partcol2[<span class="operator">=</span>val2] ...)] </span><br><span class="line"><span class="keyword">VALUES</span> ( <span class="keyword">value</span> [, <span class="keyword">value</span> ...] )</span><br></pre></td></tr></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用时必须为表中的每个列都提供值。不支持只向部分列插入值（可以为缺省值的列提供空值来消除这个弊端）；</p>
</li>
<li class="lvl-2">
<p>如果目标表表支持 ACID 及其事务管理器，则插入后自动提交；</p>
</li>
<li class="lvl-2">
<p>不支持支持复杂类型 (array, map, struct, union) 的插入。</p>
</li>
</ul>
<h2 id="四、更新和删除数据">四、更新和删除数据</h2>
<h3 id="4-1-语法">4.1 语法</h3>
<p>更新和删除的语法比较简单，和关系型数据库一致。需要注意的是这两个操作都只能在支持 ACID 的表，也就是事务表上才能执行。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 更新</span></span><br><span class="line"><span class="keyword">UPDATE</span> tablename <span class="keyword">SET</span> <span class="keyword">column</span> <span class="operator">=</span> <span class="keyword">value</span> [, <span class="keyword">column</span> <span class="operator">=</span> <span class="keyword">value</span> ...] [<span class="keyword">WHERE</span> expression]</span><br><span class="line"></span><br><span class="line"><span class="comment">--删除</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> tablename [<span class="keyword">WHERE</span> expression]</span><br></pre></td></tr></table></figure>
<h3 id="4-2-示例">4.2 示例</h3>
<p><strong>1. 修改配置</strong></p>
<p>首先需要更改 <code>hive-site.xml</code>，添加如下配置，开启事务支持，配置完成后需要重启 Hive 服务。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.support.concurrency<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.enforce.bucketing<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.dynamic.partition.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nonstrict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.txn.manager<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.compactor.initiator.on<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.in.test<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>2. 创建测试表</strong></p>
<p>创建用于测试的事务表，建表时候指定属性 <code>transactional = true</code> 则代表该表是事务表。需要注意的是，按照<a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions">官方文档</a> 的说明，目前 Hive 中的事务表有以下限制：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>必须是 buckets Table;</p>
</li>
<li class="lvl-2">
<p>仅支持 ORC 文件格式；</p>
</li>
<li class="lvl-2">
<p>不支持 LOAD DATA …语句。</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp_ts(  </span><br><span class="line">  empno <span class="type">int</span>,  </span><br><span class="line">  ename String</span><br><span class="line">)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span> (empno) <span class="keyword">INTO</span> <span class="number">2</span> BUCKETS STORED <span class="keyword">AS</span> ORC</span><br><span class="line">TBLPROPERTIES (&quot;transactional&quot;<span class="operator">=</span>&quot;true&quot;);</span><br></pre></td></tr></table></figure>
<p><strong>3. 插入测试数据</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ts  <span class="keyword">VALUES</span> (<span class="number">1</span>,&quot;ming&quot;),(<span class="number">2</span>,&quot;hong&quot;);</span><br></pre></td></tr></table></figure>
<p>插入数据依靠的是 MapReduce 作业，执行成功后数据如下：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-emp-ts.png"/> </div>
<p><strong>4. 测试更新和删除</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--更新数据</span></span><br><span class="line"><span class="keyword">UPDATE</span> emp_ts <span class="keyword">SET</span> ename <span class="operator">=</span> &quot;lan&quot;  <span class="keyword">WHERE</span>  empno<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--删除数据</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> emp_ts <span class="keyword">WHERE</span> empno<span class="operator">=</span><span class="number">2</span>;</span><br></pre></td></tr></table></figure>
<p>更新和删除数据依靠的也是 MapReduce 作业，执行成功后数据如下：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-emp-ts-2.png"/> </div>
<h2 id="五、查询结果写出到文件系统">五、查询结果写出到文件系统</h2>
<h3 id="5-1-语法">5.1 语法</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] DIRECTORY directory1</span><br><span class="line">  [<span class="type">ROW</span> FORMAT row_format] [STORED <span class="keyword">AS</span> file_format] </span><br><span class="line">  <span class="keyword">SELECT</span> ... <span class="keyword">FROM</span> ...</span><br></pre></td></tr></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p>OVERWRITE 关键字表示输出文件存在时，先删除后再重新写入；</p>
</li>
<li class="lvl-2">
<p>和 Load 语句一样，建议无论是本地路径还是 URL 地址都使用完整的；</p>
</li>
<li class="lvl-2">
<p>写入文件系统的数据被序列化为文本，其中列默认由^A 分隔，行由换行符分隔。如果列不是基本类型，则将其序列化为 JSON 格式。其中行分隔符不允许自定义，但列分隔符可以自定义，如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 定义列分隔符为&#x27;\t&#x27; </span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;./test-04&#x27;</span> </span><br><span class="line"><span class="type">row</span> format delimited </span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> src;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="5-2-示例">5.2 示例</h3>
<p>这里我们将上面创建的 <code>emp_ptn</code> 表导出到本地文件系统，语句如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">LOCAL</span> DIRECTORY <span class="string">&#x27;/usr/file/ouput&#x27;</span></span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp_ptn;</span><br></pre></td></tr></table></figure>
<p>导出结果如下：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-ouput.png"/> </div>
<h2 id="参考资料-18">参考资料</h2>
<ol>
<li class="lvl-3">
<p><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions">Hive Transactions</a></p>
</li>
<li class="lvl-3">
<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML">Hive Data Manipulation Language</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive数据查询实战</title>
    <url>/2021/10/15/Hive%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E5%AE%9E%E6%88%98/</url>
    <content><![CDATA[<h2 id="一、数据准备">一、数据准备</h2>
<p>为了演示查询操作，这里需要预先创建三张表，并加载测试数据。</p>
<blockquote>
<p>数据文件 emp.txt 和 dept.txt 可以从本仓库的<a href="https://gitee.com/oicio/BigData-Notes/tree/master/resources">resources</a> 目录下载。</p>
</blockquote>
<h3 id="1-1-员工表">1.1 员工表</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"> <span class="comment">-- 建表语句</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp(</span><br><span class="line">                    empno <span class="type">INT</span>,     <span class="comment">-- 员工表编号</span></span><br><span class="line">                    ename STRING,  <span class="comment">-- 员工姓名</span></span><br><span class="line">                    job STRING,    <span class="comment">-- 职位类型</span></span><br><span class="line">                    mgr <span class="type">INT</span>,</span><br><span class="line">                    hiredate <span class="type">TIMESTAMP</span>,  <span class="comment">--雇佣日期</span></span><br><span class="line">                    sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),  <span class="comment">--工资</span></span><br><span class="line">                    comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">                    deptno <span class="type">INT</span>)   <span class="comment">--部门编号</span></span><br><span class="line">    <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line"><span class="comment">--加载数据</span></span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp;</span><br></pre></td></tr></table></figure>
<h3 id="1-2-部门表">1.2 部门表</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 建表语句</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dept(</span><br><span class="line">    deptno <span class="type">INT</span>,   <span class="comment">--部门编号</span></span><br><span class="line">    dname STRING,  <span class="comment">--部门名称</span></span><br><span class="line">    loc STRING    <span class="comment">--部门所在的城市</span></span><br><span class="line">)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line"><span class="comment">--加载数据</span></span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/dept.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> dept;</span><br></pre></td></tr></table></figure>
<h3 id="1-3-分区表">1.3 分区表</h3>
<p>这里需要额外创建一张分区表，主要是为了演示分区查询：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_ptn(</span><br><span class="line">      empno <span class="type">INT</span>,</span><br><span class="line">      ename STRING,</span><br><span class="line">      job STRING,</span><br><span class="line">      mgr <span class="type">INT</span>,</span><br><span class="line">      hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">      sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">      comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line"> PARTITIONED <span class="keyword">BY</span> (deptno <span class="type">INT</span>)   <span class="comment">-- 按照部门编号进行分区</span></span><br><span class="line"> <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--加载数据</span></span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">20</span>)</span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">30</span>)</span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">40</span>)</span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<h2 id="二、单表查询">二、单表查询</h2>
<h3 id="2-1-SELECT">2.1 SELECT</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询表中全部数据</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp;</span><br></pre></td></tr></table></figure>
<h3 id="2-2-WHERE">2.2 WHERE</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询 10 号部门中员工编号大于 7782 的员工信息 </span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> empno <span class="operator">&gt;</span> <span class="number">7782</span> <span class="keyword">AND</span> deptno <span class="operator">=</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-DISTINCT">2.3  DISTINCT</h3>
<p>Hive 支持使用 DISTINCT 关键字去重。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询所有工作类型</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> job <span class="keyword">FROM</span> emp;</span><br></pre></td></tr></table></figure>
<h3 id="2-4-分区查询">2.4 分区查询</h3>
<p>分区查询 (Partition Based Queries)，可以指定某个分区或者分区范围。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询分区表中部门编号在[20,40]之间的员工</span></span><br><span class="line"><span class="keyword">SELECT</span> emp_ptn.<span class="operator">*</span> <span class="keyword">FROM</span> emp_ptn</span><br><span class="line"><span class="keyword">WHERE</span> emp_ptn.deptno <span class="operator">&gt;=</span> <span class="number">20</span> <span class="keyword">AND</span> emp_ptn.deptno <span class="operator">&lt;=</span> <span class="number">40</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-5-LIMIT">2.5 LIMIT</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询薪资最高的 5 名员工</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">ORDER</span> <span class="keyword">BY</span> sal <span class="keyword">DESC</span> LIMIT <span class="number">5</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-6-GROUP-BY">2.6 GROUP BY</h3>
<p>Hive 支持使用 GROUP BY 进行分组聚合操作。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询各个部门薪酬综合</span></span><br><span class="line"><span class="keyword">SELECT</span> deptno,<span class="built_in">SUM</span>(sal) <span class="keyword">FROM</span> emp <span class="keyword">GROUP</span> <span class="keyword">BY</span> deptno;</span><br></pre></td></tr></table></figure>
<p><code>hive.map.aggr</code> 控制程序如何进行聚合。默认值为 false。如果设置为 true，Hive 会在 map 阶段就执行一次聚合。这可以提高聚合效率，但需要消耗更多内存。</p>
<h3 id="2-7-ORDER-AND-SORT">2.7 ORDER AND SORT</h3>
<p>可以使用 ORDER BY 或者 Sort BY 对查询结果进行排序，排序字段可以是整型也可以是字符串：如果是整型，则按照大小排序；如果是字符串，则按照字典序排序。ORDER BY 和 SORT BY 的区别如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用 ORDER BY 时会有一个 Reducer 对全部查询结果进行排序，可以保证数据的全局有序性；</p>
</li>
<li class="lvl-2">
<p>使用 SORT BY 时只会在每个 Reducer 中进行排序，这可以保证每个 Reducer 的输出数据是有序的，但不能保证全局有序。</p>
</li>
</ul>
<p>由于 ORDER BY 的时间可能很长，如果你设置了严格模式 (hive.mapred.mode = strict)，则其后面必须再跟一个 <code>limit</code> 子句。</p>
<blockquote>
<p>注 ：hive.mapred.mode 默认值是 nonstrict ，也就是非严格模式。</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询员工工资，结果按照部门升序，按照工资降序排列</span></span><br><span class="line"><span class="keyword">SELECT</span> empno, deptno, sal <span class="keyword">FROM</span> emp <span class="keyword">ORDER</span> <span class="keyword">BY</span> deptno <span class="keyword">ASC</span>, sal <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-8-HAVING">2.8 HAVING</h3>
<p>可以使用 HAVING 对分组数据进行过滤。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询工资总和大于 9000 的所有部门</span></span><br><span class="line"><span class="keyword">SELECT</span> deptno,<span class="built_in">SUM</span>(sal) <span class="keyword">FROM</span> emp <span class="keyword">GROUP</span> <span class="keyword">BY</span> deptno <span class="keyword">HAVING</span> <span class="built_in">SUM</span>(sal)<span class="operator">&gt;</span><span class="number">9000</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-9-DISTRIBUTE-BY">2.9 DISTRIBUTE BY</h3>
<p>如果想要把具有相同 Key 值的数据分发到同一个 Reducer 进行处理，这可以使用 DISTRIBUTE BY 字句。需要注意的是，DISTRIBUTE BY 虽然能把具有相同 Key 值的数据分发到同一个 Reducer，但是不能保证数据在 Reducer 上是有序的。情况如下：</p>
<p>把以下 5 个数据发送到两个 Reducer 上进行处理：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">k1</span></span><br><span class="line"><span class="attr">k2</span></span><br><span class="line"><span class="attr">k4</span></span><br><span class="line"><span class="attr">k3</span></span><br><span class="line"><span class="attr">k1</span></span><br></pre></td></tr></table></figure>
<p>Reducer1 得到如下乱序数据：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">k1</span></span><br><span class="line"><span class="attr">k2</span></span><br><span class="line"><span class="attr">k1</span></span><br></pre></td></tr></table></figure>
<p>Reducer2 得到数据如下：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">k4</span></span><br><span class="line"><span class="attr">k3</span></span><br></pre></td></tr></table></figure>
<p>如果想让 Reducer 上的数据是有序的，可以结合 <code>SORT BY</code> 使用 (示例如下)，或者使用下面我们将要介绍的 CLUSTER BY。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 将数据按照部门分发到对应的 Reducer 上处理</span></span><br><span class="line"><span class="keyword">SELECT</span> empno, deptno, sal <span class="keyword">FROM</span> emp DISTRIBUTE <span class="keyword">BY</span> deptno SORT <span class="keyword">BY</span> deptno <span class="keyword">ASC</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-10-CLUSTER-BY">2.10 CLUSTER BY</h3>
<p>如果 <code>SORT BY</code> 和 <code>DISTRIBUTE BY</code> 指定的是相同字段，且 SORT BY 排序规则是 ASC，此时可以使用 <code>CLUSTER BY</code> 进行替换，同时 <code>CLUSTER BY</code> 可以保证数据在全局是有序的。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> empno, deptno, sal <span class="keyword">FROM</span> emp CLUSTER  <span class="keyword">BY</span> deptno ;</span><br></pre></td></tr></table></figure>
<h2 id="三、多表联结查询">三、多表联结查询</h2>
<p>Hive 支持内连接，外连接，左外连接，右外连接，笛卡尔连接，这和传统数据库中的概念是一致的，可以参见下图。</p>
<p>需要特别强调：JOIN 语句的关联条件必须用 ON 指定，不能用 WHERE 指定，否则就会先做笛卡尔积，再过滤，这会导致你得不到预期的结果 (下面的演示会有说明)。</p>
<div align="center"> <img width="600px"  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/sql-join.jpg"/> </div>
<h3 id="3-1-INNER-JOIN">3.1 INNER JOIN</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询员工编号为 7369 的员工的详细信息</span></span><br><span class="line"><span class="keyword">SELECT</span> e.<span class="operator">*</span>,d.<span class="operator">*</span> <span class="keyword">FROM</span> </span><br><span class="line">emp e <span class="keyword">JOIN</span> dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno </span><br><span class="line"><span class="keyword">WHERE</span> empno<span class="operator">=</span><span class="number">7369</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--如果是三表或者更多表连接，语法如下</span></span><br><span class="line"><span class="keyword">SELECT</span> a.val, b.val, c.val <span class="keyword">FROM</span> a <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.key <span class="operator">=</span> b.key1) <span class="keyword">JOIN</span> c <span class="keyword">ON</span> (c.key <span class="operator">=</span> b.key1)</span><br></pre></td></tr></table></figure>
<h3 id="3-2-LEFT-OUTER-JOIN">3.2 LEFT OUTER  JOIN</h3>
<p>LEFT OUTER  JOIN 和 LEFT  JOIN 是等价的。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 左连接</span></span><br><span class="line"><span class="keyword">SELECT</span> e.<span class="operator">*</span>,d.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">LEFT</span> <span class="keyword">OUTER</span>  <span class="keyword">JOIN</span>  dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-RIGHT-OUTER-JOIN">3.3 RIGHT OUTER  JOIN</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--右连接</span></span><br><span class="line"><span class="keyword">SELECT</span> e.<span class="operator">*</span>,d.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">RIGHT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span>  dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>
<p>执行右连接后，由于 40 号部门下没有任何员工，所以此时员工信息为 NULL。这个查询可以很好的复述上面提到的——JOIN 语句的关联条件必须用 ON 指定，不能用 WHERE 指定。你可以把 ON 改成 WHERE，你会发现无论如何都查不出 40 号部门这条数据，因为笛卡尔运算不会有 (NULL, 40) 这种情况。</p>
<div align="center"> <img width="700px"   src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-right-join.png"/> </div>
### 3.4 FULL OUTER  JOIN 
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.<span class="operator">*</span>,d.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span>  dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>
<h3 id="3-5-LEFT-SEMI-JOIN">3.5 LEFT SEMI JOIN</h3>
<p>LEFT SEMI JOIN （左半连接）是 IN/EXISTS 子查询的一种更高效的实现。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>JOIN 子句中右边的表只能在 ON 子句中设置过滤条件;</p>
</li>
<li class="lvl-2">
<p>查询结果只包含左边表的数据，所以只能 SELECT 左表中的列。</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询在纽约办公的所有员工信息</span></span><br><span class="line"><span class="keyword">SELECT</span> emp.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> emp <span class="keyword">LEFT</span> SEMI <span class="keyword">JOIN</span> dept </span><br><span class="line"><span class="keyword">ON</span> emp.deptno <span class="operator">=</span> dept.deptno <span class="keyword">AND</span> dept.loc<span class="operator">=</span>&quot;NEW YORK&quot;;</span><br><span class="line"></span><br><span class="line"><span class="comment">--上面的语句就等价于</span></span><br><span class="line"><span class="keyword">SELECT</span> emp.<span class="operator">*</span> <span class="keyword">FROM</span> emp</span><br><span class="line"><span class="keyword">WHERE</span> emp.deptno <span class="keyword">IN</span> (<span class="keyword">SELECT</span> deptno <span class="keyword">FROM</span> dept <span class="keyword">WHERE</span> loc<span class="operator">=</span>&quot;NEW YORK&quot;);</span><br></pre></td></tr></table></figure>
<h3 id="3-6-JOIN">3.6 JOIN</h3>
<p>笛卡尔积连接，这个连接日常的开发中可能很少遇到，且性能消耗比较大，基于这个原因，如果在严格模式下 (hive.mapred.mode = strict)，Hive 会阻止用户执行此操作。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">JOIN</span> dept;</span><br></pre></td></tr></table></figure>
<h2 id="四、JOIN优化">四、JOIN优化</h2>
<h3 id="4-1-STREAMTABLE">4.1 STREAMTABLE</h3>
<p>在多表进行联结的时候，如果每个 ON 字句都使用到共同的列（如下面的 <code>b.key</code>），此时 Hive 会进行优化，将多表 JOIN 在同一个 map / reduce 作业上进行。同时假定查询的最后一个表（如下面的 c 表）是最大的一个表，在对每行记录进行 JOIN 操作时，它将尝试将其他的表缓存起来，然后扫描最后那个表进行计算。因此用户需要保证查询的表的大小从左到右是依次增加的。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">`<span class="keyword">SELECT</span> a.val, b.val, c.val <span class="keyword">FROM</span> a <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.key <span class="operator">=</span> b.key) <span class="keyword">JOIN</span> c <span class="keyword">ON</span> (c.key <span class="operator">=</span> b.key)`</span><br></pre></td></tr></table></figure>
<p>然后，用户并非需要总是把最大的表放在查询语句的最后面，Hive 提供了 <code>/*+ STREAMTABLE() */</code> 标志，用于标识最大的表，示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ STREAMTABLE(d) */</span>  e.<span class="operator">*</span>,d.<span class="operator">*</span> </span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">JOIN</span> dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno</span><br><span class="line"><span class="keyword">WHERE</span> job<span class="operator">=</span><span class="string">&#x27;CLERK&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h3 id="4-2-MAPJOIN">4.2 MAPJOIN</h3>
<p>如果所有表中只有一张表是小表，那么 Hive 把这张小表加载到内存中。这时候程序会在 map 阶段直接拿另外一个表的数据和内存中表数据做匹配，由于在 map 就进行了 JOIN 操作，从而可以省略 reduce 过程，这样效率可以提升很多。Hive 中提供了 <code>/*+ MAPJOIN() */</code> 来标记小表，示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ MAPJOIN(d) */</span> e.<span class="operator">*</span>,d.<span class="operator">*</span> </span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">JOIN</span> dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno</span><br><span class="line"><span class="keyword">WHERE</span> job<span class="operator">=</span><span class="string">&#x27;CLERK&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h2 id="五、SELECT的其他用途">五、SELECT的其他用途</h2>
<p>查看当前数据库：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> current_database()</span><br></pre></td></tr></table></figure>
<h2 id="六、本地模式">六、本地模式</h2>
<p>在上面演示的语句中，大多数都会触发 MapReduce, 少部分不会触发，比如 <code>select * from emp limit 5</code> 就不会触发 MR，此时 Hive 只是简单的读取数据文件中的内容，然后格式化后进行输出。在需要执行 MapReduce 的查询中，你会发现执行时间可能会很长，这时候你可以选择开启本地模式。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--本地模式默认关闭，需要手动开启此功能</span></span><br><span class="line"><span class="keyword">SET</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>启用后，Hive 将分析查询中每个 map-reduce 作业的大小，如果满足以下条件，则可以在本地运行它：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>作业的总输入大小低于：hive.exec.mode.local.auto.inputbytes.max（默认为 128MB）；</p>
</li>
<li class="lvl-2">
<p>map-tasks 的总数小于：hive.exec.mode.local.auto.tasks.max（默认为 4）；</p>
</li>
<li class="lvl-2">
<p>所需的 reduce 任务总数为 1 或 0。</p>
</li>
</ul>
<p>因为我们测试的数据集很小，所以你再次去执行上面涉及 MR 操作的查询，你会发现速度会有显著的提升。</p>
<h2 id="参考资料-19">参考资料</h2>
<ol>
<li class="lvl-3">
<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">LanguageManual Select</a></p>
</li>
<li class="lvl-3">
<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins">LanguageManual Joins</a></p>
</li>
<li class="lvl-3">
<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+GroupBy">LanguageManual GroupBy</a></p>
</li>
<li class="lvl-3">
<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SortBy">LanguageManual SortBy</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive条件与日期函数汇总</title>
    <url>/2021/10/25/Hive%E6%9D%A1%E4%BB%B6%E4%B8%8E%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<h2 id="条件函数">条件函数</h2>
<h3 id="assert-true-BOOLEAN-condition">assert_true(BOOLEAN condition)</h3>
<ul class="lvl-0">
<li class="lvl-2">解释</li>
</ul>
<p>如果condition不为true，则抛出异常，否则返回null</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> assert_true(<span class="number">1</span><span class="operator">&lt;</span><span class="number">2</span>) <span class="comment">-- 返回null</span></span><br><span class="line"><span class="keyword">select</span> assert_true(<span class="number">1</span><span class="operator">&gt;</span><span class="number">2</span>) <span class="comment">-- 抛出异常</span></span><br></pre></td></tr></table></figure>
<h3 id="coalesce-T-v1-T-v2-…">coalesce(T v1, T v2, …)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回第一个不为null的值，如果都为null，则返回null</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">coalesce</span>(<span class="keyword">null</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="keyword">null</span>)  <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">coalesce</span>(<span class="number">1</span>,<span class="keyword">null</span>) <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">coalesce</span>(<span class="keyword">null</span>,<span class="keyword">null</span>) <span class="comment">-- 返回null</span></span><br></pre></td></tr></table></figure>
<h3 id="if-BOOLEAN-testCondition-valueTrue-valueFalseOrNull">if(BOOLEAN testCondition,valueTrue, valueFalseOrNull)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>如果testCondition条件为true，则返回第一个值，否则返回第二个值</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> if(<span class="number">1</span> <span class="keyword">is</span> <span class="keyword">null</span>,<span class="number">0</span>,<span class="number">1</span>)  <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> if(<span class="keyword">null</span> <span class="keyword">is</span> <span class="keyword">null</span>,<span class="number">0</span>,<span class="number">1</span>) <span class="comment">-- 返回0</span></span><br></pre></td></tr></table></figure>
<h3 id="isnotnull-a">isnotnull(a)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>如果参数a不为null，则返回true，否则返回false</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> isnotnull(<span class="number">1</span>) <span class="comment">-- 返回true</span></span><br><span class="line"><span class="keyword">select</span> isnotnull(<span class="keyword">null</span>) <span class="comment">-- 返回false</span></span><br></pre></td></tr></table></figure>
<h3 id="isnull-a">isnull(a)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>与isnotnull相反，如果参数a为null，则返回true，否则返回false</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> isnull(<span class="keyword">null</span>) <span class="comment">-- 返回true</span></span><br><span class="line"><span class="keyword">select</span> isnull(<span class="number">1</span>) <span class="comment">-- 返回false</span></span><br></pre></td></tr></table></figure>
<h3 id="nullif-a-b">nullif(a, b)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>如果参数a=b，返回null，否则返回a值(Hive2.2.0版本)</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">nullif</span>(<span class="number">1</span>,<span class="number">2</span>) <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">nullif</span>(<span class="number">1</span>,<span class="number">1</span>) <span class="comment">-- 返回null</span></span><br></pre></td></tr></table></figure>
<h3 id="nvl-T-value-T-default-value">nvl(T value, T default_value)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>如果value的值为null，则返回default_value默认值，否则返回value的值。在null值判断时，可以使用if函数给定默认值，也可以使用此函数给定默认值，使用该函数sql特别简洁。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> nvl(<span class="number">1</span>,<span class="number">0</span>) <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> nvl(<span class="keyword">null</span>,<span class="number">0</span>) <span class="comment">-- 返回0</span></span><br></pre></td></tr></table></figure>
<h2 id="日期函数">日期函数</h2>
<h3 id="add-months-DATE-STRING-TIMESTAMP-start-date-INT-num-months">add_months(DATE|STRING|TIMESTAMP start_date, INT num_months)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>  start_date参数可以是string, date 或者timestamp类型，num_months参数时int类型。返回一个日期，该日期是在start_date基础之上加上num_months个月，即start_date之后null_months个月的一个日期。如果start_date的时间部分的数据会被忽略。注意：如果start_date所在月份的天数大于结果日期月的天数，则返回结果月的最后一天的日期。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> add_months(&quot;2020-05-20&quot;,<span class="number">2</span>); <span class="comment">-- 返回2020-07-20</span></span><br><span class="line"><span class="keyword">select</span> add_months(&quot;2020-05-20&quot;,<span class="number">8</span>); <span class="comment">-- 返回2021-01-20</span></span><br><span class="line"><span class="keyword">select</span> add_months(&quot;2020-05-31&quot;,<span class="number">1</span>); <span class="comment">-- 返回2020-06-30,5月有31天，6月只有30天，所以返回下一个月的最后一天</span></span><br></pre></td></tr></table></figure>
<h3 id="current-date">current_date</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回查询时刻的当前日期</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">current_date</span>() <span class="comment">-- 返回当前查询日期2020-05-20</span></span><br></pre></td></tr></table></figure>
<h3 id="current-timestamp">current_timestamp()</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回查询时刻的当前时间</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">current_timestamp</span>() <span class="comment">-- 2020-05-20 14:40:47.273</span></span><br></pre></td></tr></table></figure>
<h3 id="datediff-STRING-enddate-STRING-startdate">datediff(STRING enddate, STRING startdate)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回开始日期startdate与结束日期enddate之前相差的天数</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> datediff(&quot;2020-05-20&quot;,&quot;2020-05-21&quot;); <span class="comment">-- 返回-1</span></span><br><span class="line"><span class="keyword">select</span> datediff(&quot;2020-05-21&quot;,&quot;2020-05-20&quot;); <span class="comment">-- 返回1</span></span><br></pre></td></tr></table></figure>
<h3 id="date-add-DATE-startdate-INT-days">date_add(DATE startdate, INT days)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>在startdate基础上加上几天，然后返回加上几天之后的一个日期</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> date_add(&quot;2020-05-20&quot;,<span class="number">1</span>); <span class="comment">-- 返回2020-05-21,1表示加1天</span></span><br><span class="line"><span class="keyword">select</span> date_add(&quot;2020-05-20&quot;,<span class="number">-1</span>); <span class="comment">-- 返回2020-05-19，-1表示减一天</span></span><br></pre></td></tr></table></figure>
<h3 id="date-sub-DATE-startdate-INT-days">date_sub(DATE startdate, INT days)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>在startdate基础上减去几天，然后返回减去几天之后的一个日期,功能与date_add很类似</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> date_sub(&quot;2020-05-20&quot;,<span class="number">1</span>); <span class="comment">-- 返回2020-05-19,1表示减1天</span></span><br><span class="line"><span class="keyword">select</span> date_sub(&quot;2020-05-20&quot;,<span class="number">-1</span>); <span class="comment">-- 返回2020-05-21，-1表示加1天</span></span><br></pre></td></tr></table></figure>
<h3 id="date-format-DATE-TIMESTAMP-STRING-ts-STRING-fmt">date_format(DATE|TIMESTAMP|STRING ts, STRING fmt)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>将date/timestamp/string类型的值转换为一个具体格式化的字符串。支持java的SimpleDateFormat格式，第二个参数fmt必须是一个常量</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> date_format(<span class="string">&#x27;2020-05-20&#x27;</span>, <span class="string">&#x27;yyyy&#x27;</span>); <span class="comment">-- 返回2020</span></span><br><span class="line"><span class="keyword">select</span> date_format(<span class="string">&#x27;2020-05-20&#x27;</span>, <span class="string">&#x27;MM&#x27;</span>); <span class="comment">-- 返回05</span></span><br><span class="line"><span class="keyword">select</span> date_format(<span class="string">&#x27;2020-05-20&#x27;</span>, <span class="string">&#x27;dd&#x27;</span>); <span class="comment">-- 返回20</span></span><br><span class="line"><span class="comment">-- 返回2020年05月20日 00时00分00秒</span></span><br><span class="line"><span class="keyword">select</span> date_format(<span class="string">&#x27;2020-05-20&#x27;</span>, <span class="string">&#x27;yyyy年MM月dd日 HH时mm分ss秒&#x27;</span>) ;</span><br><span class="line"><span class="keyword">select</span> date_format(<span class="string">&#x27;2020-05-20&#x27;</span>, <span class="string">&#x27;yy/MM/dd&#x27;</span>) <span class="comment">-- 返回 20/05/20</span></span><br></pre></td></tr></table></figure>
<h3 id="dayofmonth-STRING-date">dayofmonth(STRING date)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回一个日期或时间的天,与day()函数功能相同</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> dayofmonth(<span class="string">&#x27;2020-05-20&#x27;</span>) <span class="comment">-- 返回20</span></span><br></pre></td></tr></table></figure>
<h3 id="extract-field-FROM-source">extract(field FROM source)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>  提取 day, dayofweek, hour, minute, month, quarter, second, week 或者year的值，field可以选择day, dayofweek, hour, minute, month, quarter, second, week 或者year，source必须是一个date、timestamp或者可以转为 date 、timestamp的字符串。注意：Hive 2.2.0版本之后支持该函数</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(<span class="keyword">year</span> <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回2020，年</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(quarter <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回2，季度</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(<span class="keyword">month</span> <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回05，月份</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(week <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回21，同weekofyear，一年中的第几周</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(dayofweek <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回4,代表星期三</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(<span class="keyword">day</span> <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回20，天</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(<span class="keyword">hour</span> <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回15，小时</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(<span class="keyword">minute</span> <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回21，分钟</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(<span class="keyword">second</span> <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回34，秒</span></span><br></pre></td></tr></table></figure>
<h4 id="year-STRING-date">year(STRING date)</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回时间的年份,可以用extract函数替代</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">year</span>(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>); <span class="comment">-- 返回2020</span></span><br></pre></td></tr></table></figure>
<h4 id="quarter-DATE-TIMESTAMP-STRING-a">quarter(DATE|TIMESTAMP|STRING a)</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回给定时间或日期的季度，1至4个季度,可以用extract函数替代</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> quarter(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>); <span class="comment">-- 返回2，第2季度</span></span><br></pre></td></tr></table></figure>
<h4 id="month-STRING-date">month(STRING date)</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回时间的月份,可以用extract函数替代</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">month</span>(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>) <span class="comment">-- 返回5</span></span><br></pre></td></tr></table></figure>
<h4 id="day-STRING-date">day(STRING date)</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回一个日期或者时间的天,可以用extract函数替代</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">day</span>(&quot;2020-05-20&quot;); <span class="comment">-- 返回20</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">day</span>(&quot;2020-05-20 15:05:27.5&quot;); <span class="comment">-- 返回20</span></span><br></pre></td></tr></table></figure>
<h4 id="hour-STRING-date">hour(STRING date)</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回一个时间的小时,可以用extract函数替代</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">hour</span>(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>);<span class="comment">-- 返回15</span></span><br></pre></td></tr></table></figure>
<h4 id="minute-STRING-date">minute(STRING date)</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回一个时间的分钟值,可以用extract函数替代</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">minute</span>(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>); <span class="comment">-- 返回21</span></span><br></pre></td></tr></table></figure>
<h4 id="second-STRING-date">second(STRING date)</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回一个时间的秒,可以用extract函数替代</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">second</span>(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>); <span class="comment">--返回34</span></span><br></pre></td></tr></table></figure>
<h3 id="from-unixtime-BIGINT-unixtime-STRING-format">from_unixtime(BIGINT unixtime [, STRING format])</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>将Unix时间戳转换为字符串格式的时间(比如yyyy-MM-dd HH:mm:ss格式)</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1589960708</span>); <span class="comment">-- 返回2020-05-20 15:45:08</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1589960708</span>, <span class="string">&#x27;yyyy-MM-dd hh:mm:ss&#x27;</span>); <span class="comment">-- -- 返回2020-05-20 15:45:08</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1589960708</span>, <span class="string">&#x27;yyyy-MM-dd&#x27;</span>); <span class="comment">-- 返回2020-05-20</span></span><br></pre></td></tr></table></figure>
<h3 id="from-utc-timestamp-T-a-STRING-timezone">from_utc_timestamp(T a, STRING timezone)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>转换为特定时区的时间</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>,<span class="string">&#x27;PST&#x27;</span>); <span class="comment">-- 返回2020-05-20 08:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>,<span class="string">&#x27;GMT&#x27;</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>,<span class="string">&#x27;UTC&#x27;</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>,<span class="string">&#x27;DST&#x27;</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>,<span class="string">&#x27;CST&#x27;</span>); <span class="comment">-- 返回2020-05-20 10:21:34.0</span></span><br></pre></td></tr></table></figure>
<h3 id="last-day-STRING-date">last_day(STRING date)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回给定时间或日期所在月的最后一天，参数可以是’yyyy-MM-dd HH:mm:ss’ 或者 ‘yyyy-MM-dd’类型，时间部分会被忽略</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> last_day(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>); <span class="comment">-- 返回2020-05-31</span></span><br><span class="line"><span class="keyword">select</span> last_day(<span class="string">&#x27;2020-05-20&#x27;</span>); <span class="comment">-- 返回2020-05-31</span></span><br></pre></td></tr></table></figure>
<h3 id="to-date-STRING-timestamp">to_date(STRING timestamp)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回一个字符串时间的日期部分，去掉时间部分，2.1.0之前版本返回的是string，2.1.0版本及之后返回的是date</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> to_date(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>); <span class="comment">-- 返回2020-05-20</span></span><br><span class="line"><span class="keyword">select</span> to_date(<span class="string">&#x27;2020-05-20&#x27;</span>); <span class="comment">-- 返回2020-05-20</span></span><br></pre></td></tr></table></figure>
<h3 id="to-utc-timestamp-T-a-STRING-timezone">to_utc_timestamp(T a, STRING timezone)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>转换为世界标准时间UTC的时间戳,与from_utc_timestamp类似</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> to_utc_timestamp(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>, <span class="string">&#x27;GMT&#x27;</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br></pre></td></tr></table></figure>
<h3 id="trunc-STRING-date-STRING-format">trunc(STRING date, STRING format)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>截断日期到指定的日期精度，仅支持月（MONTH/MON/MM）或者年（YEAR/YYYY/YY）</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> trunc(<span class="string">&#x27;2020-05-20&#x27;</span>, <span class="string">&#x27;YY&#x27;</span>);   <span class="comment">-- 返回2020-01-01，返回年的1月1日</span></span><br><span class="line"><span class="keyword">select</span> trunc(<span class="string">&#x27;2020-05-20&#x27;</span>, <span class="string">&#x27;MM&#x27;</span>);   <span class="comment">-- 返回2020-05-01，返回月的第一天</span></span><br><span class="line"><span class="keyword">select</span> trunc(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>, <span class="string">&#x27;MM&#x27;</span>);   <span class="comment">-- 返回2020-05-01</span></span><br></pre></td></tr></table></figure>
<h3 id="unix-timestamp-STRING-date-STRING-pattern">unix_timestamp([STRING date [, STRING pattern]])</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>参数时可选的，当参数为空时，返回当前Unix是时间戳，精确到秒。可以指定一个具体的日期，转换为Unix时间戳格式</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 返回1589959294</span></span><br><span class="line"><span class="keyword">select</span> unix_timestamp(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>,<span class="string">&#x27;yyyy-MM-dd hh:mm:ss&#x27;</span>);</span><br><span class="line"><span class="comment">-- 返回1589904000</span></span><br><span class="line"><span class="keyword">select</span> unix_timestamp(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;yyyy-MM-dd&#x27;</span>);</span><br></pre></td></tr></table></figure>
<h3 id="weekofyear-STRING-date">weekofyear(STRING date)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回一个日期或时间在一年中的第几周，可以用extract替代</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> weekofyear(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>); <span class="comment">-- 返回21，第21周</span></span><br><span class="line"><span class="keyword">select</span> weekofyear(<span class="string">&#x27;2020-05-20&#x27;</span>); <span class="comment">-- 返回21，第21周</span></span><br></pre></td></tr></table></figure>
<h3 id="next-day-STRING-start-date-STRING-day-of-week">next_day(STRING start_date, STRING day_of_week)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>参数start_date可以是一个时间或日期，day_of_week表示星期几，比如Mo表示星期一，Tu表示星期二，Wed表示星期三，Thur表示星期四，Fri表示星期五，Sat表示星期六，Sun表示星期日。如果指定的星期几在该日期所在的周且在该日期之后，则返回当周的星期几日期，如果指定的星期几不在该日期所在的周，则返回下一个星期几对应的日期</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;Mon&#x27;</span>);<span class="comment">-- 返回当前日期的下一个周一日期:2020-05-25</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;Tu&#x27;</span>);<span class="comment">-- 返回当前日期的下一个周二日期:2020-05-26</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;Wed&#x27;</span>);<span class="comment">-- 返回当前日期的下一个周三日期:2020-05-27</span></span><br><span class="line"><span class="comment">-- 2020-05-20为周三，指定的参数为周四，所以返回当周的周四就是2020-05-21</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;Th&#x27;</span>);</span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;Fri&#x27;</span>);<span class="comment">-- 返回周五日期2020-05-22</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;Sat&#x27;</span>); <span class="comment">-- 返回周六日期2020-05-23</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;Sun&#x27;</span>); <span class="comment">-- 返回周六日期2020-05-24</span></span><br></pre></td></tr></table></figure>
<p>该函数比较重要：比如取当前日期所在的周一和周日，通过长用在按周进行汇总数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> date_add(next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;MO&#x27;</span>),<span class="number">-7</span>); <span class="comment">-- 返回当前日期的周一日期2020-05-18</span></span><br><span class="line"><span class="keyword">select</span> date_add(next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;MO&#x27;</span>),<span class="number">-1</span>); <span class="comment">-- 返回当前日期的周日日期2020-05-24</span></span><br></pre></td></tr></table></figure>
<h3 id="months-between-DATE-TIMESTAMP-STRING-date1-…-date2">months_between(DATE|TIMESTAMP|STRING date1, … date2)</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>解释</p>
</li>
</ul>
<p>返回 date1 和 date2 的月份差。如果date1大于date2，返回正值，否则返回负值，如果是相减是整数月，则返回一个整数，否则会返回小数</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使用案例</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> months_between(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;2020-05-20&#x27;</span>); <span class="comment">-- 返回0</span></span><br><span class="line"><span class="keyword">select</span> months_between(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;2020-06-20&#x27;</span>); <span class="comment">-- 返回-1</span></span><br><span class="line"><span class="comment">-- 相差的整数月</span></span><br><span class="line"><span class="keyword">select</span> months_between(<span class="string">&#x27;2020-06-30&#x27;</span>,<span class="string">&#x27;2020-05-31&#x27;</span>); <span class="comment">-- 返回1</span></span><br><span class="line"><span class="comment">-- 非整数月，一个月差一天</span></span><br><span class="line"><span class="keyword">select</span> months_between(<span class="string">&#x27;2020-06-29&#x27;</span>,<span class="string">&#x27;2020-05-31&#x27;</span>); <span class="comment">-- 返回0.93548387</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka消费者详解</title>
    <url>/2022/03/19/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="一、消费者和消费者群组">一、消费者和消费者群组</h2>
<p>在 Kafka 中，消费者通常是消费者群组的一部分，多个消费者群组共同读取同一个主题时，彼此之间互不影响。Kafka 之所以要引入消费者群组这个概念是因为 Kafka 消费者经常会做一些高延迟的操作，比如把数据写到数据库或 HDFS ，或者进行耗时的计算，在这些情况下，单个消费者无法跟上数据生成的速度。此时可以增加更多的消费者，让它们分担负载，分别处理部分分区的消息，这就是 Kafka 实现横向伸缩的主要手段。</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-consumer01.png"/> </div>
<p>需要注意的是：同一个分区只能被同一个消费者群组里面的一个消费者读取，不可能存在同一个分区被同一个消费者群里多个消费者共同读取的情况，如图：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-consumer02.png"/> </div>
<p>可以看到即便消费者 Consumer5 空闲了，但是也不会去读取任何一个分区的数据，这同时也提醒我们在使用时应该合理设置消费者的数量，以免造成闲置和额外开销。</p>
<h2 id="二、分区再均衡">二、分区再均衡</h2>
<p>因为群组里的消费者共同读取主题的分区，所以当一个消费者被关闭或发生崩溃时，它就离开了群组，原本由它读取的分区将由群组里的其他消费者来读取。同时在主题发生变化时 ， 比如添加了新的分区，也会发生分区与消费者的重新分配，分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为再均衡。正是因为再均衡，所以消费费者群组才能保证高可用性和伸缩性。</p>
<p>消费者通过向群组协调器所在的 broker 发送心跳来维持它们和群组的从属关系以及它们对分区的所有权。只要消费者以正常的时间间隔发送心跳，就被认为是活跃的，说明它还在读取分区里的消息。消费者会在轮询消息或提交偏移量时发送心跳。如果消费者停止发送心跳的时间足够长，会话就会过期，群组协调器认为它已经死亡，就会触发再均衡。</p>
<h2 id="三、创建Kafka消费者">三、创建Kafka消费者</h2>
<p>在创建消费者的时候以下以下三个选项是必选的：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>bootstrap.servers</strong> ：指定 broker 的地址清单，清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找 broker 的信息。不过建议至少要提供两个 broker 的信息作为容错；</p>
</li>
<li class="lvl-2">
<p><strong>key.deserializer</strong> ：指定键的反序列化器；</p>
</li>
<li class="lvl-2">
<p><strong>value.deserializer</strong> ：指定值的反序列化器。</p>
</li>
</ul>
<p>除此之外你还需要指明你需要想订阅的主题，可以使用如下两个 API :</p>
<ul class="lvl-0">
<li class="lvl-3">
<p><strong>consumer.subscribe(Collection&lt;String&gt; topics)</strong>  ：指明需要订阅的主题的集合；</p>
</li>
<li class="lvl-2">
<p><strong>consumer.subscribe(Pattern pattern)</strong>  ：使用正则来匹配需要订阅的集合。</p>
</li>
</ul>
<p>最后只需要通过轮询 API(<code>poll</code>) 向服务器定时请求数据。一旦消费者订阅了主题，轮询就会处理所有的细节，包括群组协调、分区再均衡、发送心跳和获取数据，这使得开发者只需要关注从分区返回的数据，然后进行业务处理。 示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">String</span> topic = <span class="string">&quot;Hello-Kafka&quot;</span>;</span><br><span class="line"><span class="type">String</span> group = <span class="string">&quot;group1&quot;</span>;</span><br><span class="line"><span class="type">Properties</span> props = <span class="keyword">new</span> <span class="type">Properties</span>();</span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop001:9092&quot;</span>);</span><br><span class="line"><span class="comment">/*指定分组 ID*/</span></span><br><span class="line">props.put(<span class="string">&quot;group.id&quot;</span>, group);</span><br><span class="line">props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"><span class="type">KafkaConsumer</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; consumer = <span class="keyword">new</span> <span class="type">KafkaConsumer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line"><span class="comment">/*订阅主题 (s)*/</span></span><br><span class="line">consumer.subscribe(<span class="type">Collections</span>.singletonList(topic));</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="comment">/*轮询获取数据*/</span></span><br><span class="line">        <span class="type">ConsumerRecords</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; records = consumer.poll(<span class="type">Duration</span>.of(<span class="number">100</span>, <span class="type">ChronoUnit</span>.<span class="type">MILLIS</span>));</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">ConsumerRecord</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; record : records) &#123;</span><br><span class="line">            <span class="type">System</span>.out.printf(<span class="string">&quot;topic = %s,partition = %d, key = %s, value = %s, offset = %d,\n&quot;</span>,</span><br><span class="line">           record.topic(), record.partition(), record.key(), record.value(), record.offset());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>本篇文章的所有示例代码可以从 Github 上进行下载：<a href="https://github.com/oicio/BigData-Notes/tree/master/code/Kafka/kafka-basis">kafka-basis</a></p>
</blockquote>
<h2 id="三、-自动提交偏移量">三、 自动提交偏移量</h2>
<h3 id="3-1-偏移量的重要性">3.1 偏移量的重要性</h3>
<p>Kafka 的每一条消息都有一个偏移量属性，记录了其在分区中的位置，偏移量是一个单调递增的整数。消费者通过往一个叫作 <code>＿consumer_offset</code> 的特殊主题发送消息，消息里包含每个分区的偏移量。 如果消费者一直处于运行状态，那么偏移量就没有什么用处。不过，如果有消费者退出或者新分区加入，此时就会触发再均衡。完成再均衡之后，每个消费者可能分配到新的分区，而不是之前处理的那个。为了能够继续之前的工作，消费者需要读取每个分区最后一次提交的偏移量，然后从偏移量指定的地方继续处理。 因为这个原因，所以如果不能正确提交偏移量，就可能会导致数据丢失或者重复出现消费，比如下面情况：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>如果提交的偏移量小于客户端处理的最后一个消息的偏移量 ，那么处于两个偏移量之间的消息就会被重复消费；</p>
</li>
<li class="lvl-2">
<p>如果提交的偏移量大于客户端处理的最后一个消息的偏移量，那么处于两个偏移量之间的消息将会丢失。</p>
</li>
</ul>
<h3 id="3-2-自动提交偏移量">3.2 自动提交偏移量</h3>
<p>Kafka 支持自动提交和手动提交偏移量两种方式。这里先介绍比较简单的自动提交：</p>
<p>只需要将消费者的 <code>enable.auto.commit</code> 属性配置为 <code>true</code> 即可完成自动提交的配置。 此时每隔固定的时间，消费者就会把 <code>poll()</code> 方法接收到的最大偏移量进行提交，提交间隔由 <code>auto.commit.interval.ms</code> 属性进行配置，默认值是 5s。</p>
<p>使用自动提交是存在隐患的，假设我们使用默认的 5s 提交时间间隔，在最近一次提交之后的 3s 发生了再均衡，再均衡之后，消费者从最后一次提交的偏移量位置开始读取消息。这个时候偏移量已经落后了 3s ，所以在这 3s 内到达的消息会被重复处理。可以通过修改提交时间间隔来更频繁地提交偏移量，减小可能出现重复消息的时间窗，不过这种情况是无法完全避免的。基于这个原因，Kafka 也提供了手动提交偏移量的 API，使得用户可以更为灵活的提交偏移量。</p>
<h2 id="四、手动提交偏移量">四、手动提交偏移量</h2>
<p>用户可以通过将 <code>enable.auto.commit</code> 设为 <code>false</code>，然后手动提交偏移量。基于用户需求手动提交偏移量可以分为两大类：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>手动提交当前偏移量：即手动提交当前轮询的最大偏移量；</p>
</li>
<li class="lvl-2">
<p>手动提交固定偏移量：即按照业务需求，提交某一个固定的偏移量。</p>
</li>
</ul>
<p>而按照 Kafka API，手动提交偏移量又可以分为同步提交和异步提交。</p>
<h3 id="4-1-同步提交">4.1 同步提交</h3>
<p>通过调用 <code>consumer.commitSync()</code> 来进行同步提交，不传递任何参数时提交的是当前轮询的最大偏移量。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(<span class="number">100</span>, ChronoUnit.MILLIS));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        System.out.println(record);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*同步提交*/</span></span><br><span class="line">    consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果某个提交失败，同步提交还会进行重试，这可以保证数据能够最大限度提交成功，但是同时也会降低程序的吞吐量。基于这个原因，Kafka 还提供了异步提交的 API。</p>
<h3 id="4-2-异步提交">4.2 异步提交</h3>
<p>异步提交可以提高程序的吞吐量，因为此时你可以尽管请求数据，而不用等待 Broker 的响应。代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(<span class="number">100</span>, ChronoUnit.MILLIS));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        System.out.println(record);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*异步提交并定义回调*/</span></span><br><span class="line">    consumer.commitAsync(<span class="keyword">new</span> <span class="title class_">OffsetCommitCallback</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception)</span> &#123;</span><br><span class="line">          <span class="keyword">if</span> (exception != <span class="literal">null</span>) &#123;</span><br><span class="line">             System.out.println(<span class="string">&quot;错误处理&quot;</span>);</span><br><span class="line">             offsets.forEach((x, y) -&gt; System.out.printf(<span class="string">&quot;topic = %s,partition = %d, offset = %s \n&quot;</span>,</span><br><span class="line">                                                            x.topic(), x.partition(), y.offset()));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>异步提交存在的问题是，在提交失败的时候不会进行自动重试，实际上也不能进行自动重试。假设程序同时提交了 200 和 300 的偏移量，此时 200 的偏移量失败的，但是紧随其后的 300 的偏移量成功了，此时如果重试就会存在 200 覆盖 300 偏移量的可能。同步提交就不存在这个问题，因为在同步提交的情况下，300 的提交请求必须等待服务器返回 200 提交请求的成功反馈后才会发出。基于这个原因，某些情况下，需要同时组合同步和异步两种提交方式。</p>
<blockquote>
<p>注：虽然程序不能在失败时候进行自动重试，但是我们是可以手动进行重试的，你可以通过一个 Map&lt;TopicPartition, Integer&gt; offsets 来维护你提交的每个分区的偏移量，然后当失败时候，你可以判断失败的偏移量是否小于你维护的同主题同分区的最后提交的偏移量，如果小于则代表你已经提交了更大的偏移量请求，此时不需要重试，否则就可以进行手动重试。</p>
</blockquote>
<h3 id="4-3-同步加异步提交">4.3  同步加异步提交</h3>
<p>下面这种情况，在正常的轮询中使用异步提交来保证吞吐量，但是因为在最后即将要关闭消费者了，所以此时需要用同步提交来保证最大限度的提交成功。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="type">ConsumerRecords</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; records = consumer.poll(<span class="type">Duration</span>.of(<span class="number">100</span>, <span class="type">ChronoUnit</span>.<span class="type">MILLIS</span>));</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">ConsumerRecord</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; record : records) &#123;</span><br><span class="line">            <span class="type">System</span>.out.println(record);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 异步提交</span></span><br><span class="line">        consumer.commitAsync();</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> (<span class="type">Exception</span> e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 因为即将要关闭消费者，所以要用同步提交保证提交成功</span></span><br><span class="line">        consumer.commitSync();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        consumer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-4-提交特定偏移量">4.4 提交特定偏移量</h3>
<p>在上面同步和异步提交的 API 中，实际上我们都没有对 commit 方法传递参数，此时默认提交的是当前轮询的最大偏移量，如果你需要提交特定的偏移量，可以调用它们的重载方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*同步提交特定偏移量*/</span></span><br><span class="line">commitSync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets) </span><br><span class="line"><span class="comment">/*异步提交特定偏移量*/</span>    </span><br><span class="line">commitAsync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback)</span><br></pre></td></tr></table></figure>
<p>需要注意的是，因为你可以订阅多个主题，所以 <code>offsets</code> 中必须要包含所有主题的每个分区的偏移量，示例代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(<span class="number">100</span>, ChronoUnit.MILLIS));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">            System.out.println(record);</span><br><span class="line">            <span class="comment">/*记录每个主题的每个分区的偏移量*/</span></span><br><span class="line">            <span class="type">TopicPartition</span> <span class="variable">topicPartition</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(record.topic(), record.partition());</span><br><span class="line">            <span class="type">OffsetAndMetadata</span> <span class="variable">offsetAndMetadata</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(record.offset()+<span class="number">1</span>, <span class="string">&quot;no metaData&quot;</span>);</span><br><span class="line">            <span class="comment">/*TopicPartition 重写过 hashCode 和 equals 方法，所以能够保证同一主题和分区的实例不会被重复添加*/</span></span><br><span class="line">            offsets.put(topicPartition, offsetAndMetadata);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*提交特定偏移量*/</span></span><br><span class="line">        consumer.commitAsync(offsets, <span class="literal">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="五、监听分区再均衡">五、监听分区再均衡</h2>
<p>因为分区再均衡会导致分区与消费者的重新划分，有时候你可能希望在再均衡前执行一些操作：比如提交已经处理但是尚未提交的偏移量，关闭数据库连接等。此时可以在订阅主题时候，调用 <code>subscribe</code> 的重载方法传入自定义的分区再均衡监听器。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">/*订阅指定集合内的所有主题*/</span></span><br><span class="line">subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span><br><span class="line"> <span class="comment">/*使用正则匹配需要订阅的主题*/</span>    </span><br><span class="line">subscribe(Pattern pattern, ConsumerRebalanceListener listener)    </span><br></pre></td></tr></table></figure>
<p>代码示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">consumer.subscribe(Collections.singletonList(topic), <span class="keyword">new</span> <span class="title class_">ConsumerRebalanceListener</span>() &#123;</span><br><span class="line">    <span class="comment">/*该方法会在消费者停止读取消息之后，再均衡开始之前就调用*/</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;再均衡即将触发&quot;</span>);</span><br><span class="line">        <span class="comment">// 提交已经处理的偏移量</span></span><br><span class="line">        consumer.commitSync(offsets);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*该方法会在重新分配分区之后，消费者开始读取消息之前被调用*/</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(<span class="number">100</span>, ChronoUnit.MILLIS));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">            System.out.println(record);</span><br><span class="line">            <span class="type">TopicPartition</span> <span class="variable">topicPartition</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(record.topic(), record.partition());</span><br><span class="line">            <span class="type">OffsetAndMetadata</span> <span class="variable">offsetAndMetadata</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(record.offset() + <span class="number">1</span>, <span class="string">&quot;no metaData&quot;</span>);</span><br><span class="line">            <span class="comment">/*TopicPartition 重写过 hashCode 和 equals 方法，所以能够保证同一主题和分区的实例不会被重复添加*/</span></span><br><span class="line">            offsets.put(topicPartition, offsetAndMetadata);</span><br><span class="line">        &#125;</span><br><span class="line">        consumer.commitAsync(offsets, <span class="literal">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="六-、退出轮询">六 、退出轮询</h2>
<p>Kafka 提供了 <code>consumer.wakeup()</code> 方法用于退出轮询，它通过抛出 <code>WakeupException</code> 异常来跳出循环。需要注意的是，在退出线程时最好显示的调用 <code>consumer.close()</code> , 此时消费者会提交任何还没有提交的东西，并向群组协调器发送消息，告知自己要离开群组，接下来就会触发再均衡 ，而不需要等待会话超时。</p>
<p>下面的示例代码为监听控制台输出，当输入 <code>exit</code> 时结束轮询，关闭消费者并退出程序：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*调用 wakeup 优雅的退出*/</span></span><br><span class="line"><span class="keyword">final</span> <span class="type">Thread</span> <span class="variable">mainThread</span> <span class="operator">=</span> Thread.currentThread();</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt; &#123;</span><br><span class="line">    <span class="type">Scanner</span> <span class="variable">sc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scanner</span>(System.in);</span><br><span class="line">    <span class="keyword">while</span> (sc.hasNext()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">&quot;exit&quot;</span>.equals(sc.next())) &#123;</span><br><span class="line">            consumer.wakeup();</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">/*等待主线程完成提交偏移量、关闭消费者等操作*/</span></span><br><span class="line">                mainThread.join();</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).start();</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(<span class="number">100</span>, ChronoUnit.MILLIS));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; rd : records) &#123;</span><br><span class="line">            System.out.printf(<span class="string">&quot;topic = %s,partition = %d, key = %s, value = %s, offset = %d,\n&quot;</span>,</span><br><span class="line">                              rd.topic(), rd.partition(), rd.key(), rd.value(), rd.offset());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> (WakeupException e) &#123;</span><br><span class="line">    <span class="comment">//对于 wakeup() 调用引起的 WakeupException 异常可以不必处理</span></span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">    System.out.println(<span class="string">&quot;consumer 关闭&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="七、独立的消费者">七、独立的消费者</h2>
<p>因为 Kafka 的设计目标是高吞吐和低延迟，所以在 Kafka 中，消费者通常都是从属于某个群组的，这是因为单个消费者的处理能力是有限的。但是某些时候你的需求可能很简单，比如可能只需要一个消费者从一个主题的所有分区或者某个特定的分区读取数据，这个时候就不需要消费者群组和再均衡了， 只需要把主题或者分区分配给消费者，然后开始读取消息井提交偏移量即可。</p>
<p>在这种情况下，就不需要订阅主题， 取而代之的是消费者为自己分配分区。 一个消费者可以订阅主题（井加入消费者群组），或者为自己分配分区，但不能同时做这两件事情。 分配分区的示例代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;TopicPartition&gt; partitions = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">List&lt;PartitionInfo&gt; partitionInfos = consumer.partitionsFor(topic);</span><br><span class="line"></span><br><span class="line"><span class="comment">/*可以指定读取哪些分区 如这里假设只读取主题的 0 分区*/</span></span><br><span class="line"><span class="keyword">for</span> (PartitionInfo partition : partitionInfos) &#123;</span><br><span class="line">    <span class="keyword">if</span> (partition.partition()==<span class="number">0</span>)&#123;</span><br><span class="line">        partitions.add(<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(partition.topic(), partition.partition()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 为消费者指定分区</span></span><br><span class="line">consumer.assign(partitions);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;Integer, String&gt; records = consumer.poll(Duration.of(<span class="number">100</span>, ChronoUnit.MILLIS));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;Integer, String&gt; record : records) &#123;</span><br><span class="line">        System.out.printf(<span class="string">&quot;partition = %s, key = %d, value = %s\n&quot;</span>,</span><br><span class="line">                          record.partition(), record.key(), record.value());</span><br><span class="line">    &#125;</span><br><span class="line">    consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="附录-Kafka消费者可选属性">附录 : Kafka消费者可选属性</h2>
<h3 id="1-fetch-min-byte">1. fetch.min.byte</h3>
<p>消费者从服务器获取记录的最小字节数。如果可用的数据量小于设置值，broker 会等待有足够的可用数据时才会把它返回给消费者。</p>
<h3 id="2-fetch-max-wait-ms">2. <a href="http://fetch.max.wait.ms">fetch.max.wait.ms</a></h3>
<p>broker 返回给消费者数据的等待时间，默认是 500ms。</p>
<h3 id="3-max-partition-fetch-bytes">3. max.partition.fetch.bytes</h3>
<p>该属性指定了服务器从每个分区返回给消费者的最大字节数，默认为 1MB。</p>
<h3 id="4-session-timeout-ms">4. <a href="http://session.timeout.ms">session.timeout.ms</a></h3>
<p>消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。</p>
<h3 id="5-auto-offset-reset">5. auto.offset.reset</h3>
<p>该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>latest (默认值) ：在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的最新记录）;</p>
</li>
<li class="lvl-2">
<p>earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。</p>
</li>
</ul>
<h3 id="6-enable-auto-commit">6. enable.auto.commit</h3>
<p>是否自动提交偏移量，默认值是 true。为了避免出现重复消费和数据丢失，可以把它设置为 false。</p>
<h3 id="7-client-id">7. <a href="http://client.id">client.id</a></h3>
<p>客户端 id，服务器用来识别消息的来源。</p>
<h3 id="8-max-poll-records">8. max.poll.records</h3>
<p>单次调用 <code>poll()</code> 方法能够返回的记录数量。</p>
<h3 id="9-receive-buffer-bytes-send-buffer-byte">9. receive.buffer.bytes &amp; send.buffer.byte</h3>
<p>这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。</p>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka生产者详解</title>
    <url>/2022/03/19/Kafka%E7%94%9F%E4%BA%A7%E8%80%85%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="一、生产者发送消息的过程">一、生产者发送消息的过程</h2>
<p>首先介绍一下 Kafka 生产者发送消息的过程：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>Kafka 会将发送消息包装为 ProducerRecord 对象， ProducerRecord 对象包含了目标主题和要发送的内容，同时还可以指定键和分区。在发送 ProducerRecord 对象前，生产者会先把键和值对象序列化成字节数组，这样它们才能够在网络上传输。</p>
</li>
<li class="lvl-2">
<p>接下来，数据被传给分区器。如果之前已经在 ProducerRecord 对象里指定了分区，那么分区器就不会再做任何事情。如果没有指定分区 ，那么分区器会根据 ProducerRecord 对象的键来选择一个分区，紧接着，这条记录被添加到一个记录批次里，这个批次里的所有消息会被发送到相同的主题和分区上。有一个独立的线程负责把这些记录批次发送到相应的 broker 上。</p>
</li>
<li class="lvl-2">
<p>服务器在收到这些消息时会返回一个响应。如果消息成功写入 Kafka，就返回一个 RecordMetaData 对象，它包含了主题和分区信息，以及记录在分区里的偏移量。如果写入失败，则会返回一个错误。生产者在收到错误之后会尝试重新发送消息，如果达到指定的重试次数后还没有成功，则直接抛出异常，不再重试。</p>
</li>
</ul>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-send-messgaes.png"/> </div>
<h2 id="二、创建生产者">二、创建生产者</h2>
<h3 id="2-1-项目依赖">2.1 项目依赖</h3>
<p>本项目采用 Maven 构建，想要调用 Kafka 生产者 API，需要导入 <code>kafka-clients</code> 依赖，如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-创建生产者">2.2 创建生产者</h3>
<p>创建 Kafka 生产者时，以下三个属性是必须指定的：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>bootstrap.servers</strong> ：指定 broker 的地址清单，清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找 broker 的信息。不过建议至少要提供两个 broker 的信息作为容错；</p>
</li>
<li class="lvl-2">
<p><strong>key.serializer</strong> ：指定键的序列化器；</p>
</li>
<li class="lvl-2">
<p><strong>value.serializer</strong> ：指定值的序列化器。</p>
</li>
</ul>
<p>创建的示例代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SimpleProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    public static void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">String</span> topicName = <span class="string">&quot;Hello-Kafka&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">Properties</span> props = <span class="keyword">new</span> <span class="type">Properties</span>();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop001:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        <span class="comment">/*创建生产者*/</span></span><br><span class="line">        <span class="type">Producer</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; producer = <span class="keyword">new</span> <span class="type">KafkaProducer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            <span class="type">ProducerRecord</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; record = <span class="keyword">new</span> <span class="type">ProducerRecord</span>&lt;&gt;(topicName, <span class="string">&quot;hello&quot;</span> + i, </span><br><span class="line">                                                                         <span class="string">&quot;world&quot;</span> + i);</span><br><span class="line">            <span class="comment">/* 发送消息*/</span></span><br><span class="line">            producer.send(record);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*关闭生产者*/</span></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>本篇文章的所有示例代码可以从 Github 上进行下载：<a href="https://github.com/oicio/BigData-Notes/tree/master/code/Kafka/kafka-basis">kafka-basis</a></p>
</blockquote>
<h3 id="2-3-测试">2.3 测试</h3>
<h4 id="1-启动Kakfa">1. 启动Kakfa</h4>
<p>Kafka 的运行依赖于 zookeeper，需要预先启动，可以启动 Kafka 内置的 zookeeper，也可以启动自己安装的：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">zookeeper启动命令</span></span><br><span class="line">bin/zkServer.sh start</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">内置zookeeper启动命令</span></span><br><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties</span><br></pre></td></tr></table></figure>
<p>启动单节点 kafka 用于测试：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">bin/kafka-server-start.sh config/server.properties</span></span><br></pre></td></tr></table></figure>
<h4 id="2-创建topic">2. 创建topic</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">创建用于测试主题</span></span><br><span class="line">bin/kafka-topics.sh --create \</span><br><span class="line">                    --bootstrap-server hadoop001:9092 \</span><br><span class="line">                     --replication-factor 1 --partitions 1 \</span><br><span class="line">                     --topic Hello-Kafka</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看所有主题</span></span><br><span class="line"> bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092</span><br></pre></td></tr></table></figure>
<h4 id="3-启动消费者">3. 启动消费者</h4>
<p>启动一个控制台消费者用于观察写入情况，启动命令如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic Hello-Kafka --from-beginning</span></span><br></pre></td></tr></table></figure>
<h4 id="4-运行项目">4. 运行项目</h4>
<p>此时可以看到消费者控制台，输出如下，这里 <code>kafka-console-consumer</code> 只会打印出值信息，不会打印出键信息。</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-simple-producer.png"/> </div>
<h3 id="2-4-可能出现的问题">2.4 可能出现的问题</h3>
<p>在这里可能出现的一个问题是：生产者程序在启动后，一直处于等待状态。这通常出现在你使用默认配置启动 Kafka 的情况下，此时需要对 <code>server.properties</code> 文件中的 <code>listeners</code> 配置进行更改：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">hadoop001 为我启动kafka服务的主机名，你可以换成自己的主机名或者ip地址</span></span><br><span class="line">listeners=PLAINTEXT://hadoop001:9092</span><br></pre></td></tr></table></figure>
<h2 id="二、发送消息">二、发送消息</h2>
<p>上面的示例程序调用了 <code>send</code> 方法发送消息后没有做任何操作，在这种情况下，我们没有办法知道消息发送的结果。想要知道消息发送的结果，可以使用同步发送或者异步发送来实现。</p>
<h3 id="2-1-同步发送">2.1 同步发送</h3>
<p>在调用 <code>send</code> 方法后可以接着调用 <code>get()</code> 方法，<code>send</code> 方法的返回值是一个 Future&lt;RecordMetadata&gt;对象，RecordMetadata 里面包含了发送消息的主题、分区、偏移量等信息。改写后的代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="type">ProducerRecord</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; record = <span class="keyword">new</span> <span class="type">ProducerRecord</span>&lt;&gt;(topicName, <span class="string">&quot;k&quot;</span> + i, <span class="string">&quot;world&quot;</span> + i);</span><br><span class="line">        <span class="comment">/*同步发送消息*/</span></span><br><span class="line">        <span class="type">RecordMetadata</span> metadata = producer.send(record).get();</span><br><span class="line">        <span class="type">System</span>.out.printf(<span class="string">&quot;topic=%s, partition=%d, offset=%s \n&quot;</span>,</span><br><span class="line">                metadata.topic(), metadata.partition(), metadata.offset());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (<span class="type">InterruptedException</span> | <span class="type">ExecutionException</span> e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此时得到的输出如下：偏移量和调用次数有关，所有记录都分配到了 0 分区，这是因为在创建 <code>Hello-Kafka</code> 主题时候，使用 <code>--partitions</code> 指定其分区数为 1，即只有一个分区。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">topic=Hello-Kafka, partition=0, offset=40 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=41 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=42 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=43 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=44 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=45 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=46 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=47 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=48 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=49 </span><br></pre></td></tr></table></figure>
<h3 id="2-2-异步发送">2.2 异步发送</h3>
<p>通常我们并不关心发送成功的情况，更多关注的是失败的情况，因此 Kafka 提供了异步发送和回调函数。 代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    <span class="type">ProducerRecord</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; record = <span class="keyword">new</span> <span class="type">ProducerRecord</span>&lt;&gt;(topicName, <span class="string">&quot;k&quot;</span> + i, <span class="string">&quot;world&quot;</span> + i);</span><br><span class="line">    <span class="comment">/*异步发送消息，并监听回调*/</span></span><br><span class="line">    producer.send(record, <span class="keyword">new</span> <span class="type">Callback</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        public void onCompletion(<span class="type">RecordMetadata</span> metadata, <span class="type">Exception</span> exception) &#123;</span><br><span class="line">            <span class="keyword">if</span> (exception != <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="type">System</span>.out.println(<span class="string">&quot;进行异常处理&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">System</span>.out.printf(<span class="string">&quot;topic=%s, partition=%d, offset=%s \n&quot;</span>,</span><br><span class="line">                        metadata.topic(), metadata.partition(), metadata.offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="三、自定义分区器">三、自定义分区器</h2>
<p>Kafka 有着默认的分区机制：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>如果键值为 null， 则使用轮询 (Round Robin) 算法将消息均衡地分布到各个分区上；</p>
</li>
<li class="lvl-2">
<p>如果键值不为 null，那么 Kafka 会使用内置的散列算法对键进行散列，然后分布到各个分区上。</p>
</li>
</ul>
<p>某些情况下，你可能有着自己的分区需求，这时候可以采用自定义分区器实现。这里给出一个自定义分区器的示例：</p>
<h3 id="3-1-自定义分区器">3.1 自定义分区器</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义分区器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomPartitioner</span> <span class="keyword">implements</span> <span class="title class_">Partitioner</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> passLine;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> &#123;</span><br><span class="line">        <span class="comment">/*从生产者配置中获取分数线*/</span></span><br><span class="line">        passLine = (Integer) configs.get(<span class="string">&quot;pass.line&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">partition</span><span class="params">(String topic, Object key, <span class="type">byte</span>[] keyBytes, Object value, </span></span><br><span class="line"><span class="params">                         <span class="type">byte</span>[] valueBytes, Cluster cluster)</span> &#123;</span><br><span class="line">        <span class="comment">/*key 值为分数，当分数大于分数线时候，分配到 1 分区，否则分配到 0 分区*/</span></span><br><span class="line">        <span class="keyword">return</span> (Integer) key &gt;= passLine ? <span class="number">1</span> : <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;分区器关闭&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要在创建生产者时指定分区器，和分区器所需要的配置参数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProducerWithPartitioner</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">String</span> <span class="variable">topicName</span> <span class="operator">=</span> <span class="string">&quot;Kafka-Partitioner-Test&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop001:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.IntegerSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*传递自定义分区器*/</span></span><br><span class="line">        props.put(<span class="string">&quot;partitioner.class&quot;</span>, <span class="string">&quot;com.oicio.producers.partitioners.CustomPartitioner&quot;</span>);</span><br><span class="line">        <span class="comment">/*传递分区器所需的参数*/</span></span><br><span class="line">        props.put(<span class="string">&quot;pass.line&quot;</span>, <span class="number">6</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;Integer, String&gt; producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt;= <span class="number">10</span>; i++) &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">score</span> <span class="operator">=</span> <span class="string">&quot;score:&quot;</span> + i;</span><br><span class="line">            ProducerRecord&lt;Integer, String&gt; record = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(topicName, i, score);</span><br><span class="line">            <span class="comment">/*异步发送消息*/</span></span><br><span class="line">            producer.send(record, (metadata, exception) -&gt;</span><br><span class="line">                    System.out.printf(<span class="string">&quot;%s, partition=%d, \n&quot;</span>, score, metadata.partition()));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-测试">3.2 测试</h3>
<p>需要创建一个至少有两个分区的主题：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --create \</span><br><span class="line">                   --bootstrap-server hadoop001:9092 \</span><br><span class="line">                    --replication-factor 1 --partitions 2 \</span><br><span class="line">                    --topic Kafka-Partitioner-Test</span><br></pre></td></tr></table></figure>
<p>此时输入如下，可以看到分数大于等于 6 分的都被分到 1 分区，而小于 6 分的都被分到了 0 分区。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">score:6, partition=1, </span><br><span class="line">score:7, partition=1, </span><br><span class="line">score:8, partition=1, </span><br><span class="line">score:9, partition=1, </span><br><span class="line">score:10, partition=1, </span><br><span class="line">score:0, partition=0, </span><br><span class="line">score:1, partition=0, </span><br><span class="line">score:2, partition=0, </span><br><span class="line">score:3, partition=0, </span><br><span class="line">score:4, partition=0, </span><br><span class="line">score:5, partition=0, </span><br><span class="line">分区器关闭</span><br></pre></td></tr></table></figure>
<h2 id="四、生产者其他属性">四、生产者其他属性</h2>
<p>上面生产者的创建都仅指定了服务地址，键序列化器、值序列化器，实际上 Kafka 的生产者还有很多可配置属性，如下：</p>
<h3 id="1-acks">1. acks</h3>
<p>acks 参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入是成功的：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>acks=0</strong> ： 消息发送出去就认为已经成功了，不会等待任何来自服务器的响应；</p>
</li>
<li class="lvl-2">
<p><strong>acks=1</strong> ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应；</p>
</li>
<li class="lvl-2">
<p><strong>acks=all</strong> ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。</p>
</li>
</ul>
<h3 id="2-buffer-memory">2. buffer.memory</h3>
<p>设置生产者内存缓冲区的大小。</p>
<h3 id="3-compression-type">3. compression.type</h3>
<p>默认情况下，发送的消息不会被压缩。如果想要进行压缩，可以配置此参数，可选值有 snappy，gzip，lz4。</p>
<h3 id="4-retries">4. retries</h3>
<p>发生错误后，消息重发的次数。如果达到设定值，生产者就会放弃重试并返回错误。</p>
<h3 id="5-batch-size">5. batch.size</h3>
<p>当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。</p>
<h3 id="6-linger-ms">6. <a href="http://linger.ms">linger.ms</a></h3>
<p>该参数制定了生产者在发送批次之前等待更多消息加入批次的时间。</p>
<h3 id="7-clent-id">7. <a href="http://clent.id">clent.id</a></h3>
<p>客户端 id,服务器用来识别消息的来源。</p>
<h3 id="8-max-in-flight-requests-per-connection">8. max.in.flight.requests.per.connection</h3>
<p>指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量，把它设置为 1 可以保证消息是按照发送的顺序写入服务器，即使发生了重试。</p>
<h3 id="9-timeout-ms-request-timeout-ms-metadata-fetch-timeout-ms">9. <a href="http://timeout.ms">timeout.ms</a>, <a href="http://request.timeout.ms">request.timeout.ms</a> &amp; <a href="http://metadata.fetch.timeout.ms">metadata.fetch.timeout.ms</a></h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a href="http://timeout.ms">timeout.ms</a> 指定了 borker 等待同步副本返回消息的确认时间；</p>
</li>
<li class="lvl-2">
<p><a href="http://request.timeout.ms">request.timeout.ms</a> 指定了生产者在发送数据时等待服务器返回响应的时间；</p>
</li>
<li class="lvl-2">
<p><a href="http://metadata.fetch.timeout.ms">metadata.fetch.timeout.ms</a> 指定了生产者在获取元数据（比如分区首领是谁）时等待服务器返回响应的时间。</p>
</li>
</ul>
<h3 id="10-max-block-ms">10. <a href="http://max.block.ms">max.block.ms</a></h3>
<p>指定了在调用 <code>send()</code> 方法或使用 <code>partitionsFor()</code> 方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法会阻塞。在阻塞时间达到 <a href="http://max.block.ms">max.block.ms</a> 时，生产者会抛出超时异常。</p>
<h3 id="11-max-request-size">11. max.request.size</h3>
<p>该参数用于控制生产者发送的请求大小。它可以指发送的单个消息的最大值，也可以指单个请求里所有消息总的大小。例如，假设这个值为 1000K ，那么可以发送的单个最大消息为 1000K ，或者生产者可以在单个请求里发送一个批次，该批次包含了 1000 个消息，每个消息大小为 1K。</p>
<h3 id="12-receive-buffer-bytes-send-buffer-byte">12. receive.buffer.bytes &amp; send.buffer.byte</h3>
<p>这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。</p>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka简介</title>
    <url>/2022/03/19/Kafka%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h2 id="一、简介">一、简介</h2>
<p>ApacheKafka 是一个分布式的流处理平台。它具有以下特点：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>支持消息的发布和订阅，类似于 RabbtMQ、ActiveMQ 等消息队列；</p>
</li>
<li class="lvl-2">
<p>支持数据实时处理；</p>
</li>
<li class="lvl-2">
<p>能保证消息的可靠性投递；</p>
</li>
<li class="lvl-2">
<p>支持消息的持久化存储，并通过多副本分布式的存储方案来保证消息的容错；</p>
</li>
<li class="lvl-2">
<p>高吞吐率，单 Broker 可以轻松处理数千个分区以及每秒百万级的消息量。</p>
</li>
</ul>
<h2 id="二、基本概念">二、基本概念</h2>
<h3 id="2-1-Messages-And-Batches">2.1 Messages And Batches</h3>
<p>Kafka 的基本数据单元被称为 message(消息)，为减少网络开销，提高效率，多个消息会被放入同一批次 (Batch) 中后再写入。</p>
<h3 id="2-2-Topics-And-Partitions">2.2 Topics And Partitions</h3>
<p>Kafka 的消息通过 Topics(主题) 进行分类，一个主题可以被分为若干个 Partitions(分区)，一个分区就是一个提交日志 (commit log)。消息以追加的方式写入分区，然后以先入先出的顺序读取。Kafka 通过分区来实现数据的冗余和伸缩性，分区可以分布在不同的服务器上，这意味着一个 Topic 可以横跨多个服务器，以提供比单个服务器更强大的性能。</p>
<p>由于一个 Topic 包含多个分区，因此无法在整个 Topic 范围内保证消息的顺序性，但可以保证消息在单个分区内的顺序性。</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-topic.png"/> </div>
<h3 id="2-3-Producers-And-Consumers">2.3 Producers And Consumers</h3>
<h4 id="1-生产者">1. 生产者</h4>
<p>生产者负责创建消息。一般情况下，生产者在把消息均衡地分布到在主题的所有分区上，而并不关心消息会被写到哪个分区。如果我们想要把消息写到指定的分区，可以通过自定义分区器来实现。</p>
<h4 id="2-消费者">2. 消费者</h4>
<p>消费者是消费者群组的一部分，消费者负责消费消息。消费者可以订阅一个或者多个主题，并按照消息生成的顺序来读取它们。消费者通过检查消息的偏移量 (offset) 来区分读取过的消息。偏移量是一个不断递增的数值，在创建消息时，Kafka 会把它添加到其中，在给定的分区里，每个消息的偏移量都是唯一的。消费者把每个分区最后读取的偏移量保存在 Zookeeper 或 Kafka 上，如果消费者关闭或者重启，它还可以重新获取该偏移量，以保证读取状态不会丢失。</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-producer-consumer.png"/> </div>
<p>一个分区只能被同一个消费者群组里面的一个消费者读取，但可以被不同消费者群组中所组成的多个消费者共同读取。多个消费者群组中消费者共同读取同一个主题时，彼此之间互不影响。</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka消费者.png"/> </div>
<h3 id="2-4-Brokers-And-Clusters">2.4 Brokers And Clusters</h3>
<p>一个独立的 Kafka 服务器被称为 Broker。Broker 接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。Broker 为消费者提供服务，对读取分区的请求做出响应，返回已经提交到磁盘的消息。</p>
<p>Broker 是集群 (Cluster) 的组成部分。每一个集群都会选举出一个 Broker 作为集群控制器 (Controller)，集群控制器负责管理工作，包括将分区分配给 Broker 和监控 Broker。</p>
<p>在集群中，一个分区 (Partition) 从属一个 Broker，该 Broker 被称为分区的首领 (Leader)。一个分区可以分配给多个 Brokers，这个时候会发生分区复制。这种复制机制为分区提供了消息冗余，如果有一个 Broker 失效，其他 Broker 可以接管领导权。</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-cluster.png"/> </div>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux环境下Hive的安装</title>
    <url>/2021/10/15/Linux%E7%8E%AF%E5%A2%83%E4%B8%8BHive%E7%9A%84%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h2 id="一、安装Hive">一、安装Hive</h2>
<h3 id="1-1-下载并解压">1.1 下载并解压</h3>
<p>下载所需版本的 Hive，这里我下载版本为 <code>cdh5.15.2</code>。下载地址：<a href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">下载后进行解压</span></span><br><span class="line"> tar -zxvf hive-1.1.0-cdh5.15.2.tar.gz</span><br></pre></td></tr></table></figure>
<h3 id="1-2-配置环境变量">1.2 配置环境变量</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">vim /etc/profile</span></span><br></pre></td></tr></table></figure>
<p>添加环境变量：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HIVE_HOME=/usr/app/hive-1.1.0-cdh5.15.2</span><br><span class="line">export PATH=$HIVE_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>
<p>使得配置的环境变量立即生效：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure>
<h3 id="1-3-修改配置">1.3 修改配置</h3>
<p><strong>1. <a href="http://hive-env.sh">hive-env.sh</a></strong></p>
<p>进入安装目录下的 <code>conf/</code> 目录，拷贝 Hive 的环境配置模板 <code>flume-env.sh.template</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure>
<p>修改 <code>hive-env.sh</code>，指定 Hadoop 的安装路径：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2</span><br></pre></td></tr></table></figure>
<p><strong>2. hive-site.xml</strong></p>
<p>新建 hive-site.xml 文件，内容如下，主要是配置存放元数据的 MySQL 的地址、驱动、用户名和密码等信息：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop001:3306/hadoop_hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="1-4-拷贝数据库驱动">1.4 拷贝数据库驱动</h3>
<p>将 MySQL 驱动包拷贝到 Hive 安装目录的 <code>lib</code> 目录下, MySQL 驱动的下载地址为：<a href="https://dev.mysql.com/downloads/connector/j/">https://dev.mysql.com/downloads/connector/j/</a>  , 在本仓库的<a href="https://github.com/oicio/BigData-Notes/tree/master/resources">resources</a> 目录下我也上传了一份，有需要的可以自行下载。</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-mysql.png"/> </div>
<h3 id="1-5-初始化元数据库">1.5 初始化元数据库</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>当使用的 hive 是 1.x 版本时，可以不进行初始化操作，Hive 会在第一次启动的时候会自动进行初始化，但不会生成所有的元数据信息表，只会初始化必要的一部分，在之后的使用中用到其余表时会自动创建；</p>
</li>
<li class="lvl-2">
<p>当使用的 hive 是 2.x 版本时，必须手动初始化元数据库。初始化命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">schematool 命令在安装目录的 bin 目录下，由于上面已经配置过环境变量，在任意位置执行即可</span></span><br><span class="line">schematool -dbType mysql -initSchema</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>这里我使用的是 CDH 的 <code>hive-1.1.0-cdh5.15.2.tar.gz</code>，对应 <code>Hive 1.1.0</code> 版本，可以跳过这一步。</p>
<h3 id="1-6-启动">1.6 启动</h3>
<p>由于已经将 Hive 的 bin 目录配置到环境变量，直接使用以下命令启动，成功进入交互式命令行后执行 <code>show databases</code> 命令，无异常则代表搭建成功。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">hive</span></span><br></pre></td></tr></table></figure>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-install-2.png"/> </div>
<p>在 Mysql 中也能看到 Hive 创建的库和存放元数据信息的表</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-mysql-tables.png"/> </div>
<h2 id="二、HiveServer2-beeline">二、HiveServer2/beeline</h2>
<p>Hive 内置了 HiveServer 和 HiveServer2 服务，两者都允许客户端使用多种编程语言进行连接，但是 HiveServer 不能处理多个客户端的并发请求，因此产生了 HiveServer2。HiveServer2（HS2）允许远程客户端可以使用各种编程语言向 Hive 提交请求并检索结果，支持多客户端并发访问和身份验证。HS2 是由多个服务组成的单个进程，其包括基于 Thrift 的 Hive 服务（TCP 或 HTTP）和用于 Web UI 的 Jetty Web 服务。</p>
<p>HiveServer2 拥有自己的 CLI 工具——Beeline。Beeline 是一个基于 SQLLine 的 JDBC 客户端。由于目前 HiveServer2 是 Hive 开发维护的重点，所以官方更加推荐使用 Beeline 而不是 Hive CLI。以下主要讲解 Beeline 的配置方式。</p>
<h3 id="2-1-修改Hadoop配置">2.1 修改Hadoop配置</h3>
<p>修改 hadoop 集群的 core-site.xml 配置文件，增加如下配置，指定 hadoop 的 root 用户可以代理本机上所有的用户。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>之所以要配置这一步，是因为 hadoop 2.0 以后引入了安全伪装机制，使得 hadoop 不允许上层系统（如 hive）直接将实际用户传递到 hadoop 层，而应该将实际用户传递给一个超级代理，由该代理在 hadoop 上执行操作，以避免任意客户端随意操作 hadoop。如果不配置这一步，在之后的连接中可能会抛出 <code>AuthorizationException</code> 异常。</p>
<blockquote>
<p>关于 Hadoop 的用户代理机制，可以参考：<a href="https://blog.csdn.net/u012948976/article/details/49904675#%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E8%A7%A3%E8%AF%BB">hadoop 的用户代理机制</a> 或 <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html">Superusers Acting On Behalf Of Other Users</a></p>
</blockquote>
<h3 id="2-2-启动hiveserver2">2.2 启动hiveserver2</h3>
<p>由于上面已经配置过环境变量，这里直接启动即可：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">nohup</span> hiveserver2 &amp;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-使用beeline">2.3 使用beeline</h3>
<p>可以使用以下命令进入 beeline 交互式命令行，出现 <code>Connected</code> 则代表连接成功。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">beeline -u jdbc:hive2://hadoop001:10000 -n root</span></span><br></pre></td></tr></table></figure>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-beeline-cli.png"/> </div>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring整合Mybatis+Phoenix</title>
    <url>/2021/10/25/SpringBoot%E6%95%B4%E5%90%88Mybatis+Phoenix/</url>
    <content><![CDATA[<h2 id="一、前言-2">一、前言</h2>
<p>使用 Spring+Mybatis 操作 Phoenix 和操作其他的关系型数据库（如 Mysql，Oracle）在配置上是基本相同的，下面会分别给出 Spring/Spring Boot 整合步骤，完整代码见本仓库：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><a href="https://github.com/ihadyou/BigData-Notes/tree/master/code/Phoenix/spring-mybatis-phoenix">Spring + Mybatis + Phoenix</a></p>
</li>
<li class="lvl-2">
<p><a href="https://github.com/ihadyou/BigData-Notes/tree/master/code/Phoenix/spring-boot-mybatis-phoenix">SpringBoot + Mybatis + Phoenix</a></p>
</li>
</ul>
<h2 id="二、Spring-Mybatis-Phoenix">二、Spring + Mybatis + Phoenix</h2>
<h3 id="2-1-项目结构">2.1 项目结构</h3>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/spring-mybatis-phoenix.png"/> </div>
<h3 id="2-2-主要依赖">2.2 主要依赖</h3>
<p>除了 Spring 相关依赖外，还需要导入 <code>phoenix-core</code> 和对应的 Mybatis 依赖包</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--mybatis 依赖包--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis-spring<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--phoenix core--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-数据库配置文件">2.3  数据库配置文件</h3>
<p>在数据库配置文件 <code>jdbc.properties</code>  中配置数据库驱动和 zookeeper 地址</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据库驱动</span></span><br><span class="line"><span class="attr">phoenix.driverClassName</span>=<span class="string">org.apache.phoenix.jdbc.PhoenixDriver</span></span><br><span class="line"><span class="comment"># zookeeper地址</span></span><br><span class="line"><span class="attr">phoenix.url</span>=<span class="string">jdbc:phoenix:192.168.0.105:2181</span></span><br></pre></td></tr></table></figure>
<h3 id="2-4-配置数据源和会话工厂">2.4  配置数据源和会话工厂</h3>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">beans</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://www.springframework.org/schema/beans&quot;</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xmlns:context</span>=<span class="string">&quot;http://www.springframework.org/schema/context&quot;</span> <span class="attr">xmlns:tx</span>=<span class="string">&quot;http://www.springframework.org/schema/tx&quot;</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd</span></span></span><br><span class="line"><span class="string"><span class="tag">        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.1.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 开启注解包扫描--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">context:component-scan</span> <span class="attr">base-package</span>=<span class="string">&quot;com.ihadyou.*&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--指定配置文件的位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">context:property-placeholder</span> <span class="attr">location</span>=<span class="string">&quot;classpath:jdbc.properties&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--配置数据源--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;dataSource&quot;</span> <span class="attr">class</span>=<span class="string">&quot;org.springframework.jdbc.datasource.DriverManagerDataSource&quot;</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--Phoenix 配置--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;driverClassName&quot;</span> <span class="attr">value</span>=<span class="string">&quot;$&#123;phoenix.driverClassName&#125;&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;url&quot;</span> <span class="attr">value</span>=<span class="string">&quot;$&#123;phoenix.url&#125;&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--配置 mybatis 会话工厂 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;sqlSessionFactory&quot;</span> <span class="attr">class</span>=<span class="string">&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;dataSource&quot;</span> <span class="attr">ref</span>=<span class="string">&quot;dataSource&quot;</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--指定 mapper 文件所在的位置--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;mapperLocations&quot;</span> <span class="attr">value</span>=<span class="string">&quot;classpath*:/mappers/**/*.xml&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;configLocation&quot;</span> <span class="attr">value</span>=<span class="string">&quot;classpath:mybatisConfig.xml&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--扫描注册接口 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--作用:从接口的基础包开始递归搜索，并将它们注册为 MapperFactoryBean(只有至少一种方法的接口才会被注册;, 具体类将被忽略)--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">class</span>=<span class="string">&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--指定会话工厂 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;sqlSessionFactoryBeanName&quot;</span> <span class="attr">value</span>=<span class="string">&quot;sqlSessionFactory&quot;</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 指定 mybatis 接口所在的包 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;basePackage&quot;</span> <span class="attr">value</span>=<span class="string">&quot;com.ihadyou.dao&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">beans</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-5-Mybtais参数配置">2.5 Mybtais参数配置</h3>
<p>新建 mybtais 配置文件，按照需求配置额外参数， 更多 settings 配置项可以参考<a href="http://www.mybatis.org/mybatis-3/zh/configuration.html">官方文档</a></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">configuration</span></span></span><br><span class="line"><span class="meta">        <span class="keyword">PUBLIC</span> <span class="string">&quot;-//mybatis.org//DTD Config 3.0//EN&quot;</span></span></span><br><span class="line"><span class="meta">        <span class="string">&quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- mybatis 配置文件 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">settings</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 开启驼峰命名 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">setting</span> <span class="attr">name</span>=<span class="string">&quot;mapUnderscoreToCamelCase&quot;</span> <span class="attr">value</span>=<span class="string">&quot;true&quot;</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 打印查询 sql --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">setting</span> <span class="attr">name</span>=<span class="string">&quot;logImpl&quot;</span> <span class="attr">value</span>=<span class="string">&quot;STDOUT_LOGGING&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">settings</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-6-查询接口">2.6 查询接口</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">PopulationDao</span> &#123;</span><br><span class="line"></span><br><span class="line">    List&lt;USPopulation&gt; <span class="title function_">queryAll</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">save</span><span class="params">(USPopulation USPopulation)</span>;</span><br><span class="line"></span><br><span class="line">    USPopulation <span class="title function_">queryByStateAndCity</span><span class="params">(<span class="meta">@Param(&quot;state&quot;)</span> String state, <span class="meta">@Param(&quot;city&quot;)</span> String city)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">deleteByStateAndCity</span><span class="params">(<span class="meta">@Param(&quot;state&quot;)</span> String state, <span class="meta">@Param(&quot;city&quot;)</span> String city)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">mapper</span></span></span><br><span class="line"><span class="meta">        <span class="keyword">PUBLIC</span> <span class="string">&quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;</span></span></span><br><span class="line"><span class="meta">        <span class="string">&quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">namespace</span>=<span class="string">&quot;com.ihadyou.dao.PopulationDao&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;queryAll&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;com.ihadyou.bean.USPopulation&quot;</span>&gt;</span></span><br><span class="line">        SELECT * FROM us_population</span><br><span class="line">    <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">&quot;save&quot;</span>&gt;</span></span><br><span class="line">        UPSERT INTO us_population VALUES( #&#123;state&#125;, #&#123;city&#125;, #&#123;population&#125; )</span><br><span class="line">    <span class="tag">&lt;/<span class="name">insert</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;queryByStateAndCity&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;com.ihadyou.bean.USPopulation&quot;</span>&gt;</span></span><br><span class="line">        SELECT * FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">delete</span> <span class="attr">id</span>=<span class="string">&quot;deleteByStateAndCity&quot;</span>&gt;</span></span><br><span class="line">        DELETE FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">delete</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">mapper</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-7-单元测试">2.7 单元测试</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RunWith(SpringRunner.class)</span></span><br><span class="line"><span class="meta">@ContextConfiguration(&#123;&quot;classpath:springApplication.xml&quot;&#125;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">PopulationDaoTest</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> PopulationDao populationDao;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">queryAll</span><span class="params">()</span> &#123;</span><br><span class="line">        List&lt;USPopulation&gt; USPopulationList = populationDao.queryAll();</span><br><span class="line">        <span class="keyword">if</span> (USPopulationList != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (USPopulation USPopulation : USPopulationList) &#123;</span><br><span class="line">                System.out.println(USPopulation.getCity() + <span class="string">&quot; &quot;</span> + USPopulation.getPopulation());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">save</span><span class="params">()</span> &#123;</span><br><span class="line">        populationDao.save(<span class="keyword">new</span> <span class="title class_">USPopulation</span>(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>, <span class="number">66666</span>));</span><br><span class="line">        <span class="type">USPopulation</span> <span class="variable">usPopulation</span> <span class="operator">=</span> populationDao.queryByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">update</span><span class="params">()</span> &#123;</span><br><span class="line">        populationDao.save(<span class="keyword">new</span> <span class="title class_">USPopulation</span>(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>, <span class="number">99999</span>));</span><br><span class="line">        <span class="type">USPopulation</span> <span class="variable">usPopulation</span> <span class="operator">=</span> populationDao.queryByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">delete</span><span class="params">()</span> &#123;</span><br><span class="line">        populationDao.deleteByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        <span class="type">USPopulation</span> <span class="variable">usPopulation</span> <span class="operator">=</span> populationDao.queryByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="三、SpringBoot-Mybatis-Phoenix">三、SpringBoot + Mybatis + Phoenix</h2>
<h3 id="3-1-项目结构-2">3.1 项目结构</h3>
<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/spring-boot-mybatis-phoenix.png"/> </div>
<h3 id="3-2-主要依赖-2">3.2 主要依赖</h3>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--spring 1.5 x 以上版本对应 mybatis 1.3.x (1.3.1)</span></span><br><span class="line"><span class="comment">        关于更多 spring-boot 与 mybatis 的版本对应可以参见 &lt;a href=&quot;http://www.mybatis.org/spring-boot-starter/mybatis-spring-boot-autoconfigure/&quot;&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis.spring.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis-spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--phoenix core--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>spring boot 与 mybatis 版本的对应关系：</p>
<table>
<thead>
<tr>
<th>MyBatis-Spring-Boot-Starter 版本</th>
<th>MyBatis-Spring 版本</th>
<th>Spring Boot 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1.3.x (1.3.1)</strong></td>
<td>1.3 or higher</td>
<td>1.5 or higher</td>
</tr>
<tr>
<td><strong>1.2.x (1.2.1)</strong></td>
<td>1.3 or higher</td>
<td>1.4 or higher</td>
</tr>
<tr>
<td><strong>1.1.x (1.1.1)</strong></td>
<td>1.3 or higher</td>
<td>1.3 or higher</td>
</tr>
<tr>
<td><strong>1.0.x (1.0.2)</strong></td>
<td>1.2 or higher</td>
<td>1.3 or higher</td>
</tr>
</tbody>
</table>
<h3 id="3-3-配置数据源">3.3 配置数据源</h3>
<p>在 application.yml 中配置数据源，spring boot 2.x 版本默认采用 Hikari 作为数据库连接池，Hikari 是目前 java 平台性能最好的连接池，性能好于 druid。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">datasource:</span></span><br><span class="line">    <span class="comment">#zookeeper 地址</span></span><br><span class="line">    <span class="attr">url:</span> <span class="string">jdbc:phoenix:192.168.0.105:2181</span></span><br><span class="line">    <span class="attr">driver-class-name:</span> <span class="string">org.apache.phoenix.jdbc.PhoenixDriver</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果不想配置对数据库连接池做特殊配置的话,以下关于连接池的配置就不是必须的</span></span><br><span class="line">    <span class="comment"># spring-boot 2.X 默认采用高性能的 Hikari 作为连接池 更多配置可以参考 https://github.com/brettwooldridge/HikariCP#configuration-knobs-baby</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">com.zaxxer.hikari.HikariDataSource</span></span><br><span class="line">    <span class="attr">hikari:</span></span><br><span class="line">      <span class="comment"># 池中维护的最小空闲连接数</span></span><br><span class="line">      <span class="attr">minimum-idle:</span> <span class="number">10</span></span><br><span class="line">      <span class="comment"># 池中最大连接数，包括闲置和使用中的连接</span></span><br><span class="line">      <span class="attr">maximum-pool-size:</span> <span class="number">20</span></span><br><span class="line">      <span class="comment"># 此属性控制从池返回的连接的默认自动提交行为。默认为 true</span></span><br><span class="line">      <span class="attr">auto-commit:</span> <span class="literal">true</span></span><br><span class="line">      <span class="comment"># 允许最长空闲时间</span></span><br><span class="line">      <span class="attr">idle-timeout:</span> <span class="number">30000</span></span><br><span class="line">      <span class="comment"># 此属性表示连接池的用户定义名称，主要显示在日志记录和 JMX 管理控制台中，以标识池和池配置。 默认值：自动生成</span></span><br><span class="line">      <span class="attr">pool-name:</span> <span class="string">custom-hikari</span></span><br><span class="line">      <span class="comment">#此属性控制池中连接的最长生命周期，值 0 表示无限生命周期，默认 1800000 即 30 分钟</span></span><br><span class="line">      <span class="attr">max-lifetime:</span> <span class="number">1800000</span></span><br><span class="line">      <span class="comment"># 数据库连接超时时间,默认 30 秒，即 30000</span></span><br><span class="line">      <span class="attr">connection-timeout:</span> <span class="number">30000</span></span><br><span class="line">      <span class="comment"># 连接测试 sql 这个地方需要根据数据库方言差异而配置 例如 oracle 就应该写成  select 1 from dual</span></span><br><span class="line">      <span class="attr">connection-test-query:</span> <span class="string">SELECT</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mybatis 相关配置</span></span><br><span class="line"><span class="attr">mybatis:</span></span><br><span class="line">  <span class="attr">configuration:</span></span><br><span class="line">    <span class="comment"># 是否打印 sql 语句 调试的时候可以开启</span></span><br><span class="line">    <span class="attr">log-impl:</span> <span class="string">org.apache.ibatis.logging.stdout.StdOutImpl</span></span><br></pre></td></tr></table></figure>
<h3 id="3-4-新建查询接口">3.4 新建查询接口</h3>
<p>上面 Spring+Mybatis 我们使用了 XML 的方式来写 SQL，为了体现 Mybatis 支持多种方式，这里使用注解的方式来写 SQL。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Mapper</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">PopulationDao</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Select(&quot;SELECT * from us_population&quot;)</span></span><br><span class="line">    List&lt;USPopulation&gt; <span class="title function_">queryAll</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Insert(&quot;UPSERT INTO us_population VALUES( #&#123;state&#125;, #&#123;city&#125;, #&#123;population&#125; )&quot;)</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">save</span><span class="params">(USPopulation USPopulation)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Select(&quot;SELECT * FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;&quot;)</span></span><br><span class="line">    USPopulation <span class="title function_">queryByStateAndCity</span><span class="params">(String state, String city)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Delete(&quot;DELETE FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;&quot;)</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">deleteByStateAndCity</span><span class="params">(String state, String city)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-5-单元测试">3.5 单元测试</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RunWith(SpringRunner.class)</span></span><br><span class="line"><span class="meta">@SpringBootTest</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">PopulationTest</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> PopulationDao populationDao;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">queryAll</span><span class="params">()</span> &#123;</span><br><span class="line">        List&lt;USPopulation&gt; USPopulationList = populationDao.queryAll();</span><br><span class="line">        <span class="keyword">if</span> (USPopulationList != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (USPopulation USPopulation : USPopulationList) &#123;</span><br><span class="line">                System.out.println(USPopulation.getCity() + <span class="string">&quot; &quot;</span> + USPopulation.getPopulation());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">save</span><span class="params">()</span> &#123;</span><br><span class="line">        populationDao.save(<span class="keyword">new</span> <span class="title class_">USPopulation</span>(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>, <span class="number">66666</span>));</span><br><span class="line">        <span class="type">USPopulation</span> <span class="variable">usPopulation</span> <span class="operator">=</span> populationDao.queryByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">update</span><span class="params">()</span> &#123;</span><br><span class="line">        populationDao.save(<span class="keyword">new</span> <span class="title class_">USPopulation</span>(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>, <span class="number">99999</span>));</span><br><span class="line">        <span class="type">USPopulation</span> <span class="variable">usPopulation</span> <span class="operator">=</span> populationDao.queryByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">delete</span><span class="params">()</span> &#123;</span><br><span class="line">        populationDao.deleteByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        <span class="type">USPopulation</span> <span class="variable">usPopulation</span> <span class="operator">=</span> populationDao.queryByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="附：建表语句">附：建表语句</h2>
<p>上面单元测试涉及到的测试表的建表语句如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> us_population (</span><br><span class="line">      state <span class="type">CHAR</span>(<span class="number">2</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">      city <span class="type">VARCHAR</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">      population <span class="type">BIGINT</span></span><br><span class="line">      <span class="keyword">CONSTRAINT</span> my_pk <span class="keyword">PRIMARY</span> KEY (state, city));</span><br><span class="line">      </span><br><span class="line"><span class="comment">-- 测试数据</span></span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;NY&#x27;</span>,<span class="string">&#x27;New York&#x27;</span>,<span class="number">8143197</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;CA&#x27;</span>,<span class="string">&#x27;Los Angeles&#x27;</span>,<span class="number">3844829</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;IL&#x27;</span>,<span class="string">&#x27;Chicago&#x27;</span>,<span class="number">2842518</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;TX&#x27;</span>,<span class="string">&#x27;Houston&#x27;</span>,<span class="number">2016582</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;PA&#x27;</span>,<span class="string">&#x27;Philadelphia&#x27;</span>,<span class="number">1463281</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;AZ&#x27;</span>,<span class="string">&#x27;Phoenix&#x27;</span>,<span class="number">1461575</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;TX&#x27;</span>,<span class="string">&#x27;San Antonio&#x27;</span>,<span class="number">1256509</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;CA&#x27;</span>,<span class="string">&#x27;San Diego&#x27;</span>,<span class="number">1255540</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;CA&#x27;</span>,<span class="string">&#x27;San Jose&#x27;</span>,<span class="number">912332</span>);</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Sqoop一致性探讨</title>
    <url>/2021/10/25/Sqoop%E4%B8%80%E8%87%B4%E6%80%A7%E6%8E%A2%E8%AE%A8/</url>
    <content><![CDATA[<h2 id="Sqoop导入导出Null存储一致性问题">Sqoop导入导出Null存储一致性问题</h2>
<p>Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用–input-null-string和–input-null-non-string两个参数。导入数据时采用–null-string和–null-non-string。</p>
<h2 id="Sqoop数据导出一致性问题">Sqoop数据导出一致性问题</h2>
<h3 id="场景1：">场景1：</h3>
<p>如Sqoop在导出到Mysql时，使用4个Map任务，过程中有2个任务失败，那此时MySQL中存储了另外两个Map任务导入的数据，此时老板正好看到了这个报表数据。而开发工程师发现任务失败后，会调试问题并最终将全部数据正确的导入MySQL，那后面老板再次看报表数据，发现本次看到的数据与之前的不一致，这在生产环境是不允许的。</p>
<h4 id="解决方案：">解决方案：</h4>
<p>由于Sqoop将导出过程分解为多个事务，因此失败的导出作业可能会导致部分数据提交到数据库。在某些情况下，这可能进一步导致后续作业因插入冲突而失败，在其他情况下，这又可能导致数据重复。您可以通过–staging-table选项指定暂存表来解决此问题，该选项用作用于暂存导出数据的辅助表。最后，分阶段处理的数据将在单个事务中移至目标表</p>
<h5 id="命令：">命令：</h5>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql://192.168.137.10:3306/user_behavior \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table app_cource_study_report \</span><br><span class="line">--columns watch_video_cnt,complete_video_cnt,dt \</span><br><span class="line">--fields-terminated-by &quot;\t&quot; \</span><br><span class="line">--export-dir &quot;/user/hive/warehouse/tmp.db/app_cource_study_analysis_$&#123;day&#125;&quot; \</span><br><span class="line">--staging-table app_cource_study_report_tmp \</span><br><span class="line">--clear-staging-table \</span><br><span class="line">--input-null-string &#x27;\N&#x27;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>sqoop</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Sqoop使用案例</title>
    <url>/2021/10/25/Sqoop%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/</url>
    <content><![CDATA[<h2 id="Sqoop-原理">Sqoop 原理</h2>
<p>将导入或导出命令翻译成 mapreduce 程序来实现。在翻译出的 mapreduce 中主要是对 inputformat 和 outputformat 进行定制。</p>
<h3 id="测试-Sqoop-是否能够成功连接数据库">测试 Sqoop 是否能够成功连接数据库</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/</span></span><br><span class="line">--username root --password 000000</span><br></pre></td></tr></table></figure>
<h2 id="Sqoop-的简单使用案例">Sqoop 的简单使用案例</h2>
<h3 id="导入数据">导入数据</h3>
<p>在 Sqoop 中，“导入”概念指：从非大数据集群（RDBMS）向大数据集群（HDFS，HIVE，<br>
HBASE）中传输数据，叫做：导入，即使用 import 关键字。</p>
<h4 id="（1）全部导入">（1）全部导入</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop import \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--table staff \</span></span><br><span class="line"><span class="language-bash">--target-dir /user/company \</span></span><br><span class="line"><span class="language-bash">--delete-target-dir \</span></span><br><span class="line"><span class="language-bash">--num-mappers 1 \</span></span><br><span class="line"><span class="language-bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span></span></span><br></pre></td></tr></table></figure>
<h4 id="（2）查询导入">（2）查询导入</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop import \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--target-dir /user/company \</span></span><br><span class="line"><span class="language-bash">--delete-target-dir \</span></span><br><span class="line"><span class="language-bash">--num-mappers 1 \</span></span><br><span class="line"><span class="language-bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span></span><br><span class="line"><span class="language-bash">--query <span class="string">&#x27;select name,sex from staff where id &lt;=1 and $CONDITIONS;&#x27;</span></span></span><br></pre></td></tr></table></figure>
<p>提示：<code>must contain '$CONDITIONS' in WHERE clause</code>.<br>
如果 query 后使用的是双引号，则$CONDITIONS 前必须加转移符，防止 shell 识别为自己的变量。</p>
<h4 id="（3）导入指定列">（3）导入指定列</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop import \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--target-dir /user/company \</span></span><br><span class="line"><span class="language-bash">--delete-target-dir \</span></span><br><span class="line"><span class="language-bash">--num-mappers 1 \</span></span><br><span class="line"><span class="language-bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span></span><br><span class="line"><span class="language-bash">--columns <span class="built_in">id</span>,sex \</span></span><br><span class="line"><span class="language-bash">--table staff</span></span><br></pre></td></tr></table></figure>
<p>提示：columns 中如果涉及到多列，用逗号分隔，分隔时不要添加空格</p>
<h4 id="（4）使用-sqoop-关键字筛选查询导入数据">（4）使用 sqoop 关键字筛选查询导入数据</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop import \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--target-dir /user/company \</span></span><br><span class="line"><span class="language-bash">--delete-target-dir \</span></span><br><span class="line"><span class="language-bash">--num-mappers 1 \</span></span><br><span class="line"><span class="language-bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span></span><br><span class="line"><span class="language-bash">--table staff \</span></span><br><span class="line"><span class="language-bash">--<span class="built_in">where</span> <span class="string">&quot;id=1&quot;</span></span></span><br></pre></td></tr></table></figure>
<h4 id="RDBMS-到-Hive">RDBMS 到 Hive</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop import \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--table staff \</span></span><br><span class="line"><span class="language-bash">--num-mappers 1 \</span></span><br><span class="line"><span class="language-bash">--hive-import \</span></span><br><span class="line"><span class="language-bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span></span><br><span class="line"><span class="language-bash">--hive-overwrite \</span></span><br><span class="line"><span class="language-bash">--hive-table staff_hive</span></span><br></pre></td></tr></table></figure>
<p>提示：该过程分为两步，第一步将数据导入到 HDFS，第二步将导入到 HDFS 的数据迁移到<br>
Hive 仓库，第一步默认的临时目录是/user/atguigu/表名</p>
<h4 id="RDBMS-到-Hbase">RDBMS 到 Hbase</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop import \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--table company \</span></span><br><span class="line"><span class="language-bash">--columns <span class="string">&quot;id,name,sex&quot;</span> \</span></span><br><span class="line"><span class="language-bash">--column-family <span class="string">&quot;info&quot;</span> \</span></span><br><span class="line"><span class="language-bash">--hbase-create-table \</span></span><br><span class="line"><span class="language-bash">--hbase-row-key <span class="string">&quot;id&quot;</span> \</span></span><br><span class="line"><span class="language-bash">--hbase-table <span class="string">&quot;hbase_company&quot;</span> \</span></span><br><span class="line"><span class="language-bash">--num-mappers 1 \</span></span><br><span class="line"><span class="language-bash">--split-by <span class="built_in">id</span></span></span><br></pre></td></tr></table></figure>
<p>提示：sqoop1.4.6 只支持 HBase1.0.1 之前的版本的自动创建 HBase 表的功能解决方案：手动创建 HBase 表</p>
<h3 id="导出数据">导出数据</h3>
<p>在 Sqoop 中，“导出”概念指：从大数据集群（HDFS，HIVE，HBASE）向非大数据集群（RDBMS）中传输数据，叫做：导出，即使用 export 关键字。</p>
<h4 id="HIVE-HDFS-到-RDBMS">HIVE/HDFS 到 RDBMS</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop <span class="built_in">export</span> \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--table staff \</span></span><br><span class="line"><span class="language-bash">--num-mappers 1 \</span></span><br><span class="line"><span class="language-bash">--export-dir /user/hive/warehouse/staff_hive \</span></span><br><span class="line"><span class="language-bash">--input-fields-terminated-by <span class="string">&quot;\t&quot;</span></span></span><br></pre></td></tr></table></figure>
<p>提示：Mysql 中如果表不存在，不会自动创建</p>
<h3 id="脚本打包">脚本打包</h3>
<p>使用 opt 格式的文件打包 sqoop 命令，然后执行</p>
<h4 id="1-创建一个-opt-文件">1) 创建一个.opt 文件</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash"><span class="built_in">mkdir</span> opt</span></span><br><span class="line"><span class="meta">$ </span><span class="language-bash"><span class="built_in">touch</span> opt/job_HDFS2RDBMS.opt</span></span><br></pre></td></tr></table></figure>
<h4 id="2-编写-sqoop-脚本">2) 编写 sqoop 脚本</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">vi opt/job_HDFS2RDBMS.opt</span></span><br><span class="line">export</span><br><span class="line">--connect</span><br><span class="line">jdbc:mysql://hadoop102:3306/company</span><br><span class="line">--username</span><br><span class="line">root</span><br><span class="line">--password</span><br><span class="line">000000</span><br><span class="line">--table</span><br><span class="line">staff</span><br><span class="line">--num-mappers</span><br><span class="line">1</span><br><span class="line">--export-dir</span><br><span class="line">/user/hive/warehouse/staff_hive</span><br><span class="line">--input-fields-terminated-by</span><br><span class="line">&quot;\t&quot;</span><br></pre></td></tr></table></figure>
<h4 id="3-执行该脚本">3) 执行该脚本</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop --options-file opt/job_HDFS2RDBMS.opt</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>sqoop</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title>ZooKeeper使用场景</title>
    <url>/2021/10/25/ZooKeeper%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/</url>
    <content><![CDATA[<h2 id="ZooKeeper-是什么">ZooKeeper 是什么</h2>
<p>  ZooKeeper 是一个分布式的，开放源码的分布式应用程序协调服务，是 Google 的 Chubby 一个开源的实现，它是集群的管理者，监视着集群中各个节点的状态根据节点提交的反馈进行下一步合理操作。最终，将简单易用的接口和性能高效、功能稳定的系统提供给用户。<br>
  客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所连接的 zookeeper 机器来处理。对于写请求，这些请求会同时发给其他 zookeeper 机器并且达成一致后，请求才会返回成功。因此，随着 zookeeper 的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。有序性是 zookeeper 中非常重要的一个特性，所有的更新都是全局有序的，每个更新都有一个唯一的时间戳，这个时间戳称为 zxid（Zookeeper Transaction Id）。而读请求只会相对于更新有序，也就是读请求的返回结果中会带有这个 zookeeper 最新的 zxid。</p>
<h2 id="Zookeeper-工作原理">Zookeeper 工作原理</h2>
<p>  Zookeeper 的核心是原子广播，这个机制保证了各个 Server 之间的同步。实现这个机制的协议叫做 Zab 协议。Zab 协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab 就进入了恢复模式，当领导者被选举出来，且大多数 Server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 Server 具有相同的系统状态。</p>
<h2 id="ZooKeeper-提供了什么">ZooKeeper 提供了什么</h2>
<p>1、文件系统<br>
2、通知机制</p>
<h3 id="Zookeeper-文件系统">Zookeeper 文件系统</h3>
<p>  Zookeeper 提供一个多层级的节点命名空间（节点称为 znode）。与文件系统不同的是，这些节点都可以设置关联的数据，而文件系统中只有文件节点可以存放数据而目录节点不行。Zookeeper 为了保证高吞吐和低延迟，在内存中维护了这个树状的目录结构，这种特性使得 Zookeeper 不能用于存放大量的数据，每个节点的存放数据上限为 1M。</p>
<h4 id="四种类型的-znode">四种类型的 znode</h4>
<p>1、PERSISTENT-持久化目录节点客户端与 zookeeper 断开连接后，该节点依旧存在<br>
2、PERSISTENT_SEQUENTIAL-持久化顺序编号目录节点客户端与 zookeeper 断开连接后，该节点依旧存在，只是 Zookeeper 给该节点名称进行顺序编号<br>
3、EPHEMERAL-临时目录节点客户端与 zookeeper 断开连接后，该节点被删除<br>
4、EPHEMERAL_SEQUENTIAL-临时顺序编号目录节点</p>
<p>  客户端与 zookeeper 断开连接后，该节点被删除，只是 Zookeeper 给该节点名称进行顺序编号</p>
<h3 id="Zookeeper-通知机制">Zookeeper 通知机制</h3>
<p>  client 端会对某个 znode 建立一个 watcher 事件，当该 znode 发生变化时，这些 client 会收到 zk 的通知，然后 client 可以根据 znode 变化来做出业务上的改变等。</p>
<h4 id="zookeeper-watch-机制">zookeeper watch 机制</h4>
<p>  Watch 机制官方声明：一个 Watch 事件是一个一次性的触发器，当被设置了 Watch 的数据发生了改变的时候，则服务器将这个改变发送给设置了 Watch 的客户端，以便通知它们。<br>
Zookeeper 机制的特点：</p>
<ol>
<li class="lvl-3">
<p>一次性触发数据发生改变时，一个 watcher event 会被发送到 client，但是 client 只会收到一次这样的信息。</p>
</li>
<li class="lvl-3">
<p>watcher event 异步发送 watcher 的通知事件从 server 发送到 client 是异步的，这就存在一个问题，不同的客户端和服务器之间通过 socket 进行通信，由于网络延迟或其他因素导致客户端在不通的时刻监听到事件，由于 Zookeeper 本身提供了 ordering guarantee，即客户端监听事件后，才会感知它所监视 znode 发生了变化。所以我们使用 Zookeeper 不能期望能够监控到节点每次的变化。Zookeeper 只能保证最终的一致性，而无法保证强一致性。</p>
</li>
<li class="lvl-3">
<p>数据监视 Zookeeper 有数据监视和子数据监视 getdata() and exists()设置数据监视，getchildren()设置了子节点监视。</p>
</li>
<li class="lvl-3">
<p>注册 watcher getData、exists、getChildren</p>
</li>
<li class="lvl-3">
<p>触发 watcher create、delete、setData</p>
</li>
<li class="lvl-3">
<p>setData()会触发 znode 上设置的 data watch（如果 set 成功的话）。一个成功的 create() 操作会触发被创建的 znode 上的数据 watch，以及其父节点上的 child watch。而一个成功的 delete()操作将会同时触发一个 znode 的 data watch 和 child watch（因为这样就没有子节点了），同时也会触发其父节点的 childwatch。</p>
</li>
<li class="lvl-3">
<p>当一个客户端连接到一个新的服务器上时，watch 将会被以任意会话事件触发。当与一个服务器失去连接的时候，是无法接收到 watch 的。而当 client 重新连接时，如果需要的话，所有先前注册过的 watch，都会被重新注册。通常这是完全透明的。只有在一个特殊情况下，watch 可能会丢失：对于一个未创建的 znode 的exist watch，如果在客户端断开连接期间被创建了，并且随后在客户端连接上之前又删除了，这种情况下，这个 watch 事件可能会被丢失。</p>
</li>
<li class="lvl-3">
<p>Watch 是轻量级的，其实就是本地 JVM 的 Callback，服务器端只是存了是否有设置了 Watcher 的布尔类型</p>
</li>
</ol>
<h2 id="Zookeeper-做了什么">Zookeeper 做了什么</h2>
<p>1、命名服务<br>
2、配置管理<br>
3、集群管理<br>
4、分布式锁<br>
5、队列管理</p>
<h3 id="zk-的命名服务（文件系统）">zk 的命名服务（文件系统）</h3>
<p>  命名服务是指通过指定的名字来获取资源或者服务的地址，利用 zk 创建一个全局的路径，即是唯一的路径，这个路径就可以作为一个名字，指向集群中的集群，提供的服务的地址，或者一个远程的对象等等。</p>
<h3 id="zk-的配置管理（文件系统、通知机制）">zk 的配置管理（文件系统、通知机制）</h3>
<p>  程序分布式的部署在不同的机器上，将程序的配置信息放在 zk 的 znode 下，当有配置发生改变时，也就是znode 发生变化时，可以通过改变 zk 中某个目录节点的内容，利用 watcher 通知给各个客户端，从而更改配置。</p>
<h3 id="Zookeeper-集群管理（文件系统、通知机制）">Zookeeper 集群管理（文件系统、通知机制）</h3>
<p>  所谓集群管理无在乎两点：是否有机器退出和加入、选举 master。<br>
  对于第一点，所有机器约定在父目录下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与zookeeper 的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知：某个兄弟目录被删除，于是，所有人都知道：它上船了。<br>
  新机器加入也是类似，所有机器收到通知：新兄弟目录加入，highcount 又有了，对于第二点，我们稍微改变一下，所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为 master 就好。</p>
<h3 id="Zookeeper-分布式锁（文件系统、通知机制）">Zookeeper 分布式锁（文件系统、通知机制）</h3>
<p>  有了 zookeeper 的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。<br>
  对于第一类，我们将 zookeeper 上的一个 znode 看作是一把锁，通过 createznode 的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。用完删除掉自己创建的distribute_lock 节点就释放出锁。<br>
  对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master 一样，编号最小的获得锁，用完删除，依次方便。</p>
<h4 id="获取分布式锁的流程">获取分布式锁的流程</h4>
<p>  在获取分布式锁的时候在 locker 节点下创建临时顺序节点，释放锁的时候删除该临时节点。客户端调用createNode 方法在 locker 下创建临时顺序节点，然后调用 getChildren(“locker”)来获取 locker 下面的所有子节点，注意此时不用设置任何 Watcher。客户端获取到所有的子节点 path 之后，如果发现自己创建的节点在所有创建的子节点序号最小，那么就认为该客户端获取到了锁。如果发现自己创建的节点并非 locker 所有子节点中最小的，说明自己还没有获取到锁，此时客户端需要找到比自己小的那个节点，然后对其调用 exist()方法，同时对其注册事件监听器。之后，让这个被关注的节点删除，则客户端的 Watcher 会收到相应通知，此时再次判断自己创建的节点是否是 locker 子节点中序号最小的，如果是则获取到了锁，如果不是则重复以上步骤继续获取到比自己小的一个节点并注册监听。当前这个过程中还需要许多的逻辑判断。<br>
  代码的实现主要是基于互斥锁，获取分布式锁的重点逻辑在于BaseDistributedLock，实现了基于Zookeeper 实现分布式锁的细节。</p>
<h3 id="Zookeeper-队列管理（文件系统、通知机制）">Zookeeper 队列管理（文件系统、通知机制）</h3>
<p>两种类型的队列：<br>
1、同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。<br>
2、队列按照 FIFO 方式进行入队和出队操作。第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。在特定的目录下创建PERSISTENT_SEQUENTIAL 节点，创建成功时 Watcher 通知等待的队列，队列删除序列号最小的节点用以消费。此场景下 Zookeeper 的 znode 用于消息存储，znode 存储的数据就是消息队列中的消息内容，<br>
SEQUENTIAL 序列号就是消息的编号，按序取出即可。由于创建的节点是持久化的，所以不必担心队列消息的丢失问题。</p>
<h2 id="Zookeeper其他问题">Zookeeper其他问题</h2>
<h3 id="Zookeeper-数据复制">Zookeeper 数据复制</h3>
<p>Zookeeper 作为一个集群提供一致的数据服务，自然，它要在所有机器间做数据复制。数据复制的好处：<br>
1、容错：一个节点出错，不致于让整个系统停止工作，别的节点可以接管它的工作；<br>
2、提高系统的扩展能力 ：把负载分布到多个节点上，或者增加节点来提高系统的负载能力；<br>
3、提高性能：让客户端本地访问就近的节点，提高用户访问速度。从客户端读写访问的透明度来看，数据复制集群系统分下面两种：<br>
1、写主(WriteMaster) ：对数据的修改提交给指定的节点。读无此限制，可以读取任何一个节点。这种情况下客户端需要对读与写进行区别，俗称读写分离；<br>
2、写任意(Write Any)：对数据的修改可提交给任意的节点，跟读一样。这种情况下，客户端对集群节点的角色与变化透明。</p>
<p>对 zookeeper 来说，它采用的方式是写任意。通过增加机器，它的读吞吐能力和响应能力扩展性非常好，而写，随着机器的增多吞吐能力肯定下降（这也是它建立 observer 的原因），而响应能力则取决于具体实现方式，是延迟复制保持最终一致性，还是立即复制快速响应。</p>
<h3 id="zookeeper-保证事务的顺序一致性">zookeeper 保证事务的顺序一致性</h3>
<p>  zookeeper 采用了递增的事务 Id 来标识，所有的 proposal（提议）都在被提出的时候加上了 zxid，zxid 实际上是一个 64 位的数字，高 32 位是 epoch（时期; 纪元; 世; 新时代）用来标识 leader 是否发生改变，如果有新的 leader 产生出来，epoch 会自增，低 32 位用来递增计数。当新产生 proposal 的时候，会依据数据库的两阶段过程，首先会向其他的 server 发出事务执行请求，如果超过半数的机器都能执行并且能够成功，那么就会开始执行。</p>
<h3 id="Zookeeper-下-Server-工作状态">Zookeeper 下 Server 工作状态</h3>
<p>每个 Server 在工作过程中有三种状态：<br>
LOOKING：当前 Server 不知道 leader 是谁，正在搜寻<br>
LEADING：当前 Server 即为选举出来的 leader<br>
FOLLOWING：leader 已经选举出来，当前 Server 与之同步</p>
<h3 id="zookeeper-如何选取主-leader">zookeeper 如何选取主 leader</h3>
<p>当 leader 崩溃或者 leader 失去大多数的 follower，这时 zk 进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的 Server 都恢复到一个正确的状态。Zk 的选举算法有两种：一种是基于 basic paxos 实现的，另外一种是基于 fast paxos 算法实现的。系统默认的选举算法为 fast paxos。</p>
<p>1、Zookeeper 选主流程(basic paxos)<br>
（1）选举线程由当前 Server 发起选举的线程担任，其主要功能是对投票结果进行统计，并选出推荐的Server；（2）选举线程首先向所有 Server 发起一次询问(包括自己)；（3）选举线程收到回复后，验证是否是自己发起的询问(验证 zxid 是否一致)，然后获取对方的 id(myid)，并存储到当前询问对象列表中，最后获取对方提议的 leader 相关信息(id,zxid)，并将这些信息存储到当次选举的投票记录表中；（4）收到所有 Server 回复以后，就计算出 zxid 最大的那个 Server，并将这个 Server 相关信息设置成下一次要投票的 Server；（5）线程将当前 zxid 最大的 Server 设置为当前 Server 要推荐的 Leader，如果此时获胜的 Server 获得 n/2+ 1 的 Server 票数，设置当前推荐的 leader 为获胜的 Server，将根据获胜的 Server 相关信息设置自己的状态，否则，继续这个过程，直到 leader 被选举出来。 通过流程分析我们可以得出：要使 Leader 获得多数Server 的支持，则 Server 总数必须是奇数 2n+1，且存活的 Server 的数目不得少于 n+1. 每个 Server 启动后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的 server 还会从磁盘快照中恢复数据和会话信息，zk 会记录事务日志并定期进行快照，方便在恢复时进行状态恢复。</p>
<p>2、Zookeeper 选主流程(basic paxos)<br>
fast paxos 流程是在选举过程中，某 Server 首先向所有 Server 提议自己要成为 leader，当其它 Server 收到提议以后，解决 epoch 和 zxid 的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息，重复这个流程，最后一定能选举出 Leader。</p>
<h3 id="Zookeeper-同步流程">Zookeeper 同步流程</h3>
<p>选完 Leader 以后，zk 就进入状态同步过程。<br>
1、Leader 等待 server 连接；<br>
2、Follower 连接 leader，将最大的 zxid 发送给 leader；<br>
3、Leader 根据 follower 的 zxid 确定同步点；<br>
4、完成同步后通知 follower 已经成为 uptodate 状态；<br>
5、Follower 收到 uptodate 消息后，又可以重新接受 client 的请求进行服务了。</p>
<h4 id="机器中为什么会有-leader">机器中为什么会有 leader</h4>
<p>在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他的机器可以共享这个结果，这样可以大大减少重复计算，提高性能，于是就需要进行 leader 选举。</p>
<h3 id="zk-节点宕机如何处理">zk 节点宕机如何处理</h3>
<p>Zookeeper 本身也是集群，推荐配置不少于 3 个服务器。Zookeeper 自身也要保证当一个节点宕机时，其他节点会继续提供服务。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>如果是一个 Follower 宕机，还有 2 台服务器提供访问，因为 Zookeeper 上的数据是有多个副本的，数据并不会丢失；</p>
</li>
<li class="lvl-2">
<p>如果是一个 Leader 宕机，Zookeeper 会选举出新的 Leader。</p>
</li>
</ul>
<p>ZK 集群的机制是只要超过半数的节点正常，集群就能正常提供服务。只有在 ZK 节点挂得太多，只剩一半或不到一半节点能工作，集群才失效。所以<br>
3 个节点的 cluster 可以挂掉 1 个节点(leader 可以得到 2 票&gt;1.5)<br>
2 个节点的 cluster 就不能挂掉任何 1 个节点了(leader 可以得到 1 票&lt;=1)</p>
<h4 id="zookeeper-负载均衡和-nginx-负载均衡区别">zookeeper 负载均衡和 nginx 负载均衡区别</h4>
<p>zk 的负载均衡是可以调控，nginx 只是能调权重，其他需要可控的都需要自己写插件；但是 nginx 的吞吐量比zk 大很多，应该说按业务选择用哪种方式。</p>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Sqoop常用命令及参数</title>
    <url>/2021/10/25/Sqoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E5%8F%82%E6%95%B0/</url>
    <content><![CDATA[<h2 id="Sqoop-常用命令及参数">Sqoop 常用命令及参数</h2>
<h3 id="常用命令列举">常用命令列举</h3>
<table>
<thead>
<tr>
<th style="text-align:left">序号</th>
<th style="text-align:left">命令</th>
<th style="text-align:left">类</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">import</td>
<td style="text-align:left">ImportTool</td>
<td style="text-align:left">将数据导入到集群</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">export</td>
<td style="text-align:left">ExportTool</td>
<td style="text-align:left">将集群数据导出</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">codegen</td>
<td style="text-align:left">CodeGenTool</td>
<td style="text-align:left">获取数据库中某张表数据生成Java 并打包Jar</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td style="text-align:left">create-hive-table</td>
<td style="text-align:left">CreateHiveTableTool</td>
<td style="text-align:left">创建 Hive 表</td>
</tr>
<tr>
<td style="text-align:left">5</td>
<td style="text-align:left">eval</td>
<td style="text-align:left">EvalSqlTool</td>
<td style="text-align:left">查看 SQL 执行结果</td>
</tr>
<tr>
<td style="text-align:left">6</td>
<td style="text-align:left">import-all-tables</td>
<td style="text-align:left">ImportAllTablesTool</td>
<td style="text-align:left">导入某个数据库下所有表到 HDFS 中</td>
</tr>
<tr>
<td style="text-align:left">7</td>
<td style="text-align:left">job</td>
<td style="text-align:left">JobTool</td>
<td style="text-align:left">用来生成一个 sqoop的任务，生成后，该任务并不执行，除非使用命令执行该任务</td>
</tr>
<tr>
<td style="text-align:left">8</td>
<td style="text-align:left">list-databases</td>
<td style="text-align:left">ListDatabasesTool</td>
<td style="text-align:left">列出所有数据库名</td>
</tr>
<tr>
<td style="text-align:left">9</td>
<td style="text-align:left">list-tables</td>
<td style="text-align:left">ListTablesTool</td>
<td style="text-align:left">列出某个数据库下所有表</td>
</tr>
<tr>
<td style="text-align:left">10</td>
<td style="text-align:left">merge</td>
<td style="text-align:left">MergeTool</td>
<td style="text-align:left">将 HDFS 中不同目录下面的数据合在一起，并存放在指定的目录中</td>
</tr>
<tr>
<td style="text-align:left">11</td>
<td style="text-align:left">metastore</td>
<td style="text-align:left">MetastoreTool</td>
<td style="text-align:left">记录 sqoop job 的元数据信息，如果不启动 metastore 实例，则默认的元数据存储目录为：~/.sqoop，如果要更改存储目录，可以 在 配 置 文 件sqoop-site.xml 中进行更改</td>
</tr>
</tbody>
</table>
<h3 id="命令-参数详解">命令&amp;参数详解</h3>
<p>对于不同的命令，有不同的参数.</p>
<h4 id="公用参数：数据库连接">公用参数：数据库连接</h4>
<table>
<thead>
<tr>
<th style="text-align:left">序号</th>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">–connect</td>
<td style="text-align:left">连接关系型数据库的 URL</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">–connection-manager</td>
<td style="text-align:left">指定要使用的连接管理类</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">–driver</td>
<td style="text-align:left">Hadoop 根目录</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td style="text-align:left">–help</td>
<td style="text-align:left">打印帮助信息</td>
</tr>
<tr>
<td style="text-align:left">5</td>
<td style="text-align:left">–password</td>
<td style="text-align:left">连接数据库的密码</td>
</tr>
<tr>
<td style="text-align:left">6</td>
<td style="text-align:left">–username</td>
<td style="text-align:left">连接数据库的用户名</td>
</tr>
<tr>
<td style="text-align:left">7</td>
<td style="text-align:left">–verbose</td>
<td style="text-align:left">在控制台打印出详细信息</td>
</tr>
</tbody>
</table>
<h4 id="公用参数：import">公用参数：import</h4>
<table>
<thead>
<tr>
<th style="text-align:left">序号</th>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">–enclosed-by <char></td>
<td style="text-align:left">给字段值前加上指定的字符</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">–escaped-by <char></td>
<td style="text-align:left">对字段中的双引号加转义符</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">–fields-terminated-by <char></td>
<td style="text-align:left">设定每个字段是以什么符号作为结束，默认为逗号</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td style="text-align:left">–lines-terminated-by <char></td>
<td style="text-align:left">设定每行记录之间的分隔符，默认是\n</td>
</tr>
<tr>
<td style="text-align:left">5</td>
<td style="text-align:left">–mysql-delimiters</td>
<td style="text-align:left">Mysql 默认的分隔符设置，字段之间以逗号分隔，行之间以\n 分隔，默认转义符是\，字段值以单引号包裹。</td>
</tr>
<tr>
<td style="text-align:left">6</td>
<td style="text-align:left">–optionally-enclosed-by <char></td>
<td style="text-align:left">给带有双引号或单引号的字段值前后加上指定字符。</td>
</tr>
</tbody>
</table>
<h4 id="公用参数：export">公用参数：export</h4>
<table>
<thead>
<tr>
<th style="text-align:left">序号</th>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">–input-enclosed-by <char></td>
<td style="text-align:left">对字段值前后加上指定字符</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">–input-escaped-by <char></td>
<td style="text-align:left">对含有转移符的字段做转义处理</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">–input-fields-terminated-by <char></td>
<td style="text-align:left">字段之间的分隔符</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td style="text-align:left">–input-lines-terminated-by <char></td>
<td style="text-align:left">行之间的分隔符</td>
</tr>
<tr>
<td style="text-align:left">5</td>
<td style="text-align:left">–input-optionally-enclosed-by <char></td>
<td style="text-align:left">给带有双引号或单引号的字段前后加上指定字符</td>
</tr>
</tbody>
</table>
<h4 id="公用参数：hive">公用参数：hive</h4>
<table>
<thead>
<tr>
<th style="text-align:left">序号</th>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">–hive-delims-replacement <arg></td>
<td style="text-align:left">用自定义的字符串替换掉数据中的\r\n和\013 \010等字符</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">–hive-drop-import-delims</td>
<td style="text-align:left">在导入数据到 hive 时，去掉数据中的\r\n\013\010 这样的字符</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">–map-column-hive <arg></td>
<td style="text-align:left">生成 hive 表时，可以更改生成字段的数据类型</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td style="text-align:left">–hive-partition-key</td>
<td style="text-align:left">创建分区，后面直接跟分区名，分区字段的默认类型为string</td>
</tr>
<tr>
<td style="text-align:left">5</td>
<td style="text-align:left">–hive-partition-value <v></td>
<td style="text-align:left">导入数据时，指定某个分区的值</td>
</tr>
<tr>
<td style="text-align:left">6</td>
<td style="text-align:left">–hive-home <dir></td>
<td style="text-align:left">hive 的安装目录，可以通过该参数覆盖之前默认配置的目录</td>
</tr>
<tr>
<td style="text-align:left">7</td>
<td style="text-align:left">–hive-import</td>
<td style="text-align:left">将数据从关系数据库中导入到 hive 表中</td>
</tr>
<tr>
<td style="text-align:left">8</td>
<td style="text-align:left">–hive-overwrite</td>
<td style="text-align:left">覆盖掉在 hive 表中已经存在的数据</td>
</tr>
<tr>
<td style="text-align:left">9</td>
<td style="text-align:left">–create-hive-table</td>
<td style="text-align:left">默认是 false，即，如果目标表已经存在了，那么创建任务失败。</td>
</tr>
<tr>
<td style="text-align:left">10</td>
<td style="text-align:left">–hive-table</td>
<td style="text-align:left">后面接要创建的 hive 表,默认使用 MySQL 的表名</td>
</tr>
<tr>
<td style="text-align:left">11</td>
<td style="text-align:left">–table</td>
<td style="text-align:left">指定关系数据库的表名</td>
</tr>
</tbody>
</table>
<h4 id="命令-参数：import">命令&amp;参数：import</h4>
<p>将关系型数据库中的数据导入到 HDFS（包括 Hive，HBase）中，如果导入的是 Hive，那么当 Hive 中没有对应表时，则自动创建。</p>
<h5 id="1-命令：">1) 命令：</h5>
<h6 id="如：导入数据到-hive-中">如：导入数据到 hive 中</h6>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop import \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--table staff \</span></span><br><span class="line"><span class="language-bash">--hive-import</span></span><br></pre></td></tr></table></figure>
<h6 id="如：增量导入数据到-hive-中，mode-append">如：增量导入数据到 hive 中，mode=append</h6>
<p>append 导入：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop import \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--table staff \</span></span><br><span class="line"><span class="language-bash">--num-mappers 1 \</span></span><br><span class="line"><span class="language-bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span></span><br><span class="line"><span class="language-bash">--target-dir /user/hive/warehouse/staff_hive \</span></span><br><span class="line"><span class="language-bash">--check-column <span class="built_in">id</span> \</span></span><br><span class="line"><span class="language-bash">--incremental append \</span></span><br><span class="line"><span class="language-bash">--last-value 3</span></span><br></pre></td></tr></table></figure>
<p>尖叫提示：append 不能与–hive-等参数同时使用（Append mode for hive imports is not yet supported. Please remove the parameter --append-mode）</p>
<h6 id="如：增量导入数据到-hdfs-中，mode-lastmodified">如：增量导入数据到 hdfs 中，mode=lastmodified</h6>
<p>先在 mysql 中建表并插入几条数据：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> company.staff_timestamp(id <span class="type">int</span>(<span class="number">4</span>), name <span class="type">varchar</span>(<span class="number">255</span>), sex <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">last_modified <span class="type">timestamp</span> <span class="keyword">DEFAULT</span> <span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">ON</span> <span class="keyword">UPDATE</span></span><br><span class="line"><span class="built_in">CURRENT_TIMESTAMP</span>);</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> company.staff_timestamp (id, name, sex) <span class="keyword">values</span>(<span class="number">1</span>, <span class="string">&#x27;AAA&#x27;</span>, <span class="string">&#x27;female&#x27;</span>);</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> company.staff_timestamp (id, name, sex) <span class="keyword">values</span>(<span class="number">2</span>, <span class="string">&#x27;BBB&#x27;</span>, <span class="string">&#x27;female&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p>先导入一部分数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop import \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--table staff_timestamp \</span></span><br><span class="line"><span class="language-bash">--delete-target-dir \</span></span><br><span class="line"><span class="language-bash">--m 1</span></span><br></pre></td></tr></table></figure>
<p>再增量导入一部分数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt; </span><span class="language-bash">insert into company.staff_timestamp (<span class="built_in">id</span>, name, sex) values(3, <span class="string">&#x27;CCC&#x27;</span>, <span class="string">&#x27;female&#x27;</span>);</span></span><br><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop import \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--table staff_timestamp \</span></span><br><span class="line"><span class="language-bash">--check-column last_modified \</span></span><br><span class="line"><span class="language-bash">--incremental lastmodified \</span></span><br><span class="line"><span class="language-bash">--last-value <span class="string">&quot;2017-09-28 22:20:38&quot;</span> \</span></span><br><span class="line"><span class="language-bash">--m 1 \</span></span><br><span class="line"><span class="language-bash">--append</span></span><br></pre></td></tr></table></figure>
<p>尖叫提示：使用 lastmodified 方式导入数据要指定增量数据是要–append（追加）还是要–merge-key（合并）尖叫提示：last-value 指定的值是会包含于增量导入的数据中</p>
<h5 id="2-参数：">2) 参数：</h5>
<table>
<thead>
<tr>
<th style="text-align:left">序号</th>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">- -append</td>
<td style="text-align:left">将数据追加到 HDFS 中已经存在的 DataSet 中，如果使用该参数，sqoop 会把数据先导入到临时文件目录，再合并。</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">- -as-avrodatafile</td>
<td style="text-align:left">将数据导入到一个 Avro 数据文件中</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">- -as-sequencefile</td>
<td style="text-align:left">将数据导入到一个 sequence文件中</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td style="text-align:left">- -as-textfile</td>
<td style="text-align:left">将数据导入到一个普通文本文件中</td>
</tr>
<tr>
<td style="text-align:left">5</td>
<td style="text-align:left">- -boundary-query <statement></td>
<td style="text-align:left">边界查询，导入的数据为该参数的值（一条 sql 语句）所执行的结果区间内的数据。</td>
</tr>
<tr>
<td style="text-align:left">6</td>
<td style="text-align:left">- -columns &lt;col1, col2, col3&gt;</td>
<td style="text-align:left">指定要导入的字段</td>
</tr>
<tr>
<td style="text-align:left">7</td>
<td style="text-align:left">- -direct</td>
<td style="text-align:left">直接导入模式，使用的是关系数据库自带的导入导出工具，以便加快导入导出过程。</td>
</tr>
<tr>
<td style="text-align:left">8</td>
<td style="text-align:left">- -direct-split-size</td>
<td style="text-align:left">在使用上面 direct 直接导入的基础上，对导入的流按字节分块，即达到该阈值就产生一个新的文件</td>
</tr>
<tr>
<td style="text-align:left">9</td>
<td style="text-align:left">- -inline-lob-limit</td>
<td style="text-align:left">设定大对象数据类型的最大值</td>
</tr>
<tr>
<td style="text-align:left">10</td>
<td style="text-align:left">–m 或–num-mappers</td>
<td style="text-align:left">启动 N 个 map 来并行导入数据，默认 4 个。</td>
</tr>
<tr>
<td style="text-align:left">11</td>
<td style="text-align:left">–query 或–e <statement></td>
<td style="text-align:left">将查询结果的数据导入，使用时必须伴随参–target-dir，–hive-table，如果查询中有where 条件，则条件后必须加上$CONDITIONS 关键字</td>
</tr>
<tr>
<td style="text-align:left">12</td>
<td style="text-align:left">–split-by <column-name></td>
<td style="text-align:left">按照某一列来切分表的工作单元，不能与–autoreset-to-one-mapper 连用</td>
</tr>
<tr>
<td style="text-align:left">13</td>
<td style="text-align:left">–table <table-name></td>
<td style="text-align:left">关系数据库的表名</td>
</tr>
<tr>
<td style="text-align:left">14</td>
<td style="text-align:left">–target-dir <dir></td>
<td style="text-align:left">指定 HDFS 路径</td>
</tr>
<tr>
<td style="text-align:left">15</td>
<td style="text-align:left">–warehouse-dir <dir></td>
<td style="text-align:left">与 14 参数不能同时使用，导入数据到 HDFS 时指定的目录</td>
</tr>
<tr>
<td style="text-align:left">16</td>
<td style="text-align:left">–where</td>
<td style="text-align:left">从关系数据库导入数据时的查询条件</td>
</tr>
<tr>
<td style="text-align:left">17</td>
<td style="text-align:left">–z 或–compress</td>
<td style="text-align:left">允许压缩</td>
</tr>
<tr>
<td style="text-align:left">18</td>
<td style="text-align:left">–compression-codec</td>
<td style="text-align:left">指定 hadoop 压缩编码类，默认为 gzip(Use Hadoop codecdefault gzip)</td>
</tr>
<tr>
<td style="text-align:left">19</td>
<td style="text-align:left">–null-string <null-string></td>
<td style="text-align:left">string 类型的列如果 null，替换为指定字符串</td>
</tr>
<tr>
<td style="text-align:left">20</td>
<td style="text-align:left">–null-non-string <null-string></td>
<td style="text-align:left">非 string 类型的列如果 null，替换为指定字符串</td>
</tr>
<tr>
<td style="text-align:left">21</td>
<td style="text-align:left">–check-column <col></td>
<td style="text-align:left">作为增量导入判断的列名</td>
</tr>
<tr>
<td style="text-align:left">22</td>
<td style="text-align:left">–incremental <mode></td>
<td style="text-align:left">mode：append 或 lastmodified</td>
</tr>
<tr>
<td style="text-align:left">23</td>
<td style="text-align:left">–last-value <value></td>
<td style="text-align:left">指定某一个值，用于标记增量导入的位置</td>
</tr>
</tbody>
</table>
<h4 id="命令-参数：export">命令&amp;参数：export</h4>
<p>从 HDFS（包括 Hive 和 HBase）中奖数据导出到关系型数据库中。</p>
<h5 id="1-命令：-2">1) 命令：</h5>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop <span class="built_in">export</span> \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--table staff \</span></span><br><span class="line"><span class="language-bash">--export-dir /user/company \</span></span><br><span class="line"><span class="language-bash">--input-fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span></span><br><span class="line"><span class="language-bash">--num-mappers 1</span></span><br></pre></td></tr></table></figure>
<h5 id="2-参数：-2">2) 参数：</h5>
<table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>–direct</td>
<td>利用数据库自带的导入导出工具，以便于提高效率</td>
</tr>
<tr>
<td>2</td>
<td>–export-dir <dir></td>
<td>存放数据的 HDFS 的源目录</td>
</tr>
<tr>
<td>3</td>
<td>m 或–num-mappers <n></td>
<td>启动 N 个 map 来并行导入数据，默认 4 个</td>
</tr>
<tr>
<td>4</td>
<td>–table <table-name></td>
<td>指定导出到哪个 RDBMS 中的表</td>
</tr>
<tr>
<td>5</td>
<td>–update-key <col-name></td>
<td>对某一列的字段进行更新操作</td>
</tr>
<tr>
<td>6</td>
<td>–update-mode <mode></td>
<td>updateonlyallowinsert(默认)</td>
</tr>
<tr>
<td>7</td>
<td>–input-null-string <null-string></td>
<td>请参考 import</td>
</tr>
<tr>
<td>8</td>
<td>–input-null-non-string <null-string></td>
<td>请参考 import</td>
</tr>
<tr>
<td>9</td>
<td>–staging-table <staging-table-name></td>
<td>创建一张临时表，用于存放所有事务的结果，然后将所有事务结果一次性导入到目标表中，防止错误。</td>
</tr>
<tr>
<td>10</td>
<td>–clear-staging-table</td>
<td>如果第 9 个参数非空，则可以在导出操作执行前，清空临时事务结果表</td>
</tr>
</tbody>
</table>
<h4 id="命令-参数：codegen">命令&amp;参数：codegen</h4>
<p>将关系型数据库中的表映射为一个 Java 类，在该类中有各列对应的各个字段。</p>
<h5 id="1-命令：-3">1) 命令：</h5>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop codegen \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--table staff \</span></span><br><span class="line"><span class="language-bash">--bindir /home/admin/Desktop/staff \</span></span><br><span class="line"><span class="language-bash">--class-name Staff \</span></span><br><span class="line"><span class="language-bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span></span></span><br></pre></td></tr></table></figure>
<h5 id="2-参数：-3">2) 参数：</h5>
<table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>–bindir <dir></td>
<td>指定生成的 Java 文件、编译成的 class 文件及将生成文件打包为 jar 的文件输出路径</td>
</tr>
<tr>
<td>2</td>
<td>–class-name <name></td>
<td>设定生成的 Java 文件指定的名称</td>
</tr>
<tr>
<td>3</td>
<td>–outdir <dir></td>
<td>生成 Java 文件存放的路径</td>
</tr>
<tr>
<td>4</td>
<td>–package-name <name></td>
<td>包名，如 com.z，就会生成 com和 z 两级目录</td>
</tr>
<tr>
<td>5</td>
<td>–input-null-non-string <null-str></td>
<td>在生成的 Java 文件中，可以将 null 字符串或者不存在的字符串设置为想要设定的值（例如空字符串）</td>
</tr>
<tr>
<td>6</td>
<td>–input-null-string <null-str></td>
<td>将 null 字符串替换成想要替换的值（一般与 5 同时使用）</td>
</tr>
<tr>
<td>7</td>
<td>–map-column-java <arg></td>
<td>数据库字段在生成的 Java 文件中会映射成各种属性，且默认的数据类型与数据库类型保持对应关系。该参数可以改变默认类型，例如：–map-column-java id=long,name=String</td>
</tr>
<tr>
<td>8</td>
<td>–null-non-string <null-str></td>
<td>在生成 Java 文件时，可以将不存在或者 null 的字符串设置为其他值</td>
</tr>
<tr>
<td>9</td>
<td>–null-string <null-str></td>
<td>在生成 Java 文件时，将 null字符串设置为其他值（一般与8 同时用）</td>
</tr>
<tr>
<td>10</td>
<td>–table <table-name></td>
<td>对应关系数据库中的表名，生成的 Java 文件中的各个属性与该表的各个字段一一对应</td>
</tr>
</tbody>
</table>
<h4 id="命令-参数：create-hive-table">命令&amp;参数：create-hive-table</h4>
<p>生成与关系数据库表结构对应的 hive 表结构。</p>
<h5 id="1-命令：-4">1) 命令：</h5>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop create-hive-table \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--table staff \</span></span><br><span class="line"><span class="language-bash">--hive-table hive_staff</span></span><br></pre></td></tr></table></figure>
<h5 id="2-参数：-4">2) 参数：</h5>
<table>
<thead>
<tr>
<th style="text-align:left">序号</th>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">–hive-home <dir></td>
<td style="text-align:left">Hive 的安装目录，可以通过该参数覆盖掉默认的 Hive 目录</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">–hive-overwrite</td>
<td style="text-align:left">覆盖掉在 Hive 表中已经存在的数据</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">–create-hive-table</td>
<td style="text-align:left">默认是 false，如果目标表已经存在了，那么创建任务会失败</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td style="text-align:left">–hive-table</td>
<td style="text-align:left">后面接要创建的 hive 表</td>
</tr>
<tr>
<td style="text-align:left">5</td>
<td style="text-align:left">–table</td>
<td style="text-align:left">指定关系数据库的表名</td>
</tr>
</tbody>
</table>
<h4 id="命令-参数：eval">命令&amp;参数：eval</h4>
<p>可以快速的使用 SQL 语句对关系型数据库进行操作，经常用于在 import 数据之前，了解一下 SQL 语句是否正确，数据是否正常，并可以将结果显示在控制台。</p>
<h5 id="1-命令：-5">1) 命令：</h5>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop <span class="built_in">eval</span> \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--query <span class="string">&quot;SELECT * FROM staff&quot;</span></span></span><br></pre></td></tr></table></figure>
<h5 id="2-参数：-5">2) 参数：</h5>
<table>
<thead>
<tr>
<th style="text-align:left">序号</th>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">–query 或–e</td>
<td style="text-align:left">后跟查询的 SQL 语句</td>
</tr>
</tbody>
</table>
<h4 id="命令-参数：import-all-tables">命令&amp;参数：import-all-tables</h4>
<p>可以将 RDBMS 中的所有表导入到 HDFS 中，每一个表都对应一个 HDFS 目录</p>
<h5 id="1-命令：-6">1) 命令：</h5>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop import-all-tables \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--warehouse-dir /all_tables</span></span><br></pre></td></tr></table></figure>
<h5 id="2-参数">2) 参数</h5>
<p>这些参数的含义均和 import 对应的含义一致</p>
<table>
<thead>
<tr>
<th style="text-align:left">序号</th>
<th style="text-align:left">参数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">–as-avrodatafile</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">–as-sequencefile</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">–as-textfile</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td style="text-align:left">–direct</td>
</tr>
<tr>
<td style="text-align:left">5</td>
<td style="text-align:left">–direct-split-size <n></td>
</tr>
<tr>
<td style="text-align:left">6</td>
<td style="text-align:left">–inline-lob-limit <n></td>
</tr>
<tr>
<td style="text-align:left">7</td>
<td style="text-align:left">–m 或—num-mappers <n></td>
</tr>
<tr>
<td style="text-align:left">8</td>
<td style="text-align:left">–warehouse-dir <dir></td>
</tr>
<tr>
<td style="text-align:left">9</td>
<td style="text-align:left">-z 或–compress</td>
</tr>
<tr>
<td style="text-align:left">10</td>
<td style="text-align:left">–compression-codec</td>
</tr>
</tbody>
</table>
<h4 id="命令-参数：job">命令&amp;参数：job</h4>
<p>用来生成一个 sqoop 任务，生成后不会立即执行，需要手动执行。</p>
<h5 id="1-命令：-7">1) 命令：</h5>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop job \</span></span><br><span class="line"><span class="language-bash">--create myjob -- import-all-tables \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000</span></span><br><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop job \</span></span><br><span class="line"><span class="language-bash">--list</span></span><br><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop job \</span></span><br><span class="line"><span class="language-bash">--<span class="built_in">exec</span> myjob</span></span><br></pre></td></tr></table></figure>
<p>尖叫提示：注意 import-all-tables 和它左边的–之间有一个空格尖叫提示：如果需要连接 metastore，则–meta-connect jdbc:hsqldb:hsql://linux01:16000/sqoop</p>
<h5 id="2-参数-2">2) 参数</h5>
<table>
<thead>
<tr>
<th style="text-align:left">序号</th>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">–create <job-id></td>
<td style="text-align:left">创建 job 参数</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">–delete <job-id></td>
<td style="text-align:left">删除一个 job</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">–exec <job-id></td>
<td style="text-align:left">执行一个 job</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td style="text-align:left">–help</td>
<td style="text-align:left">显示 job 帮助</td>
</tr>
<tr>
<td style="text-align:left">5</td>
<td style="text-align:left">–list</td>
<td style="text-align:left">显示 job 列表</td>
</tr>
<tr>
<td style="text-align:left">6</td>
<td style="text-align:left">–meta-connect <jdbc-uri></td>
<td style="text-align:left">用来连接 metastore 服务</td>
</tr>
<tr>
<td style="text-align:left">7</td>
<td style="text-align:left">–show <job-id></td>
<td style="text-align:left">显示一个 job 的信息</td>
</tr>
<tr>
<td style="text-align:left">8</td>
<td style="text-align:left">–verbose</td>
<td style="text-align:left">打印命令运行时的详细信息</td>
</tr>
<tr>
<td style="text-align:left">尖叫提示：在执行一个 job 时，如果需要手动输入数据库密码，可以做如下优化</td>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>sqoop.metastore.client.record.password<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, allow saved passwords in the metastore.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="命令-参数：list-databases">命令&amp;参数：list-databases</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop list-databases \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/ \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000</span></span><br></pre></td></tr></table></figure>
<h4 id="命令-参数：list-tables">命令&amp;参数：list-tables</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop list-tables \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000</span></span><br></pre></td></tr></table></figure>
<h4 id="命令-参数：merge">命令&amp;参数：merge</h4>
<p>将 HDFS 中不同目录下面的数据合并在一起并放入指定目录中</p>
<h5 id="数据环境：">数据环境：</h5>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">new_staff</span><br><span class="line">1 AAA male</span><br><span class="line">2 BBB male</span><br><span class="line">3 CCC male</span><br><span class="line">4 DDD male</span><br><span class="line">old_staff</span><br><span class="line">1 AAA female</span><br><span class="line">2 CCC female</span><br><span class="line">3 BBB female</span><br><span class="line">6 DDD female</span><br></pre></td></tr></table></figure>
<p>尖叫提示：上边数据的列之间的分隔符应该为\t，行与行之间的分割符为\n，如果直接复制，请检查之。</p>
<h5 id="命令：-2">命令：</h5>
<p>创建 JavaBean：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop codegen \</span></span><br><span class="line"><span class="language-bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="language-bash">--username root \</span></span><br><span class="line"><span class="language-bash">--password 000000 \</span></span><br><span class="line"><span class="language-bash">--table staff \</span></span><br><span class="line"><span class="language-bash">--bindir /home/admin/Desktop/staff \</span></span><br><span class="line"><span class="language-bash">--class-name Staff \</span></span><br><span class="line"><span class="language-bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span></span></span><br></pre></td></tr></table></figure>
<p>开始合并：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop merge \</span></span><br><span class="line"><span class="language-bash">--new-data /test/new/ \</span></span><br><span class="line"><span class="language-bash">--onto /test/old/ \</span></span><br><span class="line"><span class="language-bash">--target-dir /test/merged \</span></span><br><span class="line"><span class="language-bash">--jar-file /home/admin/Desktop/staff/Staff.jar \</span></span><br><span class="line"><span class="language-bash">--class-name Staff \</span></span><br><span class="line"><span class="language-bash">--merge-key <span class="built_in">id</span></span></span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1 AAA MALE</span><br><span class="line">2 BBB MALE</span><br><span class="line">3 CCC MALE</span><br><span class="line">4 DDD MALE</span><br><span class="line">6 DDD FEMALE</span><br></pre></td></tr></table></figure>
<h5 id="参数：">参数：</h5>
<table>
<thead>
<tr>
<th style="text-align:left">序号</th>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">–new-data <path></td>
<td style="text-align:left">HDFS 待合并的数据目录，合并后在新的数据集中保留</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">–onto <path></td>
<td style="text-align:left">HDFS 合并后，重复的部分在新的数据集中被覆盖</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">–merge-key <col></td>
<td style="text-align:left">合并键，一般是主键 ID</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td style="text-align:left">–jar-file <file></td>
<td style="text-align:left">合并时引入的 jar 包，该 jar包是通过 Codegen 工具生成的 jar 包</td>
</tr>
<tr>
<td style="text-align:left">5</td>
<td style="text-align:left">–class-name <class></td>
<td style="text-align:left">对应的表名或对象名，该class 类是包含在 jar 包中的</td>
</tr>
<tr>
<td style="text-align:left">6</td>
<td style="text-align:left">–target-dir <path></td>
<td style="text-align:left">合并后的数据在 HDFS 里存放的目录</td>
</tr>
</tbody>
</table>
<h4 id="命令-参数：metastore">命令&amp;参数：metastore</h4>
<p>记录了 Sqoop job 的元数据信息，如果不启动该服务，那么默认 job 元数据的存储目录为<br>
~/.sqoop，可在 sqoop-site.xml 中修改。启动 sqoop 的 metastore 服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop metastore</span></span><br></pre></td></tr></table></figure>
<p>关闭 sqoop 的 metastore 服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">bin/sqoop metastore --shutdown</span>  </span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据组件</category>
        <category>sqoop</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/10/15/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start">Quick Start</h2>
<h3 id="Create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Hive常用查询函数</title>
    <url>/2021/10/15/hive%E5%B8%B8%E7%94%A8%E6%9F%A5%E8%AF%A2%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h2 id="1-空字段赋值NVL函数">1 空字段赋值NVL函数</h2>
<p>1.函数说明<br>
  NVL：给值为NULL的数据赋值，它的格式是NVL( string1, replace_with)。它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL ，则返回NULL。</p>
<p>2.数据准备：采用员工表</p>
<p>3.查询：如果员工的comm为NULL，则用-1代替</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select nvl(comm,-1) from emp;</span><br></pre></td></tr></table></figure>
<p>4.查询：如果员工的comm为NULL，则用领导id代替</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select nvl(comm,mgr) from emp;</span><br></pre></td></tr></table></figure>
<h2 id="2-CASE-WHEN函数">2.CASE WHEN函数</h2>
<ol>
<li class="lvl-3">
<p>数据准备</p>
</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:center">name</th>
<th style="text-align:center">dept_id</th>
<th style="text-align:center">sex</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">悟空</td>
<td style="text-align:center">A</td>
<td style="text-align:center">男</td>
</tr>
<tr>
<td style="text-align:center">大海</td>
<td style="text-align:center">A</td>
<td style="text-align:center">男</td>
</tr>
<tr>
<td style="text-align:center">宋宋</td>
<td style="text-align:center">B</td>
<td style="text-align:center">男</td>
</tr>
<tr>
<td style="text-align:center">凤姐</td>
<td style="text-align:center">A</td>
<td style="text-align:center">女</td>
</tr>
<tr>
<td style="text-align:center">婷姐</td>
<td style="text-align:center">B</td>
<td style="text-align:center">女</td>
</tr>
<tr>
<td style="text-align:center">婷婷</td>
<td style="text-align:center">B</td>
<td style="text-align:center">女</td>
</tr>
</tbody>
</table>
<p>2．需求求出不同部门男女各多少人。结果如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A     2       1</span><br><span class="line">B     1       2</span><br></pre></td></tr></table></figure>
<p>3．创建本地emp_sex.txt，导入数据<br>
[ihadu@hadoop102 datas]$ vi emp_sex.txt<br>
悟空	A	男大海	A	男宋宋	B	男凤姐	A	女婷姐	B	女婷婷	B	女</p>
<p>4．创建hive表并导入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp_sex(</span><br><span class="line">name string,</span><br><span class="line">dept_id string,</span><br><span class="line">sex string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/emp_sex.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> emp_sex;</span><br></pre></td></tr></table></figure>
<p>5．按需求查询数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  dept_id,</span><br><span class="line">  <span class="built_in">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">&#x27;男&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) male_count,</span><br><span class="line">  <span class="built_in">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">&#x27;女&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) female_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  emp_sex</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  dept_id;</span><br></pre></td></tr></table></figure>
<h2 id="3-行转列">3.行转列</h2>
<p>1．相关函数说明<br>
  CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;<br>
  CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;<br>
  COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</p>
<p>2．数据准备</p>
<table>
<thead>
<tr>
<th style="text-align:center">name</th>
<th style="text-align:center">constellation</th>
<th style="text-align:center">blood_type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">孙悟空</td>
<td style="text-align:center">白羊座</td>
<td style="text-align:center">A</td>
</tr>
<tr>
<td style="text-align:center">大海</td>
<td style="text-align:center">射手座</td>
<td style="text-align:center">A</td>
</tr>
<tr>
<td style="text-align:center">宋宋</td>
<td style="text-align:center">白羊座</td>
<td style="text-align:center">B</td>
</tr>
<tr>
<td style="text-align:center">猪八戒</td>
<td style="text-align:center">白羊座</td>
<td style="text-align:center">A</td>
</tr>
<tr>
<td style="text-align:center">凤姐</td>
<td style="text-align:center">射手座</td>
<td style="text-align:center">A</td>
</tr>
</tbody>
</table>
<p>3．需求把星座和血型一样的人归类到一起。结果如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">射手座,A            大海|凤姐</span><br><span class="line">白羊座,A            孙悟空|猪八戒</span><br><span class="line">白羊座,B            宋宋</span><br></pre></td></tr></table></figure>
<p>4．创建本地constellation.txt，导入数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[ihadu@hadoop102 datas]$ vi person_info.txt</span><br><span class="line">孙悟空	白羊座	A</span><br><span class="line">大海	 射手座	A</span><br><span class="line">宋宋	 白羊座	B</span><br><span class="line">猪八戒   白羊座	A</span><br><span class="line">凤姐	  射手座	A</span><br></pre></td></tr></table></figure>
<p>5．创建hive表并导入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> person_info(</span><br><span class="line">name string,</span><br><span class="line">constellation string,</span><br><span class="line">blood_type string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line">load data <span class="keyword">local</span> inpath “<span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>datas<span class="operator">/</span>person_info.txt” <span class="keyword">into</span> <span class="keyword">table</span> person_info;</span><br></pre></td></tr></table></figure>
<p>6．按需求查询数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    t1.base,</span><br><span class="line">    concat_ws(<span class="string">&#x27;|&#x27;</span>, collect_set(t1.name)) name</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span></span><br><span class="line">        name,</span><br><span class="line">        concat(constellation, &quot;,&quot;, blood_type) base</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">        person_info) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    t1.base;</span><br></pre></td></tr></table></figure>
<h2 id="4-列转行">4.列转行</h2>
<p>1．函数说明<br>
Explode(col)：将hive一列中复杂的array或者map结构拆分成多行。</p>
<p>Lateral view<br>
用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias<br>
解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p>
<p>2．数据准备</p>
<table>
<thead>
<tr>
<th style="text-align:center">movie</th>
<th style="text-align:center">category</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">《金刚川》</td>
<td style="text-align:center">悬疑,动作,科幻,剧情</td>
</tr>
<tr>
<td style="text-align:center">《我和我的家乡》</td>
<td style="text-align:center">悬疑,警匪,动作,心理,剧情</td>
</tr>
<tr>
<td style="text-align:center">《心灵奇旅》</td>
<td style="text-align:center">战争,动作,灾难</td>
</tr>
</tbody>
</table>
<p>3．需求将电影分类中的数组数据展开。结果如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">《金刚川》      悬疑</span><br><span class="line">《金刚川》      动作</span><br><span class="line">《金刚川》      科幻</span><br><span class="line">《金刚川》      剧情</span><br><span class="line">《我和我的家乡》   悬疑</span><br><span class="line">《我和我的家乡》   警匪</span><br><span class="line">《我和我的家乡》   动作</span><br><span class="line">《我和我的家乡》   心理</span><br><span class="line">《我和我的家乡》   剧情</span><br><span class="line">《心灵奇旅》      战争</span><br><span class="line">《心灵奇旅》      动作</span><br><span class="line">《心灵奇旅》      灾难</span><br></pre></td></tr></table></figure>
<p>4．创建本地movie.txt，导入数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[ihadu@hadoop102 datas]$ vi movie.txt</span><br><span class="line">《疑犯追踪》	悬疑,动作,科幻,剧情</span><br><span class="line">《Lie to me》	悬疑,警匪,动作,心理,剧情</span><br><span class="line">《战狼2》	战争,动作,灾难</span><br></pre></td></tr></table></figure>
<p>5．创建hive表并导入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> movie_info(</span><br><span class="line">    movie string,</span><br><span class="line">    category <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;</span><br><span class="line">collection items terminated <span class="keyword">by</span> &quot;,&quot;;</span><br><span class="line"></span><br><span class="line">load data <span class="keyword">local</span> inpath &quot;/opt/module/datas/movie.txt&quot; <span class="keyword">into</span> <span class="keyword">table</span> movie_info;</span><br></pre></td></tr></table></figure>
<p>6．按需求查询数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    movie,</span><br><span class="line">    category_name</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> explode(category) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure>
<h2 id="5-窗口函数">5.窗口函数</h2>
<p>1．相关函数说明<br>
OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化<br>
CURRENT ROW：当前行<br>
n PRECEDING：往前n行数据<br>
n FOLLOWING：往后n行数据<br>
UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点<br>
LAG(col,n)：往前第n行数据<br>
LEAD(col,n)：往后第n行数据<br>
NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。</p>
<p>2．数据准备：name，orderdate，cost</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">jack,2017-01-01,10</span><br><span class="line">tony,2017-01-02,15</span><br><span class="line">jack,2017-02-03,23</span><br><span class="line">tony,2017-01-04,29</span><br><span class="line">jack,2017-01-05,46</span><br><span class="line">jack,2017-04-06,42</span><br><span class="line">tony,2017-01-07,50</span><br><span class="line">jack,2017-01-08,55</span><br><span class="line">mart,2017-04-08,62</span><br><span class="line">mart,2017-04-09,68</span><br><span class="line">neil,2017-05-10,12</span><br><span class="line">mart,2017-04-11,75</span><br><span class="line">neil,2017-06-12,80</span><br><span class="line">mart,2017-04-13,94</span><br></pre></td></tr></table></figure>
<p>3．需求（1）查询在2017年4月份购买过的顾客及总人数（2）查询顾客的购买明细及月购买总额（3）上述的场景,要将cost按照日期进行累加（4）查询顾客上次的购买时间（5）查询前20%时间的订单信息</p>
<p>4．创建本地business.txt，导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[ihadu@hadoop102 datas]$ vi business.txt</span><br></pre></td></tr></table></figure>
<p>5．创建hive表并导入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> business(</span><br><span class="line">name string,</span><br><span class="line">orderdate string,</span><br><span class="line">cost <span class="type">int</span></span><br><span class="line">) <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"></span><br><span class="line">load data <span class="keyword">local</span> inpath &quot;/opt/module/datas/business.txt&quot; <span class="keyword">into</span> <span class="keyword">table</span> business;</span><br></pre></td></tr></table></figure>
<p>6．按需求查询数据（1）查询在2017年4月份购买过的顾客及总人数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,<span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">over</span> ()</span><br><span class="line"><span class="keyword">from</span> business</span><br><span class="line"><span class="keyword">where</span> <span class="built_in">substring</span>(orderdate,<span class="number">1</span>,<span class="number">7</span>) <span class="operator">=</span> <span class="string">&#x27;2017-04&#x27;</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> name;</span><br></pre></td></tr></table></figure>
<p>（2）查询顾客的购买明细及月购买总额</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	name,</span><br><span class="line">	orderdate,</span><br><span class="line">	cost,</span><br><span class="line">	<span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">month</span>(orderdate))</span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure>
<p>（3）上述的场景,要将cost按照日期进行累加</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,orderdate,cost,</span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>() <span class="keyword">as</span> sample1,<span class="comment">--所有行相加</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name) <span class="keyword">as</span> sample2,<span class="comment">--按name分组，组内数据相加</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> sample3,<span class="comment">--按name分组，组内数据累加</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> UNBOUNDED PRECEDING <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span> ) <span class="keyword">as</span> sample4 ,<span class="comment">--和sample3一样,由起点到当前行的聚合</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> PRECEDING <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span>) <span class="keyword">as</span> sample5, <span class="comment">--当前行和前面一行做聚合</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> PRECEDING <span class="keyword">AND</span> <span class="number">1</span> FOLLOWING ) <span class="keyword">as</span> sample6,<span class="comment">--当前行和前边一行及后面一行</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="type">row</span> <span class="keyword">and</span> UNBOUNDED FOLLOWING ) <span class="keyword">as</span> sample7 <span class="comment">--当前行及后面所有行</span></span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure>
<p>（4）查看顾客上次的购买时间</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,orderdate,cost,</span><br><span class="line"><span class="built_in">lag</span>(orderdate,<span class="number">1</span>,<span class="string">&#x27;1900-01-01&#x27;</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate ) <span class="keyword">as</span> time1,</span><br><span class="line"><span class="built_in">lag</span>(orderdate,<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> time2</span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure>
<p>（5）查询前20%时间的订单信息</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> name,orderdate,cost, <span class="built_in">ntile</span>(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) sorted</span><br><span class="line">    <span class="keyword">from</span> business</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> sorted <span class="operator">=</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure>
<p>6.Rank函数<br>
1．函数说明</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>RANK() 排序相同时会重复，总数不会变</p>
</li>
<li class="lvl-2">
<p>DENSE_RANK() 排序相同时会重复，总数会减少</p>
</li>
<li class="lvl-2">
<p>ROW_NUMBER() 会根据顺序计算</p>
</li>
</ul>
<p>2．数据准备</p>
<table>
<thead>
<tr>
<th style="text-align:center">name</th>
<th style="text-align:center">subject</th>
<th style="text-align:center">score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">孙悟空</td>
<td style="text-align:center">语文</td>
<td style="text-align:center">87</td>
</tr>
<tr>
<td style="text-align:center">孙悟空</td>
<td style="text-align:center">数学</td>
<td style="text-align:center">95</td>
</tr>
<tr>
<td style="text-align:center">孙悟空</td>
<td style="text-align:center">英语</td>
<td style="text-align:center">68</td>
</tr>
<tr>
<td style="text-align:center">大海</td>
<td style="text-align:center">语文</td>
<td style="text-align:center">94</td>
</tr>
<tr>
<td style="text-align:center">大海</td>
<td style="text-align:center">数学</td>
<td style="text-align:center">56</td>
</tr>
<tr>
<td style="text-align:center">大海</td>
<td style="text-align:center">英语</td>
<td style="text-align:center">84</td>
</tr>
<tr>
<td style="text-align:center">宋宋</td>
<td style="text-align:center">语文</td>
<td style="text-align:center">64</td>
</tr>
<tr>
<td style="text-align:center">宋宋</td>
<td style="text-align:center">数学</td>
<td style="text-align:center">86</td>
</tr>
<tr>
<td style="text-align:center">宋宋</td>
<td style="text-align:center">英语</td>
<td style="text-align:center">84</td>
</tr>
<tr>
<td style="text-align:center">婷婷</td>
<td style="text-align:center">语文</td>
<td style="text-align:center">65</td>
</tr>
<tr>
<td style="text-align:center">婷婷</td>
<td style="text-align:center">数学</td>
<td style="text-align:center">85</td>
</tr>
<tr>
<td style="text-align:center">婷婷</td>
<td style="text-align:center">英语</td>
<td style="text-align:center">78</td>
</tr>
</tbody>
</table>
<p>3．需求计算每门学科成绩排名。</p>
<p>4．创建本地movie.txt，导入数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[ihadu@hadoop102 datas]$ vi score.txt</span><br></pre></td></tr></table></figure>
<p>5．创建hive表并导入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score(</span><br><span class="line">name string,</span><br><span class="line">subject string, </span><br><span class="line">score <span class="type">int</span>) </span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/score.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> score;</span><br></pre></td></tr></table></figure>
<p>6．按需求查询数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,</span><br><span class="line">subject,</span><br><span class="line">score,</span><br><span class="line"><span class="built_in">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) rp,</span><br><span class="line"><span class="built_in">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) drp,</span><br><span class="line"><span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) rmp</span><br><span class="line"><span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>
<p>查询结果：</p>
<pre><code class="language-shell">name    subject score   rp      drp     rmp
孙悟空  数学    95      1       1       1
宋宋    数学    86      2       2       2
婷婷    数学    85      3       3       3
大海    数学    56      4       4       4
宋宋    英语    84      1       1       1
大海    英语    84      1       1       2
婷婷    英语    78      3       2       3
孙悟空  英语    68      4       3       4
大海    语文    94      1       1       1
孙悟空  语文    87      2       2       2
婷婷    语文    65      3       3       3
宋宋    语文    64      4       4       4
</code></pre>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive的简介及核心概念</title>
    <url>/2021/10/15/hive%E7%9A%84%E7%AE%80%E4%BB%8B%E5%8F%8A%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h2 id="一、简介-2">一、简介</h2>
<p>Hive 是一个构建在 Hadoop 之上的数据仓库，它可以将结构化的数据文件映射成表，并提供类 SQL 查询功能，用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。</p>
<p><strong>特点</strong>：</p>
<ol>
<li class="lvl-3">
<p>简单、容易上手 (提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也能很好地进行大数据分析；</p>
</li>
<li class="lvl-3">
<p>灵活性高，可以自定义用户函数 (UDF) 和存储格式；</p>
</li>
<li class="lvl-3">
<p>为超大的数据集设计的计算和存储能力，集群扩展容易;</p>
</li>
<li class="lvl-3">
<p>统一的元数据管理，可与 presto／impala／sparksql 等共享数据；</p>
</li>
<li class="lvl-3">
<p>执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。</p>
</li>
</ol>
<h2 id="二、Hive的体系架构">二、Hive的体系架构</h2>
<div align="center"> <img width="600px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive体系架构.png"/> </div>
<h3 id="2-1-command-line-shell-thrift-jdbc">2.1 command-line shell &amp; thrift/jdbc</h3>
<p>可以用 command-line shell 和 thrift／jdbc 两种方式来操作数据：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>command-line shell</strong>：通过 hive 命令行的的方式来操作数据；</p>
</li>
<li class="lvl-2">
<p><strong>thrift／jdbc</strong>：通过 thrift 协议按照标准的 JDBC 的方式操作数据。</p>
</li>
</ul>
<h3 id="2-2-Metastore">2.2 Metastore</h3>
<p>在 Hive 中，表名、表结构、字段名、字段类型、表的分隔符等统一被称为元数据。所有的元数据默认存储在 Hive 内置的 derby 数据库中，但由于 derby 只能有一个实例，也就是说不能有多个命令行客户端同时访问，所以在实际生产环境中，通常使用 MySQL 代替 derby。</p>
<p>Hive 进行的是统一的元数据管理，就是说你在 Hive 上创建了一张表，然后在 presto／impala／sparksql 中都是可以直接使用的，它们会从 Metastore 中获取统一的元数据信息，同样的你在 presto／impala／sparksql 中创建一张表，在 Hive 中也可以直接使用。</p>
<h3 id="2-3-HQL的执行流程">2.3 HQL的执行流程</h3>
<p>Hive 在执行一条 HQL 的时候，会经过以下步骤：</p>
<ol>
<li class="lvl-3">
<p>语法解析：Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象 语法树 AST Tree；</p>
</li>
<li class="lvl-3">
<p>语义解析：遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock；</p>
</li>
<li class="lvl-3">
<p>生成逻辑执行计划：遍历 QueryBlock，翻译为执行操作树 OperatorTree；</p>
</li>
<li class="lvl-3">
<p>优化逻辑执行计划：逻辑层优化器进行 OperatorTree 变换，合并不必要的 ReduceSinkOperator，减少 shuffle 数据量；</p>
</li>
<li class="lvl-3">
<p>生成物理执行计划：遍历 OperatorTree，翻译为 MapReduce 任务；</p>
</li>
<li class="lvl-3">
<p>优化物理执行计划：物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划。</p>
</li>
</ol>
<blockquote>
<p>关于 Hive SQL 的详细执行流程可以参考美团技术团队的文章：<a href="https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html">Hive SQL 的编译过程</a></p>
</blockquote>
<h2 id="三、数据类型">三、数据类型</h2>
<h3 id="3-1-基本数据类型">3.1 基本数据类型</h3>
<p>Hive 表中的列支持以下基本数据类型：</p>
<table>
<thead>
<tr>
<th>大类</th>
<th>类型</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Integers（整型）</strong></td>
<td>TINYINT—1 字节的有符号整数 <br/>SMALLINT—2 字节的有符号整数<br/> INT—4 字节的有符号整数<br/> BIGINT—8 字节的有符号整数</td>
</tr>
<tr>
<td><strong>Boolean（布尔型）</strong></td>
<td>BOOLEAN—TRUE/FALSE</td>
</tr>
<tr>
<td><strong>Floating point numbers（浮点型）</strong></td>
<td>FLOAT— 单精度浮点型 <br/>DOUBLE—双精度浮点型</td>
</tr>
<tr>
<td><strong>Fixed point numbers（定点数）</strong></td>
<td>DECIMAL—用户自定义精度定点数，比如 DECIMAL(7,2)</td>
</tr>
<tr>
<td><strong>String types（字符串）</strong></td>
<td>STRING—指定字符集的字符序列<br/> VARCHAR—具有最大长度限制的字符序列 <br/>CHAR—固定长度的字符序列</td>
</tr>
<tr>
<td><strong>Date and time types（日期时间类型）</strong></td>
<td>TIMESTAMP —  时间戳 <br/>TIMESTAMP WITH LOCAL TIME ZONE — 时间戳，纳秒精度<br/> DATE—日期类型</td>
</tr>
<tr>
<td><strong>Binary types（二进制类型）</strong></td>
<td>BINARY—字节序列</td>
</tr>
</tbody>
</table>
<blockquote>
<p>TIMESTAMP 和 TIMESTAMP WITH LOCAL TIME ZONE 的区别如下：</p>
<ul class="lvl-1">
<li class="lvl-2"><strong>TIMESTAMP WITH LOCAL TIME ZONE</strong>：用户提交时间给数据库时，会被转换成数据库所在的时区来保存。查询时则按照查询客户端的不同，转换为查询客户端所在时区的时间。</li>
<li class="lvl-2"><strong>TIMESTAMP</strong> ：提交什么时间就保存什么时间，查询时也不做任何转换。</li>
</ul>
</blockquote>
<h3 id="3-2-隐式转换">3.2 隐式转换</h3>
<p>Hive 中基本数据类型遵循以下的层次结构，按照这个层次结构，子类型到祖先类型允许隐式转换。例如 INT 类型的数据允许隐式转换为 BIGINT 类型。额外注意的是：按照类型层次结构允许将 STRING 类型隐式转换为 DOUBLE 类型。</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hive-data-type.png"/> </div>
<h3 id="3-3-复杂类型">3.3 复杂类型</h3>
<table>
<thead>
<tr>
<th>类型</th>
<th>描述</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>STRUCT</strong></td>
<td>类似于对象，是字段的集合，字段的类型可以不同，可以使用 <code>名称.字段名</code> 方式进行访问</td>
<td>STRUCT (‘xiaoming’, 12 , ‘2018-12-12’)</td>
</tr>
<tr>
<td><strong>MAP</strong></td>
<td>键值对的集合，可以使用 <code> 名称[key]</code> 的方式访问对应的值</td>
<td>map(‘a’, 1, ‘b’, 2)</td>
</tr>
<tr>
<td><strong>ARRAY</strong></td>
<td>数组是一组具有相同类型和名称的变量的集合，可以使用 <code> 名称[index]</code> 访问对应的值</td>
<td>ARRAY(‘a’, ‘b’, ‘c’, ‘d’)</td>
</tr>
</tbody>
</table>
<h3 id="3-4-示例">3.4 示例</h3>
<p>如下给出一个基本数据类型和复杂数据类型的使用示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> students(</span><br><span class="line">  name      STRING,   <span class="comment">-- 姓名</span></span><br><span class="line">  age       <span class="type">INT</span>,      <span class="comment">-- 年龄</span></span><br><span class="line">  subject   <span class="keyword">ARRAY</span><span class="operator">&lt;</span>STRING<span class="operator">&gt;</span>,   <span class="comment">--学科</span></span><br><span class="line">  score     MAP<span class="operator">&lt;</span>STRING,<span class="type">FLOAT</span><span class="operator">&gt;</span>,  <span class="comment">--各个学科考试成绩</span></span><br><span class="line">  address   STRUCT<span class="operator">&lt;</span>houseNumber:<span class="type">int</span>, street:STRING, city:STRING, province：STRING<span class="operator">&gt;</span>  <span class="comment">--家庭居住地址</span></span><br><span class="line">) <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br></pre></td></tr></table></figure>
<h2 id="四、内容格式">四、内容格式</h2>
<p>当数据存储在文本文件中，必须按照一定格式区别行和列，如使用逗号作为分隔符的 CSV 文件 (Comma-Separated Values) 或者使用制表符作为分隔值的 TSV 文件 (Tab-Separated Values)。但此时也存在一个缺点，就是正常的文件内容中也可能出现逗号或者制表符。</p>
<p>所以 Hive 默认使用了几个平时很少出现的字符，这些字符一般不会作为内容出现在文件中。Hive 默认的行和列分隔符如下表所示。</p>
<table>
<thead>
<tr>
<th>分隔符</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>\n</strong></td>
<td>对于文本文件来说，每行是一条记录，所以可以使用换行符来分割记录</td>
</tr>
<tr>
<td><strong>^A (Ctrl+A)</strong></td>
<td>分割字段 (列)，在 CREATE TABLE 语句中也可以使用八进制编码 <code>\001</code> 来表示</td>
</tr>
<tr>
<td><strong>^B</strong></td>
<td>用于分割 ARRAY 或者 STRUCT 中的元素，或者用于 MAP 中键值对之间的分割，<br/>在 CREATE TABLE 语句中也可以使用八进制编码 <code>\002</code> 表示</td>
</tr>
<tr>
<td><strong>^C</strong></td>
<td>用于 MAP 中键和值之间的分割，在 CREATE TABLE 语句中也可以使用八进制编码 <code>\003</code> 表示</td>
</tr>
</tbody>
</table>
<p>使用示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="type">INT</span>, userid <span class="type">BIGINT</span>)</span><br><span class="line"> <span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">   FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\001&#x27;</span></span><br><span class="line">   COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\002&#x27;</span></span><br><span class="line">   MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\003&#x27;</span></span><br><span class="line"> STORED <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure>
<h2 id="五、存储格式">五、存储格式</h2>
<h3 id="5-1-支持的存储格式">5.1 支持的存储格式</h3>
<p>Hive 会在 HDFS 为每个数据库上创建一个目录，数据库中的表是该目录的子目录，表中的数据会以文件的形式存储在对应的表目录下。Hive 支持以下几种文件存储格式：</p>
<table>
<thead>
<tr>
<th>格式</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TextFile</strong></td>
<td>存储为纯文本文件。 这是 Hive 默认的文件存储格式。这种存储方式数据不做压缩，磁盘开销大，数据解析开销大。</td>
</tr>
<tr>
<td><strong>SequenceFile</strong></td>
<td>SequenceFile 是 Hadoop API 提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中。这种二进制文件内部使用 Hadoop 的标准的 Writable 接口实现序列化和反序列化。它与 Hadoop API 中的 MapFile 是互相兼容的。Hive 中的 SequenceFile 继承自 Hadoop API 的 SequenceFile，不过它的 key 为空，使用 value 存放实际的值，这样是为了避免 MR 在运行 map 阶段进行额外的排序操作。</td>
</tr>
<tr>
<td><strong>RCFile</strong></td>
<td>RCFile 文件格式是 FaceBook 开源的一种 Hive 的文件存储格式，首先将表分为几个行组，对每个行组内的数据按列存储，每一列的数据都是分开存储。</td>
</tr>
<tr>
<td><strong>ORC Files</strong></td>
<td>ORC 是在一定程度上扩展了 RCFile，是对 RCFile 的优化。</td>
</tr>
<tr>
<td><strong>Avro Files</strong></td>
<td>Avro 是一个数据序列化系统，设计用于支持大批量数据交换的应用。它的主要特点有：支持二进制序列化方式，可以便捷，快速地处理大量数据；动态语言友好，Avro 提供的机制使动态语言可以方便地处理 Avro 数据。</td>
</tr>
<tr>
<td><strong>Parquet</strong></td>
<td>Parquet 是基于 Dremel 的数据模型和算法实现的，面向分析型业务的列式存储格式。它通过按列进行高效压缩和特殊的编码技术，从而在降低存储空间的同时提高了 IO 效率。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>以上压缩格式中 ORC 和 Parquet 的综合性能突出，使用较为广泛，推荐使用这两种格式。</p>
</blockquote>
<h3 id="5-2-指定存储格式">5.2 指定存储格式</h3>
<p>通常在创建表的时候使用 <code>STORED AS</code> 参数指定：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="type">INT</span>, userid <span class="type">BIGINT</span>)</span><br><span class="line"> <span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">   FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\001&#x27;</span></span><br><span class="line">   COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\002&#x27;</span></span><br><span class="line">   MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\003&#x27;</span></span><br><span class="line"> STORED <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure>
<p>各个存储文件类型指定方式如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>STORED AS TEXTFILE</p>
</li>
<li class="lvl-2">
<p>STORED AS SEQUENCEFILE</p>
</li>
<li class="lvl-2">
<p>STORED AS ORC</p>
</li>
<li class="lvl-2">
<p>STORED AS PARQUET</p>
</li>
<li class="lvl-2">
<p>STORED AS AVRO</p>
</li>
<li class="lvl-2">
<p>STORED AS RCFILE</p>
</li>
</ul>
<h2 id="六、内部表和外部表">六、内部表和外部表</h2>
<p>内部表又叫做管理表 (Managed/Internal Table)，创建表时不做任何指定，默认创建的就是内部表。想要创建外部表 (External Table)，则需要使用 External 进行修饰。 内部表和外部表主要区别如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>内部表</th>
<th>外部表</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据存储位置</td>
<td>内部表数据存储的位置由 hive.metastore.warehouse.dir 参数指定，默认情况下表的数据存储在 HDFS 的 <code>/user/hive/warehouse/数据库名.db/表名/</code>  目录下</td>
<td>外部表数据的存储位置创建表时由 <code>Location</code> 参数指定；</td>
</tr>
<tr>
<td>导入数据</td>
<td>在导入数据到内部表，内部表将数据移动到自己的数据仓库目录下，数据的生命周期由 Hive 来进行管理</td>
<td>外部表不会将数据移动到自己的数据仓库目录下，只是在元数据中存储了数据的位置</td>
</tr>
<tr>
<td>删除表</td>
<td>删除元数据（metadata）和文件</td>
<td>只删除元数据（metadata）</td>
</tr>
</tbody>
</table>
<h2 id="参考资料-20">参考资料</h2>
<ol>
<li class="lvl-3">
<p><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">Hive Getting Started</a></p>
</li>
<li class="lvl-3">
<p><a href="https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html">Hive SQL 的编译过程</a></p>
</li>
<li class="lvl-3">
<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL">LanguageManual DDL</a></p>
</li>
<li class="lvl-3">
<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types">LanguageManual Types</a></p>
</li>
<li class="lvl-3">
<p><a href="https://cwiki.apache.org/confluence/display/Hive/Managed+vs.+External+Tables">Managed vs. External Tables</a>                            |</p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka本地安装</title>
    <url>/2023/01/03/kafka%E6%9C%AC%E5%9C%B0%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h3 id="安装zookeeper">安装zookeeper</h3>
<p>地址：<a href="http://apache.org/dist/zookeeper/zookeeper-3.4.14/">zookeeper-3.4.14</a>，下载后解压</p>
<ol>
<li class="lvl-3">
<p>进入zookeeper的相关设置所在的文件目录，例如本文的：D: \ …\zookeeper-3.4.10\conf</p>
</li>
<li class="lvl-3">
<p>将&quot;zoo_sample.cfg&quot;重命名为&quot;zoo.cfg&quot;</p>
</li>
<li class="lvl-3">
<p>dataDir=/tmp/zookeeper  to  D:/ …/zookeeper-3.4.10/data</p>
</li>
<li class="lvl-3">
<p>添加系统环境变量</p>
<ul class="lvl-2">
<li class="lvl-6">
<p>系统变量中添加ZOOKEEPER_HOME=D:\ …\zookeeper-3.4.10</p>
</li>
<li class="lvl-6">
<p>编辑系统变量中的path变量，增加%ZOOKEEPER_HOME%\bin</p>
</li>
</ul>
</li>
</ol>
<h3 id="安装kafka">安装kafka</h3>
<p>地址: <a href="http://apache.org/dist/kafka/2.1.1/">kafka-2.11_2.1.1</a>，下载后解压</p>
<p>同样加入到环境变量中</p>
<h3 id="启动kafka">启动kafka</h3>
<ol>
<li class="lvl-3">
<p>启动zookeeper</p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">zkserver.cmd</span><br></pre></td></tr></table></figure>
<ol start="2">
<li class="lvl-3">
<p>启动kafka</p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">.\bin\windows\kafka-server-start.bat .\config\server.properties</span><br></pre></td></tr></table></figure>
<ol start="3">
<li class="lvl-3">
<p>申请生产者</p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">.\bin\windows\kafka-console-producer.bat --broker-list localhost:<span class="number">9092</span> --topic test</span><br></pre></td></tr></table></figure>
<ol start="4">
<li class="lvl-3">
<p>申请消费者</p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">.\bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:<span class="number">9092</span> --topic test --from-beginning</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive自定义UDF函数</title>
    <url>/2021/10/13/hive%E8%87%AA%E5%AE%9A%E4%B9%89UDF%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h3 id="1-Hive自定义函数介绍">1. Hive自定义函数介绍</h3>
<p>当Hive提供的内置函数无法满足你的业务处理需要时，此时可以考虑使用用户自定义函数（UDF: user-defined function）。<br>
Hive 中常用的UDF有如下三种:</p>
<ul class="lvl-0">
<li class="lvl-2">UDF<br>
一条记录使用函数后输出还是一条记录，比如:upper/substr;</li>
<li class="lvl-2">UDAF(User-Defined Aggregation Funcation)<br>
多条记录使用函数后输出还是一条记录，比如: count/max/min/sum/avg;</li>
<li class="lvl-2">UDTF(User-Defined Table-Generating Functions)<br>
一条记录使用函数后输出多条记录，比如: lateral view explore();</li>
</ul>
<h3 id="2-Hive自定义函数开发">2. Hive自定义函数开发</h3>
<p>需求:开发自定义函数，使得在指定字段前加上“Hello:”字样。Hive 中 UDF函数开发步骤:<br>
(1）继承UDF 类。<br>
(2）重写evaluate方法，该方法支持重载，每行记录执行一次evaluate方法。</p>
<h4 id="注意：">##### 注意：</h4>
<p>1 UDF必须要有返回值,可以是null,但是不能为 void.<br>
2 推荐使用 Text/LongWritable等Hadoop的类型,而不是Java类型(当然使用 Java类型也是可以的)。</p>
<p>功能实现:</p>
<h5 id="1-pom-xml中添加UDF函数开发的依赖包。">( 1)pom.xml中添加UDF函数开发的依赖包。</h5>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.6.0-cdh5.7.0<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hive.version</span>&gt;</span>1.1.0-cdh5.7.0<span class="tag">&lt;/<span class="name">hive.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--CDH 版本建议大家添加一个repository--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--Hadoop依赖--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupld</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactld</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>$ &#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--Hive依赖--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupld</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupld</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>$ &#123; hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupld</span>&gt;</span>org.apacne.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactld</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>$ &#123; hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="2）开发UDF函数。">(2）开发UDF函数。</h5>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kgc.bigdata.hadoop.hive;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*功能:输入xxx，输出:Hello: xxx</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">*开发UDF 函数的步骤</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment">* 1) extends UDF</span></span><br><span class="line"><span class="comment">*2）重写evaluate方法，注意该方法是支持重载的</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HelloUDF</span> <span class="keyword">extends</span> <span class="title class_">UDF</span>&#123;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*对于UDF 函数的evaluate的参数和返回值，个人建议使用Writable* <span class="doctag">@param</span> name</span></span><br><span class="line"><span class="comment">* <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> Text <span class="title function_">evaluate</span><span class="params">(Text name)</span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Text</span>(<span class="string">&quot;Hello: &quot;</span> + name);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">public</span> Text <span class="title function_">evaluate</span><span class="params">(Text name,IntWritable age)</span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Text</span>(<span class="string">&quot;Hello: &quot;</span> +name + <span class="string">&quot; , age :&quot;</span> + age);</span><br><span class="line">    &#125;</span><br><span class="line">/功能测试</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span>&#123;</span><br><span class="line"><span class="type">HelloUDF</span> <span class="variable">udf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HelloUDF</span>(</span><br><span class="line">System.out.println(udf.evaluate(<span class="keyword">new</span> <span class="title class_">Text</span>(<span class="string">&quot;zhangsan&quot;</span>)));</span><br><span class="line">System.out.println(udf.evaluate(<span class="keyword">new</span> <span class="title class_">Text</span>(<span class="string">&quot;zhangsan&quot;</span>), <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">20</span>)));</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(3）编译jar包上传到服务器。<br>
(4)将自定义UDF 函数添加到Hive 中去。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">add JAR/home/hadoop/lib/hive-1.0.jar;</span><br><span class="line">create temporary function sayHello as &#x27;com.kgc.bigdata.hadoop.hive.HelloUDF&#x27;;</span><br></pre></td></tr></table></figure>
<p>(5)使用自定义函数。<br>
//通过show functions可以看到我们自定义的sayHello函数show functions;<br>
//将员工表的ename作为自定义UDF函数的参数值，即可查看到最终的输出结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> empno, ename, sayHello(ename) <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka分区</title>
    <url>/2021/10/13/kafka%E5%88%86%E5%8C%BA/</url>
    <content><![CDATA[<h2 id="设置topic下的分区数">设置topic下的分区数</h2>
<ol>
<li class="lvl-3">在 config/server.properties 配置文件中, 可以设置一个全局的分区数量, 这个分区数量的含义是: <strong>每个主题下的分区数量</strong>, 默认为 1</li>
</ol>
<img src="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230058746-945280798.png" referrerpolicy="no-referrer">
<ol start="2">
<li class="lvl-3">
<p>也可以在创建主题的时候, 使用 --partitions 参数指定分区数量</p>
</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic my_topic --partitions 2 --replication-factor 1 </span><br></pre></td></tr></table></figure>
<p>3.查看已创建主题的分区数量:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my_topic </span><br></pre></td></tr></table></figure>
<img src="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230212246-1072040750.png" referrerpolicy="no-referrer">
<h2 id="生产者与分区">生产者与分区</h2>
<p><strong>org.apache.kafka.clients.producer.internals.DefaultPartitioner</strong></p>
<p>默认的分区策略是：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>如果在发消息的时候指定了分区，则消息投递到指定的分区</p>
</li>
<li class="lvl-2">
<p>如果没有指定分区，但是消息的key不为空，则基于key的哈希值来选择一个分区</p>
</li>
<li class="lvl-2">
<p>如果既没有指定分区，且消息的key也是空，则用轮询的方式选择一个分区</p>
</li>
</ul>
<h2 id="消费者与分区">消费者与分区</h2>
<p>首先需要了解的是:</p>
<ol>
<li class="lvl-3">
<p>消费者是以组的名义订阅主题消息, 消费者组里边包含多个消费者实例.</p>
</li>
<li class="lvl-3">
<p>主题下边包含多个分区</p>
</li>
</ol>
<p>消费者实例与主题下分区的分配关系</p>
<img src="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230325655-792813000.png" referrerpolicy="no-referrer">
<p>kafka 集群上有两个节点, 4 个分区</p>
<p>A组有 2 个消费者实例 (两个消费线程)</p>
<p>B组有 4 个消费者实例</p>
<p>由图可以看出, A组的消费者C1, C2 平均要消费两个分区的数据, 而 B 组的消费者平均消费 一 个分区的数据 ( 最理想的状态 ), 得到的结论是 : <strong>一条消息只能被一个消费组中的一个消费者实例消费到</strong>, (换句话说, 不可能出现组中的两个消费者负责同一个分区, 同组内消费者不会重复消费 )</p>
<p>等等, 考虑的场景还不够, 下边再提些问题 :</p>
<p>如果分区数大于或等于组中的消费者实例数, 那就没有问题, 但是如果消费者实例的数量 &gt; 主题下分区数量, 那么按照默认的策略 ( 之所以强调默认策略是因为可以自定义策略 ), 有一些消费者是多余的, 一直接不到消息而处于空闲状态.</p>
<p>但是假设有消费者实例就是不安分, 就造成了多个消费者负责同一个分区, 这样会造成什么 ? (重复消费就太可怕了)</p>
<p>我们知道，Kafka它在设计的时候就是要保证分区下消息的顺序，也就是说消息在一个分区中的顺序是怎样的，那么消费者在消费的时候看到的就是什么样的顺序，那么要做到这一点就首先要保证消息是由消费者主动拉取的（pull），其次还要保证一个分区只能由一个消费者负责。倘若，两个消费者负责同一个分区，那么就意味着两个消费者同时读取分区的消息，由于消费者自己可以控制读取消息的offset (偏移量)，就有可能C1才读到2，而C2读到1，C1还没提交 offset，这时C2读到2了，相当于多线程读取同一个消息，会造成消息处理的重复，且不能保证消息的顺序，这就跟主动推送（push）无异。</p>
<h2 id="消费者分区分配策略-两种">消费者分区分配策略 (两种)</h2>
<p>range策略是基于每个主题的，对于每个主题，我们以数字顺序排列可用分区，以字典顺序排列消费者。然后，将分区数量除以消费者总数，以确定分配给每个消费者的分区数量。如果没有平均划分（PS：除不尽），那么最初的几个消费者将有一个额外的分区。</p>
<p>简而言之:</p>
<ol>
<li class="lvl-3">
<p>range分配策略针对的是主题（也就是说，这里所说的分区指的某个主题的分区，消费者值的是订阅这个主题的消费者组中的消费者实例）</p>
</li>
<li class="lvl-3">
<p>首先，将分区按数字顺序排行序，消费者按消费者名称的字典顺序排好序.</p>
</li>
<li class="lvl-3">
<p>然后，用分区总数除以消费者总数。如果能够除尽，则皆大欢喜，平均分配；若除不尽，则位于排序前面的消费者将多负责一个分区.</p>
</li>
</ol>
<p>例如，假设有两个消费者C0和C1，两个主题t0和t1，并且每个主题有3个分区，分区的情况是这样的：t0p0，t0p1，t0p2，t1p0，t1p1，t1p2</p>
<p>那么，基于以上信息，最终消费者分配分区的情况是这样的：</p>
<p>C0: [t0p0, t0p1, t1p0, t1p1]</p>
<p>C1: [t0p2, t1p2]</p>
<p>因为，对于主题t0，分配的结果是C0负责P0和P1，C1负责P2；对于主题t2，也是如此，综合起来就是这个结果</p>
<p>上面的过程用图形表示的话大概是这样的 :</p>
<img src="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230609350-1656648320.png" referrerpolicy="no-referrer">
<p>roundrobin (轮询)</p>
<p>roundronbin分配策略的具体实现是org.apache.kafka.clients.consumer.RoundRobinAssignor</p>
<p>轮询分配策略是基于所有可用的消费者和所有可用的分区的</p>
<p>与前面的range策略最大的不同就是它不再局限于某个主题</p>
<p>如果所有的消费者实例的订阅都是相同的，那么这样最好了，可用统一分配，均衡分配</p>
<p>例如，假设有两个消费者C0和C1，两个主题t0和t1，每个主题有3个分区，分别是t0p0，t0p1，t0p2，t1p0，t1p1，t1p2</p>
<p>那么，最终分配的结果是这样的：</p>
<p>C0: [t0p0, t0p2, t1p1]</p>
<p>C1: [t0p1, t1p0, t1p2]</p>
<p>用图形表示大概是这样的:</p>
<img src="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230645759-1165797262.png" referrerpolicy="no-referrer">
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>linux环境变量加载顺序</title>
    <url>/2021/10/13/linux%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%8A%A0%E8%BD%BD%E9%A1%BA%E5%BA%8F/</url>
    <content><![CDATA[<h1>01、环境变量文件描述</h1>
<p><strong>/etc/profile</strong>: 此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行,并从/etc/profile.d目录的配置文件中搜集shell的设置.<br>
<strong>/etc/bashrc</strong>: 为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取.</p>
<p>//用户级别的环境变量，用户可以覆盖全局变量<br>
<strong>~/.bash_profile</strong>: 每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的.bashrc文件.<br>
<strong>~/.bashrc</strong>: 该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该文件被读取.<br>
<strong>~/.bash_logout</strong>: 当每次退出系统(退出bash shell)时,执行该文件.</p>
<p>/etc/profile中设定的变量(全局)的可以作用于任何用户,<br>
而~/.bashrc等中设定的变量(局部)只能继承/etc/profile中的变量,他们是&quot;父子&quot;关系.</p>
<p>~/.bash_profile 是交互式、login 方式进入 bash 运行的<br>
~/.bashrc 是交互式 non-login 方式进入 bash 运行的通常二者设置大致相同，所以通常前者会调用后者</p>
<h2 id="一、系统环境变量：">一、系统环境变量：</h2>
<p><strong>/etc/profile</strong> ：这个文件预设了几个重要的变量，例如PATH, USER, LOGNAME, MAIL, INPUTRC, HOSTNAME, HISTSIZE, umask等等。</p>
<p>为系统的每个用户设置环境信息。当用户第一次登陆时，该文件执行，并从/etc/profile.d目录中的配置文件搜索shell的设置（可以用于设定针对全系统所有用户的环境变量，环境变量周期是永久的）</p>
<p><strong>/etc/bashrc</strong> ：这个文件主要预设umask以及PS1。这个PS1就是我们在敲命令时，前面那串字符了，例如 [root@localhost ~]#,当bash shell被打开时,该文件被读取</p>
<p>这个文件是针对所有用户的bash初始化文件，在此设定中的环境信息将应用与所有用户的shell中，此文件会在用户每次打开shell时执行一次。（即每次新开一个终端，都会执行/etc/bashrc）**</p>
<h2 id="二、用户环境变量：">二、用户环境变量：</h2>
<p><strong>.bash_profile</strong> ：定义了用户的个人化路径与环境变量的文件名称。每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次。（在这个文件中有执行.bashrc的脚本）</p>
<p><strong>.bashrc</strong> ：该文件包含专用于你的shell的bash信息,当登录时以及每次打开新的shell时,该该文件被读取。例如你可以将用户自定义的alias或者自定义变量写到这个文件中。</p>
<p><strong>.bash_history</strong> ：记录命令历史用的。</p>
<p><strong>.bash_logout</strong> ：当退出shell时，会执行该文件。可以把一些清理的工作放到这个文件中。</p>
<p>linux加载配置项时通过下面方式首先 加载/etc/profile配置</p>
<p>然后 加载/ect/profile.d/下面的所有脚本</p>
<p>然后 加载当前用户 .bash_profile</p>
<p>然后 加载.bashrc</p>
<p>最后 加载 [/etc/bashrc]</p>
<p>/etc/profile → /etc/profile.d/*.sh → ~/.bash_profile → ~/.bashrc → [/etc/bashrc]</p>
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式计算框架—MapReduce</title>
    <url>/2021/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6%E2%80%94MapReduce/</url>
    <content><![CDATA[<h2 id="一、MapReduce概述">一、MapReduce概述</h2>
<p>Hadoop MapReduce 是一个分布式计算框架，用于编写批处理应用程序。编写好的程序可以提交到 Hadoop 集群上用于并行处理大规模的数据集。</p>
<p>MapReduce 作业通过将输入的数据集拆分为独立的块，这些块由 <code>map</code> 以并行的方式处理，框架对 <code>map</code> 的输出进行排序，然后输入到 <code>reduce</code> 中。MapReduce 框架专门用于 <code>&lt;key，value&gt;</code> 键值对处理，它将作业的输入视为一组 <code>&lt;key，value&gt;</code> 对，并生成一组 <code>&lt;key，value&gt;</code> 对作为输出。输出和输出的 <code>key</code> 和 <code>value</code> 都必须实现<a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html">Writable</a> 接口。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt; -&gt; combine -&gt; &lt;k2, v2&gt; -&gt; reduce -&gt; &lt;k3, v3&gt; (output)</span><br></pre></td></tr></table></figure>
<h2 id="二、MapReduce编程模型简述">二、MapReduce编程模型简述</h2>
<p>这里以词频统计为例进行说明，MapReduce 处理的流程如下：</p>
<div align="center"> <img width="600px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/mapreduceProcess.png"/> </div>
<ol>
<li class="lvl-3">
<p><strong>input</strong> : 读取文本文件；</p>
</li>
<li class="lvl-3">
<p><strong>splitting</strong> : 将文件按照行进行拆分，此时得到的 <code>K1</code> 行数，<code>V1</code> 表示对应行的文本内容；</p>
</li>
<li class="lvl-3">
<p><strong>mapping</strong> : 并行将每一行按照空格进行拆分，拆分得到的 <code>List(K2,V2)</code>，其中 <code>K2</code> 代表每一个单词，由于是做词频统计，所以 <code>V2</code> 的值为 1，代表出现 1 次；</p>
</li>
<li class="lvl-3">
<p><strong>shuffling</strong>：由于 <code>Mapping</code> 操作可能是在不同的机器上并行处理的，所以需要通过 <code>shuffling</code> 将相同 <code>key</code> 值的数据分发到同一个节点上去合并，这样才能统计出最终的结果，此时得到 <code>K2</code> 为每一个单词，<code>List(V2)</code> 为可迭代集合，<code>V2</code> 就是 Mapping 中的 V2；</p>
</li>
<li class="lvl-3">
<p><strong>Reducing</strong> : 这里的案例是统计单词出现的总次数，所以 <code>Reducing</code> 对 <code>List(V2)</code> 进行归约求和操作，最终输出。</p>
</li>
</ol>
<p>MapReduce 编程模型中 <code>splitting</code> 和 <code>shuffing</code> 操作都是由框架实现的，需要我们自己编程实现的只有 <code>mapping</code> 和 <code>reducing</code>，这也就是 MapReduce 这个称呼的来源。</p>
<h2 id="三、combiner-partitioner">三、combiner &amp; partitioner</h2>
<div align="center"> <img width="600px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/Detailed-Hadoop-MapReduce-Data-Flow-14.png"/> </div>
<h3 id="3-1-InputFormat-RecordReaders">3.1 InputFormat &amp; RecordReaders</h3>
<p><code>InputFormat</code> 将输出文件拆分为多个 <code>InputSplit</code>，并由 <code>RecordReaders</code> 将 <code>InputSplit</code> 转换为标准的&lt;key，value&gt;键值对，作为 map 的输出。这一步的意义在于只有先进行逻辑拆分并转为标准的键值对格式后，才能为多个 <code>map</code> 提供输入，以便进行并行处理。</p>
<h3 id="3-2-Combiner">3.2 Combiner</h3>
<p><code>combiner</code> 是 <code>map</code> 运算后的可选操作，它实际上是一个本地化的 <code>reduce</code> 操作，它主要是在 <code>map</code> 计算出中间文件后做一个简单的合并重复 <code>key</code> 值的操作。这里以词频统计为例：</p>
<p><code>map</code> 在遇到一个 hadoop 的单词时就会记录为 1，但是这篇文章里 hadoop 可能会出现 n 多次，那么 <code>map</code> 输出文件冗余就会很多，因此在 <code>reduce</code> 计算前对相同的 key 做一个合并操作，那么需要传输的数据量就会减少，传输效率就可以得到提升。</p>
<p>但并非所有场景都适合使用 <code>combiner</code>，使用它的原则是 <code>combiner</code> 的输出不会影响到 <code>reduce</code> 计算的最终输入，例如：求总数，最大值，最小值时都可以使用 <code>combiner</code>，但是做平均值计算则不能使用 <code>combiner</code>。</p>
<p>不使用 combiner 的情况：</p>
<div align="center"> <img  width="600px"  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/mapreduce-without-combiners.png"/> </div>
<p>使用 combiner 的情况：</p>
<div align="center"> <img width="600px"  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/mapreduce-with-combiners.png"/> </div>
<p>可以看到使用 combiner 的时候，需要传输到 reducer 中的数据由 12keys，降低到 10keys。降低的幅度取决于你 keys 的重复率，下文词频统计案例会演示用 combiner 降低数百倍的传输量。</p>
<h3 id="3-3-Partitioner">3.3 Partitioner</h3>
<p><code>partitioner</code> 可以理解成分类器，将 <code>map</code> 的输出按照 key 值的不同分别分给对应的 <code>reducer</code>，支持自定义实现，下文案例会给出演示。</p>
<h2 id="四、MapReduce词频统计案例">四、MapReduce词频统计案例</h2>
<h3 id="4-1-项目简介">4.1 项目简介</h3>
<p>这里给出一个经典的词频统计的案例：统计如下样本数据中每个单词出现的次数。</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">Spark</span>	<span class="string">HBase</span></span><br><span class="line"><span class="attr">Hive</span>	<span class="string">Flink	Storm	Hadoop	HBase	Spark</span></span><br><span class="line"><span class="attr">Flink</span></span><br><span class="line"><span class="attr">HBase</span>	<span class="string">Storm</span></span><br><span class="line"><span class="attr">HBase</span>	<span class="string">Hadoop	Hive	Flink</span></span><br><span class="line"><span class="attr">HBase</span>	<span class="string">Flink	Hive	Storm</span></span><br><span class="line"><span class="attr">Hive</span>	<span class="string">Flink	Hadoop</span></span><br><span class="line"><span class="attr">HBase</span>	<span class="string">Hive</span></span><br><span class="line"><span class="attr">Hadoop</span>	<span class="string">Spark	HBase	Storm</span></span><br><span class="line"><span class="attr">HBase</span>	<span class="string">Hadoop	Hive	Flink</span></span><br><span class="line"><span class="attr">HBase</span>	<span class="string">Flink	Hive	Storm</span></span><br><span class="line"><span class="attr">Hive</span>	<span class="string">Flink	Hadoop</span></span><br><span class="line"><span class="attr">HBase</span>	<span class="string">Hive</span></span><br></pre></td></tr></table></figure>
<p>为方便大家开发，我在项目源码中放置了一个工具类 <code>WordCountDataUtils</code>，用于模拟产生词频统计的样本，生成的文件支持输出到本地或者直接写到 HDFS 上。</p>
<blockquote>
<p>项目完整源码下载地址：<a href="https://github.com/oicio/BigData-Notes/tree/master/code/Hadoop/hadoop-word-count">hadoop-word-count</a></p>
</blockquote>
<h3 id="4-2-项目依赖">4.2 项目依赖</h3>
<p>想要进行 MapReduce 编程，需要导入 <code>hadoop-client</code> 依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-3-WordCountMapper">4.3 WordCountMapper</h3>
<p>将每行数据按照指定分隔符进行拆分。这里需要注意在 MapReduce 中必须使用 Hadoop 定义的类型，因为 Hadoop 预定义的类型都是可序列化，可比较的，所有类型均实现了 <code>WritableComparable</code> 接口。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, </span><br><span class="line">                                                                      InterruptedException &#123;</span><br><span class="line">        String[] words = value.toString().split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            context.write(<span class="keyword">new</span> <span class="title class_">Text</span>(word), <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>WordCountMapper</code> 对应下图的 Mapping 操作：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hadoop-code-mapping.png"/> </div>
<p><code>WordCountMapper</code> 继承自 <code>Mappe</code> 类，这是一个泛型类，定义如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">WordCountMapper <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Mapper</span>&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; &#123;</span><br><span class="line">   ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>KEYIN</strong> : <code>mapping</code> 输入 key 的类型，即每行的偏移量 (每行第一个字符在整个文本中的位置)，<code>Long</code> 类型，对应 Hadoop 中的 <code>LongWritable</code> 类型；</p>
</li>
<li class="lvl-2">
<p><strong>VALUEIN</strong> : <code>mapping</code> 输入 value 的类型，即每行数据；<code>String</code> 类型，对应 Hadoop 中 <code>Text</code> 类型；</p>
</li>
<li class="lvl-2">
<p><strong>KEYOUT</strong> ：<code>mapping</code> 输出的 key 的类型，即每个单词；<code>String</code> 类型，对应 Hadoop 中 <code>Text</code> 类型；</p>
</li>
<li class="lvl-2">
<p><strong>VALUEOUT</strong>：<code>mapping</code> 输出 value 的类型，即每个单词出现的次数；这里用 <code>int</code> 类型，对应 <code>IntWritable</code> 类型。</p>
</li>
</ul>
<h3 id="4-4-WordCountReducer">4.4 WordCountReducer</h3>
<p>在 Reduce 中进行单词出现次数的统计：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, </span><br><span class="line">                                                                                  InterruptedException &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key, <span class="keyword">new</span> <span class="title class_">IntWritable</span>(count));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如下图，<code>shuffling</code> 的输出是 reduce 的输入。这里的 key 是每个单词，values 是一个可迭代的数据类型，类似 <code>(1,1,1,...)</code>。</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hadoop-code-reducer.png"/> </div>
<h3 id="4-4-WordCountApp">4.4 WordCountApp</h3>
<p>组装 MapReduce 作业，并提交到服务器运行，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 组装作业 并提交到集群运行</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountApp</span> &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这里为了直观显示参数 使用了硬编码，实际开发中可以通过外部传参</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">HDFS_URL</span> <span class="operator">=</span> <span class="string">&quot;hdfs://192.168.0.107:8020&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">HADOOP_USER_NAME</span> <span class="operator">=</span> <span class="string">&quot;root&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//  文件输入路径和输出路径由外部传参指定</span></span><br><span class="line">        <span class="keyword">if</span> (args.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Input and output paths are necessary!&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 需要指明 hadoop 用户名，否则在 HDFS 上创建目录时可能会抛出权限不足的异常</span></span><br><span class="line">        System.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, HADOOP_USER_NAME);</span><br><span class="line"></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="comment">// 指明 HDFS 的地址</span></span><br><span class="line">        configuration.set(<span class="string">&quot;fs.defaultFS&quot;</span>, HDFS_URL);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建一个 Job</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置运行的主类</span></span><br><span class="line">        job.setJarByClass(WordCountApp.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Mapper 和 Reducer</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Mapper 输出 key 和 value 的类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Reducer 输出 key 和 value 的类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果输出目录已经存在，则必须先删除，否则重复运行程序时会抛出异常</span></span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fileSystem</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(HDFS_URL), configuration, HADOOP_USER_NAME);</span><br><span class="line">        <span class="type">Path</span> <span class="variable">outputPath</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">if</span> (fileSystem.exists(outputPath)) &#123;</span><br><span class="line">            fileSystem.delete(outputPath, <span class="literal">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置作业输入文件和输出文件的路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将作业提交到群集并等待它完成，参数设置为 true 代表打印显示对应的进度</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关闭之前创建的 fileSystem</span></span><br><span class="line">        fileSystem.close();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据作业结果,终止当前运行的 Java 虚拟机,退出程序</span></span><br><span class="line">        System.exit(result ? <span class="number">0</span> : -<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要注意的是：如果不设置 <code>Mapper</code> 操作的输出类型，则程序默认它和 <code>Reducer</code> 操作输出的类型相同。</p>
<h3 id="4-5-提交到服务器运行">4.5 提交到服务器运行</h3>
<p>在实际开发中，可以在本机配置 hadoop 开发环境，直接在 IDE 中启动进行测试。这里主要介绍一下打包提交到服务器运行。由于本项目没有使用除 Hadoop 外的第三方依赖，直接打包即可：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">mvn clean package</span></span><br></pre></td></tr></table></figure>
<p>使用以下命令提交作业：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar /usr/appjar/hadoop-word-count-1.0.jar \</span><br><span class="line">com.oicio.WordCountApp \</span><br><span class="line">/wordcount/input.txt /wordcount/output/WordCountApp</span><br></pre></td></tr></table></figure>
<p>作业完成后查看 HDFS 上生成目录：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">查看目录</span></span><br><span class="line">hadoop fs -ls /wordcount/output/WordCountApp</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看统计结果</span></span><br><span class="line">hadoop fs -cat /wordcount/output/WordCountApp/part-r-00000</span><br></pre></td></tr></table></figure>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hadoop-wordcountapp.png"/> </div>
<h2 id="五、词频统计案例进阶之Combiner">五、词频统计案例进阶之Combiner</h2>
<h3 id="5-1-代码实现">5.1 代码实现</h3>
<p>想要使用 <code>combiner</code> 功能只要在组装作业时，添加下面一行代码即可：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置 Combiner</span></span><br><span class="line">job.setCombinerClass(WordCountReducer.class);</span><br></pre></td></tr></table></figure>
<h3 id="5-2-执行结果">5.2 执行结果</h3>
<p>加入 <code>combiner</code> 后统计结果是不会有变化的，但是可以从打印的日志看出 <code>combiner</code> 的效果：</p>
<p>没有加入 <code>combiner</code> 的打印日志：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hadoop-no-combiner.png"/> </div>
<p>加入 <code>combiner</code> 后的打印日志如下：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hadoop-combiner.png"/> </div>
<p>这里我们只有一个输入文件并且小于 128M，所以只有一个 Map 进行处理。可以看到经过 combiner 后，records 由 <code>3519</code> 降低为 <code>6</code>(样本中单词种类就只有 6 种)，在这个用例中 combiner 就能极大地降低需要传输的数据量。</p>
<h2 id="六、词频统计案例进阶之Partitioner">六、词频统计案例进阶之Partitioner</h2>
<h3 id="6-1-默认的Partitioner">6.1  默认的Partitioner</h3>
<p>这里假设有个需求：将不同单词的统计结果输出到不同文件。这种需求实际上比较常见，比如统计产品的销量时，需要将结果按照产品种类进行拆分。要实现这个功能，就需要用到自定义 <code>Partitioner</code>。</p>
<p>这里先介绍下 MapReduce 默认的分类规则：在构建 job 时候，如果不指定，默认的使用的是 <code>HashPartitioner</code>：对 key 值进行哈希散列并对 <code>numReduceTasks</code> 取余。其实现如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HashPartitioner</span>&lt;K, V&gt; <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;K, V&gt; &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(K key, V value,</span></span><br><span class="line"><span class="params">                          <span class="type">int</span> numReduceTasks)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="6-2-自定义Partitioner">6.2 自定义Partitioner</h3>
<p>这里我们继承 <code>Partitioner</code> 自定义分类规则，这里按照单词进行分类：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomPartitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text text, IntWritable intWritable, <span class="type">int</span> numPartitions)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> WordCountDataUtils.WORD_LIST.indexOf(text.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在构建 <code>job</code> 时候指定使用我们自己的分类规则，并设置 <code>reduce</code> 的个数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置自定义分区规则</span></span><br><span class="line">job.setPartitionerClass(CustomPartitioner.class);</span><br><span class="line"><span class="comment">// 设置 reduce 个数</span></span><br><span class="line">job.setNumReduceTasks(WordCountDataUtils.WORD_LIST.size());</span><br></pre></td></tr></table></figure>
<h3 id="6-3-执行结果">6.3  执行结果</h3>
<p>执行结果如下，分别生成 6 个文件，每个文件中为对应单词的统计结果：</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/hadoop-wordcountcombinerpartition.png"/> </div>
<h2 id="参考资料-21">参考资料</h2>
<ol>
<li class="lvl-3">
<p><a href="https://zhuanlan.zhihu.com/p/28682581">分布式计算框架 MapReduce</a></p>
</li>
<li class="lvl-3">
<p><a href="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">Apache Hadoop 2.9.2 &gt; MapReduce Tutorial</a></p>
</li>
<li class="lvl-3">
<p><a href="https://www.tutorialscampus.com/tutorials/map-reduce/combiners.htm">MapReduce - Combiners</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>初识kudu</title>
    <url>/2021/10/13/%E5%88%9D%E8%AF%86kudu/</url>
    <content><![CDATA[<h2 id="1-1、kudu是什么">1.1、kudu是什么</h2>
<p>简单来说:dudu是一个与hbase类似的列式存储分布式数据库。官方给kudu的定位是:在更新更及时的基础上实现更快的数据分析</p>
<h2 id="1-2、为什么需要kudu">1.2、为什么需要kudu</h2>
<h3 id="1-2-1、hdfs与hbase数据存储的缺点">1.2.1、hdfs与hbase数据存储的缺点</h3>
<p>目前数据存储有了HDFS与hbase，为什么还要额外的弄一个kudu呢。<br>
HDFS:使用列式存储格式Apache Parquet，Apache ORC，适合离线分析，不支持单条纪录级别的update操作，随机读写性能差。<br>
HBASE:可以进行高效随机读写，却并不适用于基于SQL的数据分析方向，大批量数据获取时的性能较差。正因为HDFS与HBASE有上面这些缺点，KUDU较好的解决了HDFS与HBASE的这些缺点，它不及HDFS批处理快，也不及HBase随机读写能力强，但是反过来它比HBase批处理快（适用于OLAP的分析场景），而且比HDFS随机读写能力强（适用于实时写入或者更新的场景），这就是它能解决的问题。</p>
<h1>2、架构介绍</h1>
<h2 id="2-1、基本架构">2.1、基本架构</h2>
<img src="https://img-blog.csdnimg.cn/20190422091319780.png" referrerpolicy="no-referrer">
<h2 id="2-1-1、概念">2.1.1、概念</h2>
<p> Table（表）：一张table是数据存储在kudu的位置。Table具有schema和全局有序的primary key(主键)。Table被分为很多段，也就是tablets.<br>
 Tablet (段)：一个tablet是一张table连续的segment，与其他数据存储引擎或关系型数据的partition相似。Tablet存在副本机制，其中一个副本为leader tablet。任何副本都可以对读取进行服务，并且写入时需要在所有副本对应的tablet server之间达成一致性。<br>
 Tablet server：存储tablet和为tablet向client提供服务。对于给定的tablet，一个tablet server充当leader，其他tablet server充当该tablet的follower副本。只有leader服务写请求，leader与follower为每个服务提供读请求。<br>
 Master：主要用来管理元数据(元数据存储在只有一个tablet的catalog table中)，即tablet与表的基本信息，监听tserver的状态<br>
 Catalog Table: 元数据表，用来存储table(schema、locations、states)与tablet（现有的tablet列表，每个tablet及其副本所处tserver，tablet当前状态以及开始和结束键）的信息。</p>
<h1>3、存储机制</h1>
<h2 id="3-1-存储结构全景图">3.1 存储结构全景图</h2>
<img src="https://img-blog.csdnimg.cn/20190422091359222.png" referrerpolicy="no-referrer">
<h2 id="3-2-存储结构解析">3.2 存储结构解析</h2>
<p> 一个Table包含多个Tablet，其中Tablet的数量是根据hash或者range进行设置<br>
 一个Tablet中包含MetaData信息和多个RowSet信息<br>
 一个Rowset中包含一个MemRowSet与0个或多个DiskRowset，其中MemRowSet存储insert的数据，一旦MemRowSet写满会flush到磁盘生成一个或多个DiskRowSet，此时MemRowSet清空。MemRowSet默认写满1G或者120s flush一次<br>
(注意:memRowSet是行式存储，DiskRowSet是列式存储，MemRowSet基于primary key有序)。每隔tablet中会定期对一些diskrowset做compaction操作，目的是对多个diskRowSet进行重新排序，以此来使其更有序并减少diskRowSet的数量，同时在compaction的过程中慧慧resolve掉deltaStores当中的delete记录<br>
 一个DiskRowSet包含baseData与DeltaStores两部分，其中baseData存储的数据看起来不可改变，DeltaStores中存储的是改变的数据<br>
 DeltaStores包含一个DeltaMemStores和多个DeltaFile,其中DeltaMemStores放在内存，用来存储update与delete数据，一旦DeltaMemStores写满，会flush成DeltaFile。当DeltaFile过多会影响查询性能，所以KUDU每隔一段时间会执行compaction操作，将其合并到baseData中，主要是resolve掉update数据。</p>
<h1>4、kudu的工作机制</h1>
<h2 id="4-1-概述">4.1 概述</h2>
<p>1、kudu主要角色分为master与tserver<br>
2、master主要负责:管理元数据信息，监听server，当server宕机后负责tablet的重分配<br>
3、tserver主要负责tablet的存储与和数据的增删改查</p>
<h2 id="4-2-内部实现原理图">4.2 内部实现原理图</h2>
<h2 id="img-src-https-img-blog-csdnimg-cn-20190422091414844-png-referrerpolicy-no-referrer"><img src="https://img-blog.csdnimg.cn/20190422091414844.png" referrerpolicy="no-referrer"></h2>
<h2 id="4-3-读流程">4.3 读流程</h2>
<h3 id="4-3-1-概述">4.3.1 概述</h3>
<p>客户端将要读取的数据信息发送给master，master对其进行一定的校验，比如表是否存在，字段是否存在。Master返回元数据信息给client，然后client与tserver建立连接，通过metaData找到数据所在的RowSet，首先加载内存里面的数据(MemRowSet与DeltMemStore),然后加载磁盘里面的数据，最后返回最终数据给client.</p>
<h3 id="4-3-2-详细步骤图">4.3.2 详细步骤图</h3>
 <img src="https://img-blog.csdnimg.cn/20190422091428943.png" referrerpolicy="no-referrer">
<h3 id="4-3-3-详细步骤解析">4.3.3 详细步骤解析</h3>
<p>1、客户端master请求查询表指定数据<br>
2、master对请求进行校验，校验表是否存在，schema中是否存在指定查询的字段，主键是否存在<br>
3、master通过查询catalog Table返回表，将tablet对应的tserver信息、tserver状态等元数据信息返回给client<br>
4、client与tserver建立连接，通过metaData找到primary key对应的RowSet。<br>
5、首先加载RowSet内存中MemRowSet与DeltMemStore中的数据<br>
6、然后加载磁盘中的数据，也就是DiskRowSet中的BaseData与DeltFile中的数据<br>
7、返回数据给Client<br>
8、继续4-7步骤，直到拿到所有数据返回给client</p>
<h2 id="4-4、插入流程">4.4、插入流程</h2>
<h3 id="4-4-1-概述">4.4.1 概述</h3>
<p>Client首先连接master，获取元数据信息。然后连接tserver，查找MemRowSet与DeltMemStore中是否存在相同primary key，如果存在，则报错;如果不存在，则将待插入的数据写入WAL日志，然后将数据写入MemRowSet。</p>
<h3 id="4-4-2-详细步骤图">4.4.2 详细步骤图</h3>
<img src="https://img-blog.csdnimg.cn/20190422091442719.png" referrerpolicy="no-referrer"> 
<h3 id="4-4-3-详细步骤解析">4.4.3 详细步骤解析</h3>
<p>1、client向master请求预写表的元数据信息<br>
2、master会进行一定的校验，表是否存在，字段是否存在等<br>
3、如果master校验通过，则返回表的分区、tablet与其对应的tserver给client；如果校验失败则报错给client。<br>
4、client根据master返回的元数据信息，将请求发送给tablet对应的tserver.<br>
5、tserver首先会查询内存中MemRowSet与DeltMemStore中是否存在与待插入数据主键相同的数据，如果存在则报错<br>
6、tserver会讲写请求预写到WAL日志，用来server宕机后的恢复操作<br>
7、将数据写入内存中的MemRowSet中，一旦MemRowSet的大小达到1G或120s后，MemRowSet会flush成一个或DiskRowSet,用来将数据持久化<br>
8、返回client数据处理完毕</p>
<h2 id="4-5、数据更新流程">4.5、数据更新流程</h2>
<h3 id="4-5-1-概述">4.5.1 概述</h3>
<p>Client首先向master请求元数据，然后根据元数据提供的tablet信息，连接tserver，根据数据所处位置的不同，有不同的操作:在内存MemRowSet中的数据，会将更新信息写入数据所在行的mutation链表中；在磁盘中的数据，会将更新信息写入DeltMemStore中。</p>
<h3 id="4-5-2、详细步骤图">4.5.2、详细步骤图</h3>
 <img src="https://img-blog.csdnimg.cn/20190422091450166.png" referrerpolicy="no-referrer">
<h3 id="4-5-3-详细步骤解析">4.5.3 详细步骤解析</h3>
<p>1、client向master请求预更新表的元数据，首先master会校验表是否存在，字段是否存在，如果校验通过则会返回给client表的分区、tablet、tablet所在tserver信息<br>
2、client向tserver发起更新请求<br>
3、将更新操作预写如WAL日志，用来在server宕机后的数据恢复<br>
4、根据tserver中待更新的数据所处位置的不同，有不同的处理方式:<br>
如果数据在内存中，则从MemRowSet中找到数据所处的行，然后在改行的mutation链表中写入更新信息，在MemRowSet flush的时候，将更新合并到baseData中如果数据在DiskRowSet中，则将更新信息写入DeltMemStore中，DeltMemStore达到一定大小后会flush成DeltFile。<br>
5、更新完毕后返回消息给client。</p>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>kudu</category>
      </categories>
      <tags>
        <tag>kudu</tag>
      </tags>
  </entry>
  <entry>
    <title>基于ZooKeeper搭建Hadoop</title>
    <url>/2021/10/15/%E5%9F%BA%E4%BA%8EZooKeeper%E6%90%AD%E5%BB%BAHadoop/</url>
    <content><![CDATA[<h2 id="一、高可用简介">一、高可用简介</h2>
<p>Hadoop 高可用 (High Availability) 分为 HDFS 高可用和 YARN 高可用，两者的实现基本类似，但 HDFS NameNode 对数据存储及其一致性的要求比 YARN ResourceManger 高得多，所以它的实现也更加复杂，故下面先进行讲解：</p>
<h3 id="1-1-高可用整体架构">1.1 高可用整体架构</h3>
<p>HDFS 高可用架构如下：</p>
<p><img src="https://camo.githubusercontent.com/b0b8e7bfcf3d75fc2048d0fb1671e7c7ffa234f8/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f484446532d48412d4172636869746563747572652d45647572656b612e706e67" alt="img"></p>
<blockquote>
<p><em>图片引用自：<a href="https://www.edureka.co/blog/how-to-set-up-hadoop-cluster-with-hdfs-high-availability/">https://www.edureka.co/blog/how-to-set-up-hadoop-cluster-with-hdfs-high-availability/</a></em></p>
</blockquote>
<p>HDFS 高可用架构主要由以下组件所构成：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>Active NameNode 和 Standby NameNode</strong>：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。</p>
</li>
<li class="lvl-2">
<p><strong>主备切换控制器 ZKFailoverController</strong>：ZKFailoverController  作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode  的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换，当然 NameNode 目前也支持不依赖于  Zookeeper 的手动主备切换。</p>
</li>
<li class="lvl-2">
<p><strong>Zookeeper 集群</strong>：为主备切换控制器提供主备选举支持。</p>
</li>
<li class="lvl-2">
<p><strong>共享存储系统</strong>：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了  NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和 NameNode  通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。</p>
</li>
<li class="lvl-2">
<p><strong>DataNode 节点</strong>：除了通过共享存储系统共享 HDFS 的元数据信息之外，主 NameNode 和备 NameNode 还需要共享 HDFS 的数据块和 DataNode 之间的映射关系。DataNode 会同时向主 NameNode 和备  NameNode 上报数据块的位置信息。</p>
</li>
</ul>
<h3 id="1-2-基于-QJM-的共享存储系统的数据同步机制分析">1.2 基于 QJM 的共享存储系统的数据同步机制分析</h3>
<p>目前 Hadoop 支持使用 Quorum Journal Manager (QJM) 或 Network File System  (NFS) 作为共享的存储系统，这里以 QJM 集群为例进行说明：Active NameNode 首先把 EditLog 提交到  JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog，当  Active NameNode  宕机后， Standby  NameNode 在确认元数据完全同步之后就可以对外提供服务。</p>
<p>需要说明的是向 JournalNode 集群写入 EditLog 是遵循 “过半写入则成功” 的策略，所以你至少要有 3 个  JournalNode 节点，当然你也可以继续增加节点数量，但是应该保证节点总数是奇数。同时如果有 2N+1 台  JournalNode，那么根据过半写的原则，最多可以容忍有 N 台 JournalNode 节点挂掉。</p>
<p><img src="https://camo.githubusercontent.com/3f5274ffc21109f2390bdd2e1ec37d3e837b4cc1/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d514a4d2de5908ce6ada5e69cbae588b62e706e67" alt="img"></p>
<h3 id="1-3-NameNode-主备切换">1.3 NameNode 主备切换</h3>
<p>NameNode 实现主备切换的流程下图所示：</p>
<p><img src="https://camo.githubusercontent.com/33bb0d7c919a1ef21c959460dc3e5e116070b5bb/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d6e616d656e6f6465e4b8bbe5a487e58887e68da22e706e67" alt="img"></p>
<p>\1. HealthMonitor 初始化完成之后会启动内部的线程来定时调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法，对 NameNode 的健康状态进行检测。 2. HealthMonitor 如果检测到 NameNode 的健康状态发生变化，会回调 ZKFailoverController 注册的相应方法进行处理。 3. 如果 ZKFailoverController 判断需要进行主备切换，会首先使用 ActiveStandbyElector 来进行自动的主备选举。 4. ActiveStandbyElector 与 Zookeeper 进行交互完成自动的主备选举。 5. ActiveStandbyElector 在主备选举完成后，会回调 ZKFailoverController 的相应方法来通知当前的 NameNode 成为主 NameNode 或备 NameNode。 6. ZKFailoverController 调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法将 NameNode 转换为 Active 状态或 Standby 状态。</p>
<h3 id="1-4-YARN高可用">1.4 YARN高可用</h3>
<p>YARN ResourceManager 的高可用与 HDFS NameNode 的高可用类似，但是 ResourceManager 不像 NameNode ，没有那么多的元数据信息需要维护，所以它的状态信息可以直接写到 Zookeeper 上，并依赖 Zookeeper  来进行主备选举。</p>
<p><img src="https://camo.githubusercontent.com/9e7d4c0e0db498ab5f8663d58ad38deeaa330218/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d726d2d68612d6f766572766965772e706e67" alt="img"></p>
<h2 id="二、集群规划">二、集群规划</h2>
<p>按照高可用的设计目标：需要保证至少有两个 NameNode (一主一备)  和 两个 ResourceManager (一主一备)   ，同时为满足“过半写入则成功”的原则，需要至少要有 3 个 JournalNode 节点。这里使用三台主机进行搭建，集群规划如下：</p>
<p><img src="https://camo.githubusercontent.com/9d815ddad1f979ac693a6d96f2e59c0962a72c14/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4e8a784e588922e706e67" alt="img"></p>
<h2 id="三、前置条件">三、前置条件</h2>
<ul class="lvl-0">
<li class="lvl-2">
<p>所有服务器都安装有 JDK，安装步骤可以参见：<a href="https://github.com/ihadyou/BigData-Notes/blob/master/notes/installation/Linux%E4%B8%8BJDK%E5%AE%89%E8%A3%85.md">Linux 下 JDK 的安装</a>；</p>
</li>
<li class="lvl-2">
<p>搭建好 ZooKeeper 集群，搭建步骤可以参见：<a href="https://github.com/ihadyou/BigData-Notes/blob/master/notes/installation/Zookeeper%E5%8D%95%E6%9C%BA%E7%8E%AF%E5%A2%83%E5%92%8C%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.md">Zookeeper 单机环境和集群环境搭建</a></p>
</li>
<li class="lvl-2">
<p>所有服务器之间都配置好 SSH 免密登录。</p>
</li>
</ul>
<h2 id="四、集群配置">四、集群配置</h2>
<h3 id="4-1-下载并解压">4.1 下载并解压</h3>
<p>下载 Hadoop。这里我下载的是 CDH 版本 Hadoop，下载地址为：<a href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># tar -zvxf hadoop-2.6.0-cdh5.15.2.tar.gz </span><br></pre></td></tr></table></figure>
<h3 id="4-2-配置环境变量">4.2 配置环境变量</h3>
<p>编辑 <code>profile</code> 文件：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># vim /etc/profile</span><br></pre></td></tr></table></figure>
<p>增加如下配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2</span><br><span class="line">export  PATH=$&#123;HADOOP_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure>
<p>执行 <code>source</code> 命令，使得配置立即生效：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># source /etc/profile</span><br></pre></td></tr></table></figure>
<h3 id="4-3-修改配置">4.3 修改配置</h3>
<p>进入 <code>$&#123;HADOOP_HOME&#125;/etc/hadoop</code> 目录下，修改配置文件。各个配置文件内容如下：</p>
<h4 id="1-hadoop-env-sh-3">1. <a href="http://hadoop-env.sh">hadoop-env.sh</a></h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 指定JDK的安装位置</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_201/</span><br></pre></td></tr></table></figure>
<h4 id="2-core-site-xml-3">2.  core-site.xml</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 指定 namenode 的 hdfs 协议文件系统的通信地址 --&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop001:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 指定 hadoop 集群存储临时文件的目录 --&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- ZooKeeper 集群的地址 --&gt;</span><br><span class="line">        &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop002:2181&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- ZKFC 连接到 ZooKeeper 超时时长 --&gt;</span><br><span class="line">        &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<h4 id="3-hdfs-site-xml-3">3. hdfs-site.xml</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 指定 HDFS 副本的数量 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- namenode 节点数据（即元数据）的存放位置，可以指定多个目录实现容错，多个目录用逗号分隔 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/namenode/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- datanode 节点数据（即数据块）的存放位置 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/datanode/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 集群服务的逻辑名称 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mycluster&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- NameNode ID 列表--&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- nn1 的 RPC 通信地址 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop001:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- nn2 的 RPC 通信地址 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop002:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- nn1 的 http 通信地址 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop001:50070&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- nn2 的 http 通信地址 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop002:50070&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- NameNode 元数据在 JournalNode 上的共享存储目录 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;qjournal://hadoop001:8485;hadoop002:8485;hadoop003:8485/mycluster&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- Journal Edit Files 的存储目录 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/journalnode/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 配置隔离机制，确保在任何给定时间只有一个 NameNode 处于活动状态 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;sshfence&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 使用 sshfence 机制时需要 ssh 免密登录 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- SSH 超时时间 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;30000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 访问代理类，用于确定当前处于 Active 状态的 NameNode --&gt;</span><br><span class="line">        &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 开启故障自动转移 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<h4 id="4-yarn-site-xml-2">4. yarn-site.xml</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!--配置 NodeManager 上运行的附属服务。需要配置成 mapreduce_shuffle 后才可以在 Yarn 上运行 MapReduce 程序。--&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 是否启用日志聚合 (可选) --&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 聚合日志的保存时间 (可选) --&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;86400&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 启用 RM HA --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- RM 集群标识 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;my-yarn-cluster&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- RM 的逻辑 ID 列表 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- RM1 的服务地址 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop002&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- RM2 的服务地址 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop003&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- RM1 Web 应用程序的地址 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop002:8088&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- RM2 Web 应用程序的地址 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop003:8088&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- ZooKeeper 集群的地址 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 启用自动恢复 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 用于进行持久化存储的类 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<h4 id="5-mapred-site-xml-2">5.  mapred-site.xml</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!--指定 mapreduce 作业运行在 yarn 上--&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<h4 id="5-slaves-2">5. slaves</h4>
<p>配置所有从属节点的主机名或 IP 地址，每行一个。所有从属节点上的 <code>DataNode</code> 服务和 <code>NodeManager</code> 服务都会被启动。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop001</span><br><span class="line">hadoop002</span><br><span class="line">hadoop003</span><br></pre></td></tr></table></figure>
<h3 id="4-4-分发程序">4.4 分发程序</h3>
<p>将 Hadoop 安装包分发到其他两台服务器，分发后建议在这两台服务器上也配置一下 Hadoop 的环境变量。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 将安装包分发到hadoop002</span><br><span class="line">scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/  hadoop002:/usr/app/</span><br><span class="line"># 将安装包分发到hadoop003</span><br><span class="line">scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/  hadoop003:/usr/app/</span><br></pre></td></tr></table></figure>
<h2 id="五、启动集群">五、启动集群</h2>
<h3 id="5-1-启动ZooKeeper">5.1 启动ZooKeeper</h3>
<p>分别到三台服务器上启动 ZooKeeper 服务：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure>
<h3 id="5-2-启动Journalnode">5.2 启动Journalnode</h3>
<p>分别到三台服务器的的 <code>$&#123;HADOOP_HOME&#125;/sbin</code> 目录下，启动 <code>journalnode</code> 进程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure>
<h3 id="5-3-初始化NameNode">5.3 初始化NameNode</h3>
<p>在 <code>hadop001</code> 上执行 <code>NameNode</code> 初始化命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>
<p>执行初始化命令后，需要将 <code>NameNode</code> 元数据目录的内容，复制到其他未格式化的 <code>NameNode</code> 上。元数据存储目录就是我们在 <code>hdfs-site.xml</code> 中使用 <code>dfs.namenode.name.dir</code> 属性指定的目录。这里我们需要将其复制到 <code>hadoop002</code> 上：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp -r /home/hadoop/namenode/data hadoop002:/home/hadoop/namenode/</span><br></pre></td></tr></table></figure>
<h3 id="5-4-初始化HA状态">5.4 初始化HA状态</h3>
<p>在任意一台 <code>NameNode</code> 上使用以下命令来初始化 ZooKeeper 中的 HA 状态：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hdfs zkfc -formatZK</span><br></pre></td></tr></table></figure>
<h3 id="5-5-启动HDFS">5.5 启动HDFS</h3>
<p>进入到 <code>hadoop001</code> 的 <code>$&#123;HADOOP_HOME&#125;/sbin</code> 目录下，启动 HDFS。此时 <code>hadoop001</code> 和 <code>hadoop002</code> 上的 <code>NameNode</code> 服务，和三台服务器上的 <code>DataNode</code> 服务都会被启动：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure>
<h3 id="5-6-启动YARN">5.6 启动YARN</h3>
<p>进入到 <code>hadoop002</code> 的 <code>$&#123;HADOOP_HOME&#125;/sbin</code> 目录下，启动 YARN。此时 <code>hadoop002</code> 上的 <code>ResourceManager</code> 服务，和三台服务器上的 <code>NodeManager</code> 服务都会被启动：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure>
<p>需要注意的是，这个时候 <code>hadoop003</code> 上的 <code>ResourceManager</code> 服务通常是没有启动的，需要手动启动：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure>
<h2 id="六、查看集群">六、查看集群</h2>
<h3 id="6-1-查看进程">6.1 查看进程</h3>
<p>成功启动后，每台服务器上的进程应该如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 sbin]# jps</span><br><span class="line">4512 DFSZKFailoverController</span><br><span class="line">3714 JournalNode</span><br><span class="line">4114 NameNode</span><br><span class="line">3668 QuorumPeerMain</span><br><span class="line">5012 DataNode</span><br><span class="line">4639 NodeManager</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop002 sbin]# jps</span><br><span class="line">4499 ResourceManager</span><br><span class="line">4595 NodeManager</span><br><span class="line">3465 QuorumPeerMain</span><br><span class="line">3705 NameNode</span><br><span class="line">3915 DFSZKFailoverController</span><br><span class="line">5211 DataNode</span><br><span class="line">3533 JournalNode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop003 sbin]# jps</span><br><span class="line">3491 JournalNode</span><br><span class="line">3942 NodeManager</span><br><span class="line">4102 ResourceManager</span><br><span class="line">4201 DataNode</span><br><span class="line">3435 QuorumPeerMain</span><br></pre></td></tr></table></figure>
<h3 id="6-2-查看Web-UI">6.2 查看Web UI</h3>
<p>HDFS 和 YARN 的端口号分别为 <code>50070</code> 和 <code>8080</code>，界面应该如下：</p>
<p>此时 hadoop001 上的 <code>NameNode</code> 处于可用状态：</p>
<p><img src="https://camo.githubusercontent.com/0dfee81cee0e0ea059224ad0a0df2b04b91339ef/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4312e706e67" alt="img"></p>
<p>而 hadoop002 上的 <code>NameNode</code> 则处于备用状态：</p>
<p><img src="https://camo.githubusercontent.com/f0adb502fb1dc9f1b4d524b7ee954bb3e9ca04a2/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4332e706e67" alt="img"></p>
<p>hadoop002 上的 <code>ResourceManager</code> 处于可用状态：</p>
<p><img src="https://camo.githubusercontent.com/afbb05472c97edf7c1f06b33c65f97a80171a1de/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4342e706e67" alt="img"></p>
<p>hadoop003 上的 <code>ResourceManager</code> 则处于备用状态：</p>
<p><img src="https://camo.githubusercontent.com/dbed5c91d8834f4203f1ea9206767d3efc3b1491/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4352e706e67" alt="img"></p>
<p>同时界面上也有 <code>Journal Manager</code> 的相关信息：</p>
<p><img src="https://camo.githubusercontent.com/00e35f7ac2eb2e8638bc6013be956ce728df2283/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4322e706e67" alt="img"></p>
<p>## 七、集群的二次启动</p>
<p>上面的集群初次启动涉及到一些必要初始化操作，所以过程略显繁琐。但是集群一旦搭建好后，想要再次启用它是比较方便的，步骤如下（首选需要确保 ZooKeeper 集群已经启动）：</p>
<p>在 <code> hadoop001</code> 启动 HDFS，此时会启动所有与 HDFS 高可用相关的服务，包括 NameNode，DataNode 和 JournalNode：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure>
<p>在 <code>hadoop002</code> 启动 YARN：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure>
<p>这个时候 <code>hadoop003</code> 上的 <code>ResourceManager</code> 服务通常还是没有启动的，需要手动启动：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>拉链表</title>
    <url>/2022/06/08/%E6%8B%89%E9%93%BE%E8%A1%A8/</url>
    <content><![CDATA[<!--markdown-->
<p>历史拉链表是一种数据模型，主要是针对数据仓库设计中表存储数据的方式而定义的。所谓历史拉链表，就是指记录一个事物从开始一直到当前状态的所有变化信息。拉所有记录链表可以避免按每一天存储造成的海量存储问题，同时也是处理缓慢变化数据的一种常见方式</p>
<h2 id="一、应用场景">一、应用场景</h2>
<p>现假设有如下场景：<br>
  一个企业拥有5000万会员信息，每天有20万会员资料变更，需要在数仓中记录会员表的历史变化以备分析使用，即每天都要保留一个快照供查询，反映历史数据的情况。在此场景中，需要反映5000万会员的历史变化，如果保留快照，存储两年就需要2X365X5000W条数据存储空间，数据量为365亿，如果存储更长时间，则无法估计需要的存储空间。而利用拉链算法存储，每日只向历史表中添加新增和变化的数据，每日不过20万条，存储4年也只需要3亿存储空间。</p>
<h2 id="二、实现步骤">二、实现步骤</h2>
<p>  在拉链表中，每一条数据都有一个生效日期(effective_date)和失效日期(expire_date)。假设在一个用户表中，在2019年11月8日新增了两个用户，如下表所示，则这两条记录的生效时间为当天，由于到2019年11月8日为止,这两条就还没有被修改过，所以失效时间为一个给定的比较大的值，比如：3000-12-31。</p>
<table>
<thead>
<tr>
<th style="text-align:center">member_id</th>
<th style="text-align:center">phoneno</th>
<th style="text-align:center">create_time</th>
<th style="text-align:center">update_time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">10001</td>
<td style="text-align:center">13300000001</td>
<td style="text-align:center">2019-11-08</td>
<td style="text-align:center">3000-12-31</td>
</tr>
<tr>
<td style="text-align:center">10002</td>
<td style="text-align:center">13500000002</td>
<td style="text-align:center">2019-11-08</td>
<td style="text-align:center">3000-12-31</td>
</tr>
</tbody>
</table>
<p>  第二天(2019-11-09)，用户10001被删除了，用户10002的电话号码被修改成13600000002.为了保留历史状态，用户10001的失效时间被修改为2019-11-09，用户10002则变成了两条记录，如下表所示：</p>
<table>
<thead>
<tr>
<th style="text-align:center">member_id</th>
<th style="text-align:center">phoneno</th>
<th style="text-align:center">create_time</th>
<th style="text-align:center">update_time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">10001</td>
<td style="text-align:center">13300000001</td>
<td style="text-align:center">2019-11-08</td>
<td style="text-align:center">2019-11-09</td>
</tr>
<tr>
<td style="text-align:center">10002</td>
<td style="text-align:center">13500000002</td>
<td style="text-align:center">2019-11-08</td>
<td style="text-align:center">2019-11-09</td>
</tr>
<tr>
<td style="text-align:center">10002</td>
<td style="text-align:center">13600000002</td>
<td style="text-align:center">2019-11-09</td>
<td style="text-align:center">3000-12-31</td>
</tr>
</tbody>
</table>
<p>  第三天(2019-11-10),又新增了用户10003，则用户表数据如小表所示：</p>
<table>
<thead>
<tr>
<th style="text-align:center">member_id</th>
<th style="text-align:center">phoneno</th>
<th style="text-align:center">create_time</th>
<th style="text-align:center">update_time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">10001</td>
<td style="text-align:center">13300000001</td>
<td style="text-align:center">2019-11-08</td>
<td style="text-align:center">2019-11-09</td>
</tr>
<tr>
<td style="text-align:center">10002</td>
<td style="text-align:center">13500000002</td>
<td style="text-align:center">2019-11-08</td>
<td style="text-align:center">2019-11-09</td>
</tr>
<tr>
<td style="text-align:center">10002</td>
<td style="text-align:center">13600000002</td>
<td style="text-align:center">2019-11-09</td>
<td style="text-align:center">3000-12-31</td>
</tr>
<tr>
<td style="text-align:center">10003</td>
<td style="text-align:center">13600000006</td>
<td style="text-align:center">2019-11-10</td>
<td style="text-align:center">3000-12-31</td>
</tr>
</tbody>
</table>
<p>  如果要查询最新的数据，那么只要查询失效时间为3000-12-31的数据即可，如果要查11月8号的历史数据，则筛选生效时间&lt;= 2019-11-08并且失效时间&gt;2019-11-08的数据即可。如果查询11月9号的数据，那么筛选条件则是生效时间&lt;=2019-11-09并且失效时间&gt;2019-11-09.</p>
<h2 id="三、表结构">三、表结构</h2>
<p>MySQL源member表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">member</span>(</span><br><span class="line">             member_id <span class="type">VARCHAR</span> ( <span class="number">64</span> ),</span><br><span class="line">             phoneno <span class="type">VARCHAR</span> ( <span class="number">20</span> ),</span><br><span class="line">             create_time datetime,</span><br><span class="line">             update_time datetime );</span><br></pre></td></tr></table></figure>
<p>ODS层增量表member_delta,每天一个分区:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> member_delta(</span><br><span class="line">             member_id string,</span><br><span class="line">             phoneno string,</span><br><span class="line">             create_time string,</span><br><span class="line">             update_time string)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="keyword">DAY</span> string);</span><br></pre></td></tr></table></figure>
<p>临时表:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> member_his_tmp(</span><br><span class="line">             member_id string,</span><br><span class="line">             phoneno string,</span><br><span class="line">             effective_date <span class="type">date</span>,</span><br><span class="line">             expire_date <span class="type">date</span></span><br><span class="line">             );</span><br></pre></td></tr></table></figure>
<p>DW层历史拉链表:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> member_his(</span><br><span class="line">             member_id string,</span><br><span class="line">             phoneno string,</span><br><span class="line">             effective_date <span class="type">date</span>,</span><br><span class="line">             expire_date <span class="type">date</span>);</span><br></pre></td></tr></table></figure>
<h2 id="四、Demo数据准备">四、Demo数据准备</h2>
<p>2019-11-08的数据为：</p>
<table><tbody><tr><td>member_id</td><td>phoneno</td><td>create_time</td><td>update_time</td></tr><tr><td>10001</td><td>13500000001</td><td>2019-11-08 14:47:55</td><td>2019-11-08 14:47:55</td></tr><tr><td>10002</td><td>13500000002</td><td>2019-11-08 14:48:33</td><td>2019-11-08 14:48:33</td></tr><tr><td>10003</td><td>13500000003</td><td>2019-11-08 14:48:53</td><td>2019-11-08 14:48:53</td></tr><tr><td>10004</td><td>13500000004</td><td>2019-11-08 14:49:02</td><td>2019-11-08 14:49:02</td></tr></tbody></table>
<p>2019-11-09的数据为：其中蓝色代表新增数据，红色代表修改的数据：</p>
<table><tbody><tr><td>member_id</td><td>phoneno</td><td>create_time</td><td>update_time</td></tr><tr><td>10001</td><td>13500000001</td><td>2019-11-08 14:47:55</td><td>2019-11-08 14:47:55</td></tr><tr><td bgcolor="#DC143C">10002</td><td bgcolor="#DC143C">13600000002</td><td bgcolor="#DC143C">2019-11-08 14:48:33</td><td bgcolor="#DC143C">2019-11-09 14:48:33</td></tr><tr><td>10003</td><td>13500000003</td><td>2019-11-08 14:48:53</td><td>2019-11-08 14:48:53</td></tr><tr><td>10004</td><td>13500000004</td><td>2019-11-08 14:49:02</td><td>2019-11-08 14:49:02</td></tr><tr><td bgcolor="#6495ED">10005</td><td bgcolor="#6495ED">13500000005</td><td bgcolor="#6495ED">2019-11-09 08:54:03</td><td bgcolor="#6495ED">2019-11-09 08:54:03</td></tr><tr><td bgcolor="#6495ED">10006</td><td bgcolor="#6495ED">13500000006</td><td bgcolor="#6495ED">2019-11-09 09:54:25</td><td bgcolor="#6495ED">2019-11-09 09:54:25</td></tr></tbody></table>
<p>2019-11-10的数据：其中蓝色代表新增数据，红色代表修改的数据：</p>
<table><tbody><tr><td>member_id</td><td>phoneno</td><td>create_time</td><td>update_time</td></tr><tr><td>10001</td><td>13500000001</td><td>2019-11-08 14:47:55</td><td>2019-11-08 14:47:55</td></tr><tr><td>10002</td><td>13600000002</td><td>2019-11-08 14:48:33</td><td>2019-11-09 14:48:33</td></tr><tr><td>10003</td><td>13500000003</td><td>2019-11-08 14:48:53</td><td>2019-11-08 14:48:53</td></tr><tr><td bgcolor="#DC143C">10004</td><td bgcolor="#DC143C">13600000004</td><td bgcolor="#DC143C">2019-11-08 14:49:02</td><td bgcolor="#DC143C">2019-11-10 14:49:02</td></tr><tr><td>10005</td><td>13500000005</td><td>2019-11-09 08:54:03</td><td>2019-11-09 08:54:03</td></tr><tr><td>10006</td><td>13500000006</td><td>2019-11-09 09:54:25</td><td>2019-11-09 09:54:25</td></tr><tr><td bgcolor="#6495ED">10007</td><td bgcolor="#6495ED">13500000007</td><td bgcolor="#6495ED">2019-11-10 17:41:49</td><td bgcolor="#6495ED">2019-11-10 17:41:49</td></tr></tbody></table>
<h2 id="五、全量初始装载">五、全量初始装载</h2>
<p>在启用拉链表时，先对其进行初始装载，比如以2019-11-08为开始时间，那么将MySQL源表全量抽取到ODS层member_delta表的2018-11-08的分区中，然后初始装载DW层的拉链表member_his。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> overwrite <span class="keyword">TABLE</span> member_his </span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">   member_id,</span><br><span class="line">   phoneno,</span><br><span class="line">   to_date ( create_time ) <span class="keyword">AS</span> effective_date,</span><br><span class="line">  <span class="string">&#x27;3000-12-31&#x27;</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">	member_delta </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">	<span class="keyword">DAY</span> <span class="operator">=</span> <span class="string">&#x27;2019-11-08&#x27;</span></span><br></pre></td></tr></table></figure>
<p>查询初始的历史拉链表数据:<br>
<img src="https://img-blog.csdnimg.cn/img_convert/b7b2455db73f3992a2dd519d052be2ec.png" alt=""></p>
<h2 id="六、增量抽取数据">六、增量抽取数据</h2>
<p>  每天，从源系统member表中，将前一天的增量数据抽取到ODS层的增量数据表member_delta对应的分区中。这里的增量需要通过member表中的创建时间和修改时间来确定，或者使用sqoop job监控update时间来进行增联抽取。比如，本案例中2019-11-09和2019-11-10为两个分区，分别存储了2019-11-09和2019-11-10日的增量数据。<br>
2019-11-09分区的数据为:<br>
<img src="https://img-blog.csdnimg.cn/img_convert/bf52c4d3d45507868059227dff275a86.png" alt=""></p>
<p>2019-11-10分区的数据为：<br>
<img src="https://img-blog.csdnimg.cn/img_convert/ccb1deaca665346cff84cbf1c8e694af.png" alt=""></p>
<h2 id="七、增量刷新历史拉链数据">七、增量刷新历史拉链数据</h2>
<h3 id="2019-11-09增量刷新历史拉链表：">2019-11-09增量刷新历史拉链表：</h3>
<h4 id="将数据放进临时表：">将数据放进临时表：</h4>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> overwrite <span class="keyword">TABLE</span> member_his_tmp</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (</span><br><span class="line"><span class="comment">-- 2019-11-09增量数据，代表最新的状态，该数据的生效时间是2019-11-09，过期时间为3000-12-31</span></span><br><span class="line"><span class="comment">-- 这些增量的数据需要被全部加载到历史拉链表中</span></span><br><span class="line"><span class="keyword">SELECT</span> member_id,</span><br><span class="line">       phoneno,</span><br><span class="line">       <span class="string">&#x27;2019-11-09&#x27;</span> effective_date,</span><br><span class="line">                    <span class="string">&#x27;3000-12-31&#x27;</span> expire_date</span><br><span class="line">   <span class="keyword">FROM</span> member_delta</span><br><span class="line">   <span class="keyword">WHERE</span> <span class="keyword">DAY</span><span class="operator">=</span><span class="string">&#x27;2019-11-09&#x27;</span></span><br><span class="line">   <span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="comment">-- 用当前为生效状态的拉链数据，去left join 增量数据，</span></span><br><span class="line"><span class="comment">-- 如果匹配得上，则表示该数据已发生了更新，</span></span><br><span class="line"><span class="comment">-- 此时，需要将发生更新的数据的过期时间更改为当前时间.</span></span><br><span class="line"><span class="comment">-- 如果匹配不上，则表明该数据没有发生更新，此时过期时间不变</span></span><br><span class="line"><span class="keyword">SELECT</span> a.member_id,</span><br><span class="line">       a.phoneno,</span><br><span class="line">       a.effective_date,</span><br><span class="line">       if(b.member_id <span class="keyword">IS</span> <span class="keyword">NULL</span>, to_date(a.expire_date), to_date(b.day)) expire_date</span><br><span class="line">   <span class="keyword">FROM</span></span><br><span class="line">     (<span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line">      <span class="keyword">FROM</span> member_his</span><br><span class="line">    ) a</span><br><span class="line">   <span class="keyword">LEFT</span> <span class="keyword">JOIN</span></span><br><span class="line">     (<span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line">      <span class="keyword">FROM</span> member_delta</span><br><span class="line">      <span class="keyword">WHERE</span> <span class="keyword">DAY</span><span class="operator">=</span><span class="string">&#x27;2019-11-09&#x27;</span>) b <span class="keyword">ON</span> a.member_id<span class="operator">=</span>b.member_id)his</span><br></pre></td></tr></table></figure>
<h4 id="将数据覆盖到历史拉链表">将数据覆盖到历史拉链表:</h4>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> overwrite <span class="keyword">TABLE</span> member_his</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> member_his_tmp</span><br></pre></td></tr></table></figure>
<h4 id="查看历史拉链表">查看历史拉链表:</h4>
<p><img src="https://img-blog.csdnimg.cn/img_convert/cbd380e301b1096d4c404c102cd79854.png" alt=""></p>
<h3 id="2019-11-10增量刷新历史拉链表">2019-11-10增量刷新历史拉链表</h3>
<h4 id="将数据放进临时表">将数据放进临时表</h4>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> overwrite <span class="keyword">TABLE</span> member_his_tmp</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (</span><br><span class="line"><span class="comment">-- 2019-11-10增量数据，代表最新的状态，该数据的生效时间是2019-11-10，过期时间为3000-12-31</span></span><br><span class="line"><span class="comment">-- 这些增量的数据需要被全部加载到历史拉链表中</span></span><br><span class="line"><span class="keyword">SELECT</span> member_id,</span><br><span class="line">       phoneno,</span><br><span class="line">       <span class="string">&#x27;2019-11-10&#x27;</span> effective_date,</span><br><span class="line">                    <span class="string">&#x27;3000-12-31&#x27;</span> expire_date</span><br><span class="line">   <span class="keyword">FROM</span> member_delta</span><br><span class="line">   <span class="keyword">WHERE</span> <span class="keyword">DAY</span><span class="operator">=</span><span class="string">&#x27;2019-11-10&#x27;</span></span><br><span class="line">   <span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="comment">-- 用当前为生效状态的拉链数据，去left join 增量数据，</span></span><br><span class="line"><span class="comment">-- 如果匹配得上，则表示该数据已发生了更新，</span></span><br><span class="line"><span class="comment">-- 此时，需要将发生更新的数据的过期时间更改为当前时间.</span></span><br><span class="line"><span class="comment">-- 如果匹配不上，则表明该数据没有发生更新，此时过期时间不变</span></span><br><span class="line"><span class="keyword">SELECT</span> a.member_id,</span><br><span class="line">       a.phoneno,</span><br><span class="line">       a.effective_date,</span><br><span class="line">       if(b.member_id <span class="keyword">IS</span> <span class="keyword">NULL</span>, to_date(a.expire_date), to_date(b.day)) expire_date</span><br><span class="line">   <span class="keyword">FROM</span></span><br><span class="line">     (<span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line">      <span class="keyword">FROM</span> member_his</span><br><span class="line">    ) a</span><br><span class="line">   <span class="keyword">LEFT</span> <span class="keyword">JOIN</span></span><br><span class="line">     (<span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line">      <span class="keyword">FROM</span> member_delta</span><br><span class="line">      <span class="keyword">WHERE</span> <span class="keyword">DAY</span><span class="operator">=</span><span class="string">&#x27;2019-11-10&#x27;</span>) b <span class="keyword">ON</span> a.member_id<span class="operator">=</span>b.member_id)his</span><br></pre></td></tr></table></figure>
<h4 id="查看历史拉链表-2">查看历史拉链表:</h4>
<p><img src="https://pic.downk.cc/item/5ff6ab7d3ffa7d37b311909b.png" alt=""></p>
<h4 id="将以上脚本封装成shell调度的脚本">将以上脚本封装成shell调度的脚本:</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="language-bash">如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span></span><br><span class="line">if [ -n &quot;$1&quot; ] ;then</span><br><span class="line">	do_date=$1</span><br><span class="line">else </span><br><span class="line">	do_date=`date -d &quot;-1 day&quot; +%F`  </span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line">sql=&quot;</span><br><span class="line"></span><br><span class="line">INSERT overwrite TABLE member_his_tmp</span><br><span class="line">SELECT *</span><br><span class="line">FROM</span><br><span class="line">  (</span><br><span class="line">-- 2019-11-10增量数据，代表最新的状态，该数据的生效时间是2019-11-10，过期时间为3000-12-31</span><br><span class="line">-- 这些增量的数据需要被全部加载到历史拉链表中</span><br><span class="line">SELECT member_id,</span><br><span class="line">       phoneno,</span><br><span class="line">       &#x27;$do_date&#x27; effective_date,</span><br><span class="line">       &#x27;3000-12-31&#x27; expire_date</span><br><span class="line">   FROM member_delta</span><br><span class="line">   WHERE DAY=&#x27;$do_date&#x27;</span><br><span class="line">   UNION ALL </span><br><span class="line">-- 用当前为生效状态的拉链数据，去left join 增量数据，</span><br><span class="line">-- 如果匹配得上，则表示该数据已发生了更新，</span><br><span class="line">-- 此时，需要将发生更新的数据的过期时间更改为当前时间.</span><br><span class="line">-- 如果匹配不上，则表明该数据没有发生更新，此时过期时间不变</span><br><span class="line">SELECT a.member_id,</span><br><span class="line">       a.phoneno,</span><br><span class="line">       a.effective_date,</span><br><span class="line">       if(b.member_id IS NULL, to_date(a.expire_date), to_date(b.day)) expire_date</span><br><span class="line">   FROM</span><br><span class="line">     (SELECT *</span><br><span class="line">      FROM member_his</span><br><span class="line"> ) a</span><br><span class="line">   LEFT JOIN</span><br><span class="line">     (SELECT *</span><br><span class="line">      FROM member_delta</span><br><span class="line">      WHERE DAY=&#x27;$do_date&#x27;) b ON a.member_id=b.member_id)his;</span><br><span class="line">&quot;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="language-bash">hive -e <span class="string">&quot;<span class="variable">$sql</span>&quot;</span></span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>解决方案</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>canal 基础入门(基于1.1.4)</title>
    <url>/2022/03/06/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E4%B8%8E%E8%BF%81%E7%A7%BB-canal/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="27b5fcb1739495f52fc766fd51b50417359da9db0b0f640b85652e41763eaa75">a8cefacc35a4b9cc130a23421f1e88bcd3da19bc85c03fe1ed0e9e63bbb759353c16dbc78f2aef8cc1d17774c43cfdafd82c229b72143c11fd6134bd3c1ecabcc3dfbbbba1b1b226f372f12094b387b3b7b78ac12a9c893a10da4dd077db42b8a9833ec73d59624f6d36730c080ff2b40442804e3f83b7607dc86e35862665fad9d3874f3eb174bb2cfd17a00a42ab6dbee3208a026140b55cbd2f356674adfab174028cd4710249bdbfcbc52c8b6def79f4f4a80906e4e7b1602d7eb3f64da57bc61b62e4c3f78604f4a80bfe34a053873e85b406124d006fcd0e426391d6ad313c4529595f8a7b0f529e0528fd1891b9a3689e073a0a27111066aa4877e136fab7df74b6d365b9bfff7fd28e8092cd368750a579e2a7f30cc8e3779adf1fd2e6888bfa2baf03ff1e69b0aa7b340d809eaf603db86760a0a145a6cf8e0dcc295935933456dce67a158c30b1aca409ad6a738727293ec06a4909892d5c807d70223a154b76ae42b73a87dfd34f7c4121fd398cb06a8f8fc1f1a6e0692b3fe9409f1f833269cee20b9a774a62a1aa0b3539d62967d385f452428f716d573d11a9f9d4a2b7204a1bbf8935c75fe2d122ea1ff8477efc1670b1fca48fd27c56529b3fa15ead629d34b8fb46a0eb877a16e1a73d61da5a6dc385f944ea4a698b0ffa6dc92ebaf13c7a11cc6658e8d8f385cd196bcc7ddd0e724b95c746b58828af422869cd661499dae22ac864a87ab18371cff542bc49d62d09ae23bbd6614d422aadc5e54bc311f1e4293022536940bd20872b9ea775df2f27332e96cd61ac30fb1c482cb35d9ad2141cddb6c948002863d9efb58f31cda81b2ab755f71fab34db4e5ae2bbccc6646545aa83e31d4f46f82d6ca93a69f9e4b7e76e2f0395b4a8304d69120408e1e2867ecfcbaae09f2e1ec3f91b5d13791dd55a67638ab4f860b010af585c0848b09f3bb522a524ec654db012d7bbd79c4d4735f20e8d0c521a4a38f676068e687fc8a1097f3f814e9fe8a196316d90ea65dd64b0476a8d6888d93eaf6862f03053fc163794c63004041d2b56ad682353b26e5b36698bcb8e0145d8413cf8cccc9d6530669edfc2d2f9ffc1a4f24eef6af0c42434e9da531b4637d526f6e3d6d83e2c84ba0aea4380801f98dbd1ec70f45edb8475f6dbdadcdeb9402f71cf1c2fa6a0cd2c45b3d48613d8261afd93b39d2c88dd4df11ca462a850f2424c287f3a9316828a6d1440fdd41df8077ac9d3db21a175d68bca3219cc4f360c42450bfb735ca5a161e0c44097eec26c444e3c436ec32a2a663c27b230a11d5de1b572423462749469ad8043ac627ea71cab177ca3877c8e30eb502a03d2c0d4244873627b2ab827540e89b298bfb42d5cc6834cb43f895c8f3282586fa27e54363f8a99361b96819243b2f45a56d63ce466f348991530a636471baf9591af4142c423c2f5fefdc5a1d6862d22e1422e9c1db312d1fe8d6341c76891b970b7e6fd023d9feb6a9befdd2af325c131567a3ec42dfa558743fb1e7ec1999da7a107d02a79968a9ba69bdfb45a987248c5274e3dcb33c809deeb530b02bab584ff515c880b42b154c329a9a3d4ee19237cdfdad6fdff4992dd4bb67c191a48fc325e56df1c4f45cb2ada4c4b70d4d0c374c44cb0984b832e8f495296883db707c590b2d361aabe3a27f395b05cb1d77fc91c48fa203cf2c82fe11cf2af8da90159d6c698fe1eb6887f971583fac9da5b72cdf6771a38edde76f6fe682f8f6257ed97c99b283f02b458b50d6aae67ab39e1ac3c5862fc787576696650c63b529d643f790b07c55580aa4cf5770b887ec274e5359d19010b77d15504b8d7bdd8074114a818cf09a5a8bee242667cc8e81b120bee675f83e8f51cb39c0f267cfbb4c89658898106e7124bb58e68d6ea325f78e8bb5bbccfd2051219d52c1ee4e0d2c0241e48b766a552c3fdba47b3451d4e8da42fd7476cc720df2f6ecd3f72fedb2446782a7c8fa8fc85271a5390d25ea2d979802e7a6c821fe7893978e48cdddba0ea97064aaf756b6036ed690de0517f72942d9c266cf9fa0722621011324275403e3e11c049327e5067327393dc72b9c262ff5437bbd4423dd4f15ce3024209c79f509054d3cb423d77b39b426203a80824fc517b215ba2a068e1ae520973b616f2263812ff4728b7d7da970940aaf02a9bc8d550e097d9c27b177e00d46a7587d8476fd15edb5c48bd96b38b4dcf6ebea5fcaddbb9f77b71d3e793584e37d1d131cf7280d5e242fb246e27856ccc4d030a74306e2990763cae664dc6fcd8c4244c6e47767754fad68cf4276ee617f7ca4df8e95ae174f74efb257b03af0f5b60da6a6f52947103c2dd8df52bcdedb21861ce39553f18c8bae693bb5953973a62838af927f10d775b3cf1afb52404a2e0cb8a229baefa8c518c1016db91674433122893d19b5d90f08431ee8a1a989f545cbbce2c3722919b8c476291c9b1c293fec545feb059ab6ae1dc2c12f0e8c04ad2f2a0ad8af90387cdde23a04f0d1628e479c030fb09b0263f979cc9944627e2cb978b313444e5672b0f9fdaa41e624b23692556839f1ffe336a4fd664fcf4a8bb2285ac9fe4c13cf30beffb3a895f8b53430cbedffa89b6734f0b701c2fc980287d9c30ac9dca73d98e4fc26f5cb976d19c9cbafd4f5ef4ba89ec1d274d420ff7563c0714841e24be589f37de028718dfc22ef7a551b6685d7408ab82d7ddc9acd36e5f33b75b604ffbb033113a87b79dfd3164858fbeba5ed18c0fc83f4b14f1c775eef584ca1481e1ab3da4f355330f3a83c05e0ecd681064a3554c97aa2a873109a3564881c70298535c3a0035d6adeb8c3300960e314639e1973e30f52c1d7b14cb76dbdbd140afa375bb02b7ed4e527cd0cc87254ba70cc94499a0b5964c51875c4132233b303b0a61adb5545952505c5c611cf41fa802da11d164f49550c17beb5ed0f634a5392c74b93e70334d84f1141344eb203cb9018af3937f484e6c4b714536cea440e3235effe17a27dc997f40ac5b04b8693356e6d70cf6e91698e6e621e2e9a804b448ab5699207d36b54ebd1e45458eb74c8c6d38c12cb2d28ecf772b873d98bc3c5bc0ac5809c99d320adeb1271f1b1f64e6ec9d95a97f5700fc47948d9c9c991791d849c774198d345d5818d03c48fcc732583ab4c2f7b39ef4100454dff4a8940258e792ad4604caf2520bfba3eb0eadfc203a300b357474097213eed4a5bef20557eeb8cf51e1c429d5391f2215bc8d505a45c65f8ee3de39a66f3863b3e6429e47d1a784702b105552236fb19ea419fd03e8dfe20635f62d8aab5dda7b5175f7a8fd8cf1ea991944b6237a35335a87cc1f874406c54f29929171bc5535ae033cde6c94f89d4db7eb0eb4034053194454b9bd7bf1ea4a9ba382e0cb002b6ca6d31a7fc9a0cd6d566acf02c3d7e98e2628cb58a1f1fc1ed82218ac4e8d01911dab442987cc4eedd6bd40cc8bc2d8cac8e2aeb33c78ace436895798d9770e9d2bd3fc309be3214b584770b46995ffc9115df1e09ae67a90f25bb550eb3b6f803fdbbdf309ad966b2bb7e4723f62c141d1fceac5f98d183da7e3f7cf8f26187daa9e4d1ed155686045ac92abfff6889e50a6a851c22f2a973a0f6d66710f018120b46302f819e697f1d33e52978d78815107926ba50d14db06d0f46f4ffc25d33a20181dcbd0105cd668aeb408a1da3a3c0e287e111d608eae976c781d846ba0349ccc5d2956c18678a0cf7cd483dd47ecd249c4ad2f8fc4d42c50f3a6054e9d528fb6934eaacc5d3f120a3ba031a4aa8d485c792d1997ab641877f799458cfd8d6f1683d62389c5e605e969a7e1259d10131cdcc59a3b4563d8e9a24c90ec03a8cfd5f3c5f3181589fc0734fa471d9109b1758e1513797ebbbc34d4970bf9c61e434bd1a7783a00ba3be3180e7b87c39f3fedb0e083b074ee31f4cdcb983ed1c874d2dec41ca6dff7289a65afe573e64ad365a0aafdd190ad6ebd67021d581d514a34600d7f98874edf31274042965865bf57d0ef72e86d43b63ea8b66f9495a53efb5bcfbe3747204c4335717f49330936399a4c8a7e1b9f05c7e6f43d5986d04d5560d629c7c71eb27ccb7603ce0ce795006de5c9d24c2735cbcb2bd2b2e92e65210afd37ee089d384df95aaf397af75cec82c0f32e3fcb54abc43fce0f3e8101c8127a46a08c0bf01b761fefc03bade95f4b18cc2b2297eb853df63abc24bd49984beaaedd7afbc7958da6031175ad909e1c703815ee5dd14c7ed057a71dff413da17a425dff3ce90c2e3ca476c2960a077f86b4df687f2bb0b7cfdaa093efd94d30f94fc523647d5d12ecdf87064ee9cfee0e62169d8e2bd8545d14552a6e02bbe56baa4134f5ecf0ad1e71e84d77eaff320caf8bd8c40f1c0c9eca585d225de47e1db54d0b6a0a02354e8a3f3b28531c84e0558c2485516606733c3f35c88243959c8e48eeada7d8076326ad1af038373367c376eba9e668468ab001321f535f22d4d3b653e050ca646d711a0571ef69a5239455b3b4b8b707db35c8fab66f0fbc413d3de387f242a1f78cea80343e25d18baffb41f03990a7b33c9214dfc1caa02d125aa586fb4f90dd76e2df6d48d59bf351cc1667e5f68b6c920fedb08f37f01cd801de7dc70d1f016cf2a322c0f4a0ff390aa8b3349a3c61bdf61091ee9caadac6dba91a885346bff785adefefa3972c8b884018fa030b50a126966183c34d6cd9d1a6b04863270a8295691e96e12e4f0b67794fe6d1af606d7e24d3ae1ee8856866c67d0e2a521ed92d634f51e9ed7c4926378c412344fe00a3a799aa89102ac085e40bb9136d9d94a57cd73623ae915b87b91c8039570f0abe4da4a60d172df521351faaf16bf6d9d42f8eb1509ecf31cc1b02b6838a011c31b97ee9cc0686a0805110bd6123056c0354a9596a47bcd5042cbfd232528cef2eab6a17913299687aecaf9349f868e7041897d3a3cb324d0fe656b1c4cee3a14fd32c8c1594af95345345d355217195b204da21b235ef87bb4f2954007acdc80cd09c3d8d6343b6c92fb33b3f9726e6ea1bdfdac501947cc41871cd48a744741ed5107bb69272a129ee53a384294185ce7002bce301c851efdbd288fce60b11b224960022b220ddcf5730300a74a1ef465217525ae3aad9a1cdced7af463322cc313c20f6fb998212dde91a5802338ae68662e8d9d7cc6699330f28c2bd53d655bead7e3742e1e4612f01266e9cf34d5c0bef29c7982b107412cf21c2eb01f89dbd4517b6f9f6b62a518faddc83966e24a7f154dbf01e6de35538f8bec93620c82118e3d6f2ad25a4867e9ae5cc02341352938a62d379bbb19e0eb4f29b36e0cb4c120920ea58a4fc574be21c0ca25b3e15fa486ad855cf1bfeabc5074c99656307a7e90a2c594fe12594e57d365c642b29e1ab79a373be9f297e80a00567a0a4f508787d3f693f2e5a3a433ed7a81ac84835ac8be1b815b85fd99040172c2d1c7a2c35c7079cf171f727a91cc5c64b2f4da2134bb5a694c244786ef33a4feb12c2a8872e88cb1a9c6ea462edac7be2cf0ab6a7bb5df4e0f045d4b60243e552781a15a7d2cbc60d2c7888eee8add3c46835abfb13cb175be0f1878e48fdcceebb02d0df21edcac222cb93e775cb1287cfb83fb5c9701e7c91c5b3cc845a85c4579f7a5d54c29a4f1939ea6cc860601f7d9e17fd032ed3a07c6dcef46a55a709cf5bdb3d12c77bb843562b39e5c43772d42d9a5040415d90da53522ba1e9294c535ef8cfa46c5dedb4994ef7613d6d81757d3153191eaa340f70622f3c9aee1a4ec669b9cc89e19e8b5fc08114f3603c1bfe305cdeebf9918b44cf75b4c84308becef404189e635eea7a32c49cbeef2c0d720eaaab6a1406762c75534daa576be94e8d37cee32df216df23351430e16ccb9ca45ede53cd716accc23ebc4fa53f426eb927c6194198fcd81c5682ec084c08f8396e4f3a726decc68fe3ee731437b213894cc597492c5df7f886470affc9990983145e5f447fcff0fd7cb6bbb81d4e83343f54aa1be51ded2b59477559d402164fbfa5ab3cb259650c5e5cb7fc69d60ac35e97c0b477015489624f0fa07d408ccd3e7f86c8c461c99402adaf70f0b06b3384206247388c04eea34a8d59943c332c98dc1c0654a99782fee837cf746b8899e60b380fbae7b6b9598f5dd2400dc4c5612e915608c67a9864e9640403a7fdd388db93663e8407352f84472acb87bfceb9da1c7bfafe2dc9d06b8d6507339fa327546aa10a6afe7ae78a86eb51bb55da5f19be8b2d872bc2fafa8229e14b494e45fb7914d296d9ff401456ed1884ca63b12386c7de127d5a5c869ed2dc7050b418e8e1f940cdd8ecf14a10d408b4e4582bfdd3982704b98d96c8c28a3827d718ad6af0c82b7f4017b2ba04adae0fc708b056351d28a9aa57cd2252ec31f58b849ac871fba9c0050619f2a50a4c67c1dc7f527243cfab6f634cb4e71d069785d5e50289d09949df76e2902f894b6990df2581e4188d6259f1e207b9d3abd7a519d6f5d4f14e49553e68b88a24b435eb6076cbfbc7d40914362e9719794bec689b670e3af3da35cbbe707953a5b8fc6b17ab2eb22839f9020c3af87f295d6761cb0befae5cc2c2f482979c907d9eea02fb0031f351020144809cecf6c984d53298a805ed1ff442e29c673235af78de09463c6c5e1ae0a81d405687d93bfe6ee1743f88ff49517a91af7420704f47f40ebd3ec8aeb43fa6ac94f3cbd7c9303f6e2e1e30cd26ee792629ac43afad061bbc70bc9c4fc2ee9975e5333f24c0a0dd3c94b9772eec14dd3237abd70f989f1930982bc0991e8f358e93b97173ec122a3e74ef73b470feeb255cc7e18b59f6649285e3e0c5ef121edbaa68e98a69984fbf859a31b96f3e4acfc3727f3ae64d780c824ff45968653a4c0ac1254df28df688dcf5318473cb5a87e453fceba1e0c74cbf2d3b86922cdd75a00aa435f088afd8bd3faf5887a46626341b0ef0655162ece67071908cbe1046ecc71e63e888be78c862ac0fdbfe56cd8a79377d63cf606665eea4ec52b2f0eae2f96b3582f97ed4eebb8636eba391c5d18760f7b3fa6fcb313200669b9b4c048e5acb9915f3a395ea758f5ef68f11bd6d1cf42478de839a46b9178ade0c8ac61bc4e703d2503df345ab7fc4dccaa0ea1006d02c30a57f01bf175d6120a8deb8626e5131ae832637fd1672ab9314ccbb4c86fef12f349c77e075c4abeb3207e5a6172f8f7108651c4cc5466865293c6bc49cd6d6d1b996c53fc502cb085f21dc25371becccb18728876e2102c71f9724d3b603b7c7115185940d0dcb30071327ae80845cf89a6422210eaedd2d5ca6615867afae7c7583608d3d105e9d03d74c14a22947e3f346e0f322f7e2abf1c8a5ab4eddac7b4159788f02df6ebea1d640b10fe6ec65e1307ca0fe6999c1070f6c8244fe38ea2366ecf74f94c89fe2c73b3cf11e7ad66529dc5a11dcc7fdd076db434c533c9221a55b3f1784704dacccc004915ee0dc0fd3e5d4f59d11f7dcde6047f0620c8f6bb1a112fcdfb7debbde21d63ac0e30dcdd7cf54146c15fe6df14b9ef20356d8a2de48856fff958dc7cfc25142f41de9d5d0a3963e43bc9fc142f51c4521dca271665505a1c9e53a7440547d74ae207d2278b1e15401f10a23ba356293eda1aa1e85c1bceb889e7f888e1ee767ce46a38e635a5927ab395b98c720807e2a697d54944f95234dafb3131d15fc495b6e50bdc841b48716d66466cac7a1c0deae4d9a6166a4109ac2860657abcf277374364fd7ea4126d940a57e7d7fbc7a95afd15629751dad13e84a601b5d2b86b59e8ae6bcdde5ab26a7617296512aac7bd135f101ba76a41830bd64453c08597b9163d43af69c62c867dc9c76acb6677cc2bd4c633a97c7ee499c5519af65abce9793eb83ab21b5218f7b54c52148ef4934c95d2d6a748e47b26f9d0592bb55d888e48beafcda544a7952fc8d06bae57820cff85d568560bc565d0a61f37793fdfc482351bbed939d092970cf26e3a7c7e2af9f6ea49085e8883260d1c75521f71fbb8be8ef70a0075b64a3935233994550cee34d30bfab6bf698c93f2f27bcfabd43da85130554c69ed39f6cb73ae481f31d8f07d7b3c2ab1c0f6e785344715f13fe471baacb42172cea5dd843726bc78b405a95e53565ca6a9a834a39d0bffe78ba0063e0e0aecfc839c38e99e75c84dd7f1988e35aa558e108c36aba50c773d9dfb521031ac22713f57b111762fddb24eb9d2478a8aa673c950001c51eee85e3600e043816d81c8e543731226ae4cd76a7126af26d3a3841fc2e56bfe118206fa03f730358059b239963295800420af6b3506838bd69402629c887f9bcf3bf44c5efa21773fe9156825102ccfa71268141ae57fca14e09b4145ac78178428c9725a49b4227a6ef479c865c8a1486ff43178eb05fddb230cb36315f6cff0a9891a0b78a6ded0f81fc25dcdbf8cd2fa5df4ff1154ce6785d292b57023be5d2511661ddb58c211273afe0fa5b2d9d856df7910cb5c257eebe306ef8715dbb1dc5be0a4945ca700d3f1c5b9fb6d0e0e9c800ee41e750de1f33b9b870a18153d3e84ce19c689b819e3e6d4a38c64f7712d612df2da08e4b0e4da98a4ca063f1b80eca146a5d991371f407feadf7e28a85435746940af201338dec139a801ac6be18c3dce7cd3d5d536b5ea82e75a00bd0721679a0e796ad56ae811b57d443e7e3d497dc5f073669506c3b50b423a78e9e4ff341bb96a0ceb22cdbee2ae32df0d6f78efb31f19b9a9a6119bcdd590222476037bafb2ddcb104ca92959f1d3d90fb4355e47a2448453e17020d2d65124186a495272a0a377c83b2f62a93631287083270e0dce816097f96e7654066b0e798331bcb96e8b5e0aad8d873f410c3db63c6aca20427650eb540398c0d6942bd12fff231529356b4ef5a4ed96bb714570a1447b3359783bc7caba6c770db4f385d1ad7c1465624f590605bb6eef4224695d48368d74ef863bcacfa40c82cc202cab5c50b9c908aeddb3ad106f81369d12ae13aba7b24509bccfa3d7a6939ee61212130fdbf6216545ce8938d2170d8b0876eeb11560ea29df095b6ef66e2a0e81892c8a4c9388b687f01dc554601fbd798b7741215e7df085031f8368cd1afaafc01f4c0c69db0864f9916e904c0a62616dac672010f55a08c44db1cfdbd64ee967559cd55632adc24a64bfc264882a2750d1df648a140e7da9c59a1ad8a6450d26eb290050a83f6cc34faef38eef9f30cf0eeed842d6a6c5a938c690dac49f4828c9e42d40310f274760342b3458cdee9dc6074ba938b45c5ce767a5db012d702517e23baf1f967ce8a2f5bac20a7fb742cbcc550b7a9ca7f397eda6b4ffbe55c96fecea60d28595e57fe112e6c59f64e6a288f555986540146225a311ed85a7ac4ae55eb1edfc8294cd79734cf4fc4fb5094c48b3dc82b99b8efd06ab38afd63b45c947e5d0c64a20f79d1c091d3274c68bbf215a0a020b52ab6556f7e41cdbc0ad47d4012e04d81b6d8da3bef3768fec56a380f8f0392770c650f958223aecdff12cc132ac19f9afdd9cee3b7b27c0dc186cdb7bdcfa8be6e05d64a87819f6c26b53d469ec858fea0470b37913a15ae920fe6cbd84b98a6f03780c6faabc4586035050a748ee5e46cff7e6c10ed9bce498a391d6bc9f64496e3397114fd8a3772051de10681626979bb18cb695b02ee67ec19bdc457190a5b911237766c4c3432f8e7b17dfb79d2aec03d1409dc83e3d6ecd6d2232321a04da5ffd33eb1571cf122b8620475eb6f1bd981e4b0f3ca1c7537523998631915b119c699d4f141bbacf0cb3b71bb1fabbc900b98a42618f4098df89b0f146f298402ef206d2b981d3d88b8a1f6c3083194643f928d3f5901b8fd450fd0fd59ce5c094cd40235a4e748b8dc2a11c102aeba0b356ebd3fcfccb027226dbb50bbd0a4ff52b8cf1e9199e56f34cac28e21af01fab855d4d12483f77f7fc592681e2cb21ee5fc31aed72772c1d232183717e9863d82881c8505b5c50c0e0943718d0b2f6b10bb253046de4ecbe2acf11eb0d035b2b4b657bd1d7a9acb67fad522f24fcdb81713e55ebebbcc1c7b0afe15332cce8662f911a4e1e21fe1a7e3ea5f87ee71ec466a82f5cb94b2b2066db845df8d5be8a87e6873e0c7744a884ef3d185252dcf9320adb8690f483cdee5484b974096a7a328cb30b990fe1627f67122f011702f6d9e3a001c4118065829147d1ad540da2e8bfec73ae8ce9541dc5cabd1e0d828988d74a0fc82271176fe9945aa1e8270e9a010e6e79c35d3e3e34bb4b46dd4f23a49c55dcd4159ff72298d8e82102a82e7ad1b66ea197d22565be444fbc13d3c6de4d24c3a77307d260dbbf61da9882dc56f97475483ddea581730cfbfb013ff26088a69f497d9c511d1cd5e21f33f688fe2fafd3fc45940594a6026a1d52449074d938bbe6398b5c00ad3ee5feb04807bb53754a146eb6810a11bf98f1570234810c0e41d27402460aac455d0f194e33800e355ca583325825b6f1055ac5684e134cb840aba2c90efe57cf5f74706dcd0742ddb82a64e2942212b2f1ba786325258b89af264d7a5e0e853c22a08645d4b953633fb01f11e2377f576dd709ae3aa2c282a1ab335b4bbdb6a38e0bd27ab55804a86f178b69f0a998737505103be0db0825a6e1cb6bebc4946f1aa4606efea3d197bdb02b142fa712b9fc9e904eee3c84106a7d7aabd6745358893e0ac98790cacf6e2fb77a8155aa22965db7bc15e7dba54203a49b0b63f2411d6b954a6a64dbadad7f6c264cc285442092d95e21768cc48ee0471b0b89452b7ad69d90f04dafbe964fb7c833cf620b9865d26695bb6bc464a0fe8463b6dd22c172c595421e10d3ef1210a1d3170f1b78525656af849101cb984b8cb8e90aafbaf8a12e1bd315ed350efe6e3c799b2b6ead59b51800397d5e8aa7d69919d0ba4ceb2b80dd05b2e815b8e68eb0c968cd29e8bb628d5563719b618dfd73f17775eb6e990fa145f6664a5c4b637eb886c4685680e2d7970263a47aed6b012d1351f0741c125fd3ad55d13cc5d0fc85936ddc9d65d797d2986043c0f19eee21d46d2ffab9c2ee7820ad01e24d0ec6de967419ba44a64c215b8815a8348a61bfca9f6773331895547746fed13162439fa1df741ee6d4968dbb503b01db14d29a4998a2b2cfcea452542bc0a41531c75fa7265c0f61ce0850705365afd013da6ed8893b0b2d3e40b379f9e94a63af005f8f5195835853322cdbbdab7524a4c4e5ec5da92980bc33eb779ff1a8f7c18b3e3d7ccd4f04ef4d5d8aa91af998c907dddf5124fef230a75545433001c28d967524f9fbf0f864d0c470c6f1234f1d0f230b3353eaed8bfa24b016946933532f0f13843ab1ad2ce3159a06e2c2626373aa0769833fc7da44ae6ce925df216aaf63cb97824c74dbb8f80be50bcdb3cf4a60251df1ee920a8281f91b887725a3e6fdec9a370a46bdf328aef0659e1a990cfd21683d765608dcbf70d4302fd30637714ef7553e1825a70a0bb5d4b2c2ac769f90cc32798bbd029d47a9d9b9db0dc2e638d64af89a414ce29fd96326dac1090a771263d96c2a50e9223008919b87eedd6f9b40c70db7e61cdd695676a0f10bcb7b7a15427991f788d3c6142eb93d1908029f6e895b68eb65b899eac514c0fca555b16ef5c47f23d96b041d8ff57f55777df30fa8e143cdf9eb23269682aa12911c73f24dee94aa2331521cc0e132a42dcfbe16e45d3cbaf7195b52097cae57866df5236fbde8d6b1b42f8f4dc2226e42eba0900423cf11cb3869981e45f13f7cc59ea84c51f70a0dec66183188c6f975aad5fe94749eb47454cf4e719da1c6c2b6d84b7271a6409bfa8ead35fdb42e25ef7fb34820a3c0df4b5456159c65379c4af37eb5845ff116f1319824fdcd027a715a6e15570d4efcd52962442682c079e43f143970b2153020eb5e2b6a37660a8b880e1efcbaad722b57408e93480d09cd660a0ebd6e0751f0bc84d006b4358f1126367e821863b7137c86122ff70415850af33351af1fcfd2f2fc48115b1efc0d8776250b6ec44a5586280cffade2043caa75a467606f9378396b69ab133467880ab6006681e0ca31ce9775fc1fbf1980f0d4290f38e7efc8be9ab88e99c034a65df9c70fd24960c55000999351d802c216de534b02b7845ae688931f409142148146623ddc6101d2a6ee40a29ae54e8c24a4c621286fc03cecf096950b3e7d012e5f883abe390d685334d2dbe94709c5ee2e27f719b62a4d1e353aba6ab481f1a2c60daee840584ba5e449d3916ea78f8383ae9e7d58abc062041dc319b633716253517e10cb78269be0ae4dd3c109eac557470f806c8a3b082ee9dfb1b0ca95f46c72ef119f091080d331c38fabd532e61bc5c2b38c9b205e401730fef3b40dc3000c35bfbb28e46446bc1e40c828520dd348b0a0ede95583522f1fd6a7ebec9c1c397c39315ecc7c20677d8a6354aa0684188e36b786d6341c7b06dfaf32fd28cc11d31d246d79628aee1808b6f1c5394365343d9da0061999c87b63cb86ecbc7d4a54df3ecb6f68eef32269dc88536b0a8029b0430dba36f26d1f2679e47bb6ca9e6d91360745b12434adf25f24f1a01598aba8dac57d9ce6d954613cecc0d7ffeb0a75ddc08de948028e3ac1eb67bad55ddc09b3c9118a23ae095b9773b03dd80d4b6e54db9be5c66de29a31ec4c26fbe5765f991e8b3815c60e68175fb9691cc22bd208d5696a45d454ba63744707890193e47b892b19c8525b6a7a229e1b6cfc6d08a929f6ace188e3330bba21501dc6462aef6501943b2400e496388ed52e6ddd3668327dd17e6cb4e7c6a89e9b2f931ed490309b6bffc51ce79345bfae9c19458558155f2bbc287c54c7c8051f46a5c347a9a19b326211106a7faac33a0163d12c71e32a53601d87131e6ca173db4b9b02836380b0905404628c96434b01842f75f94deef3c885f933aa5f7b58d9fbb2a352de7ce375ccb3d33b71d39e4ef923703eba092bfeab22ea7c2b394baf140ebe9b1b09fec97e8f39c0f54a520466006401e8e953fc035dd2d88256eb0fcebcd0c5651f2efdf4f07f5ea8ce26f75d3402269762f47c306cfa7d9cc5c04f66e9df9733aaac049870fd5b335e12ac4658540bb11b6daf46bda5b057cb9a789e26ba5aebb5adde77cd4c5f8f66e8c8487e81f2f1f021f387c9cfb2ee3315cd9b87cc3539264e336c2b958f40b90f43097c578062847a29439bbe43471aff9d3b26963c6f5176011e069bab73f7de998f8fd08921ec1cb6cfddca69daeb769761209c67e7c1ae0324e13446ba4142c38dd6e1a42cf5a4d7cfda3f7eb7e85cdc75cb421d8b1b3d5669c107e144f9ae257e921e864a40bdb9dfac6a6d8933865339937ef1d3a26076e83ea99275a4d729917b36f776206a8455d2b9ae0289735e1b757b84cf33950a89a83d23cf669cd019a127707ebce1a95ded7ad9763cccda34ef24af76ab3c0ea424ae54127c1c408500af9bb89ca733472b22606b3d53a862cf283d875d136aa363d16e1af30bbeb27b244795531e271372301b816eba67b7d429b89fb66c98592e309d3df1816bfbfe3e02025e51f7daa975c96a1fe6facd119624bdb4796daccc0d71cc55f3c6e610f3c11197072f35fc452b14c990f46b1d58b5dcbef211537d1f875f135f0895d8db0d7c2fa08ec1c6b2ff5f94545ce92b254fa76f516262834165f987e5f75617d00e662f2a59cf0bc0e18585bb16d2aa78e3d8d3343206f16a2c32ce5a2d45c43832f057f868e373bbd8080d850b15f698e4bda092b66e325e01ca8667cd4118c1b369af60fb3580dce3af545218d6d2e834257217a155b1594b5e36d9560efca962c6e1f3de296becd9b0b5b8d085235894e4c26564f020b40c65e3bd221659fdb5fce668b74d56f366c4b3280325ca98e8f0ecc3957a813d9c6a5b41f9174d11671989d93dfcce47e7a2cd36f12ca859ddf5a9211d5754deac18ecb090710c27c048ac71ce3515f4d54e067ec360150928cc7e40449ec69dd823359feb6b5e7c3ac8ea8c41f5fc248f6c66672b427e29678ed4a4744b4970641a4665f1c1e498010271c4e174b1707f0d054d5033c230558282f6316f9ee07a7ad0db01f11bfa9e34b92c89973a70e54815e8fea84ac2bde3ef6b62ff88a18d750eaaebf54b9e667d70c3a61c487a9dcdac5c0d7f304792715c20b932ff421172c9eabe8752c47458104d061db323a6dbfd96a463b7736052fc7d3620e16b6bb69b4c2ae34dc82183059a8fc439e9b866a6ffbaed2ba0ce4764657689f2fcb1ae861d8e462098eb2771d005759cce1cf06d208e2257d8710feda86620699c0d2c064dfb9a0271a1f093ecd9dd89a16f3260cd786c9ccb0bd1d2cf93d9d00823f5ddc3772ee70c288130a7b42afda98571ca72ce775ca34a17aff980d0a23e489a4056c3692887e08e067ec86ffef9846fe0d5f24c6a1efdc79b37abfc25b249e1c7db7f370302072dcf0ceb387ec73796aa462060aeb12e371b4ca295e714761c744d60e810429785d7716195a4e2a37cd5e5bcacded5d771f69e1d0fbaafa66b9ea41622aa462e8d064b6dfe8bfc2e1cdd2ba72b68c207630d52beb2ceea4571fcf5fa66c6f2422367722fab0848ee86264f73e74dcf572dda40dbe68108d168fa25658ed4ed5559e91122ceadddfab5052c4bedc48c80330</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>大数据组件</category>
        <category>canal</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>搭建Kafka高可用集群</title>
    <url>/2022/03/19/%E6%90%AD%E5%BB%BAKafka%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<h2 id="一、Zookeeper集群搭建">一、Zookeeper集群搭建</h2>
<p>为保证集群高可用，Zookeeper 集群的节点数最好是奇数，最少有三个节点，所以这里搭建一个三个节点的集群。</p>
<h3 id="1-1-下载-解压">1.1 下载 &amp; 解压</h3>
<p>下载对应版本 Zookeeper，这里我下载的版本 <code>3.4.14</code>。官方下载地址：<a href="https://archive.apache.org/dist/zookeeper/">https://archive.apache.org/dist/zookeeper/</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">下载</span></span><br><span class="line">wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz</span><br><span class="line"><span class="meta"># </span><span class="language-bash">解压</span></span><br><span class="line">tar -zxvf zookeeper-3.4.14.tar.gz</span><br></pre></td></tr></table></figure>
<h3 id="1-2-修改配置">1.2 修改配置</h3>
<p>拷贝三份 zookeeper 安装包。分别进入安装目录的 <code>conf</code> 目录，拷贝配置样本 <code>zoo_sample.cfg </code> 为 <code>zoo.cfg</code> 并进行修改，修改后三份配置文件内容分别如下：</p>
<p>zookeeper01 配置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/usr/local/zookeeper-cluster/data/01</span><br><span class="line">dataLogDir=/usr/local/zookeeper-cluster/log/01</span><br><span class="line">clientPort=2181</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">server.1 这个1是服务器的标识，可以是任意有效数字，标识这是第几个服务器节点，这个标识要写到dataDir目录下面myid文件里</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">指名集群间通讯端口和选举端口</span></span><br><span class="line">server.1=127.0.0.1:2287:3387</span><br><span class="line">server.2=127.0.0.1:2288:3388</span><br><span class="line">server.3=127.0.0.1:2289:3389</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果是多台服务器，则集群中每个节点通讯端口和选举端口可相同，IP 地址修改为每个节点所在主机 IP 即可。</p>
</blockquote>
<p>zookeeper02 配置，与 zookeeper01 相比，只有 <code>dataLogDir</code>、<code>dataLogDir</code> 和 <code>clientPort</code> 不同：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/usr/local/zookeeper-cluster/data/02</span><br><span class="line">dataLogDir=/usr/local/zookeeper-cluster/log/02</span><br><span class="line">clientPort=2182</span><br><span class="line"></span><br><span class="line">server.1=127.0.0.1:2287:3387</span><br><span class="line">server.2=127.0.0.1:2288:3388</span><br><span class="line">server.3=127.0.0.1:2289:3389</span><br></pre></td></tr></table></figure>
<p>zookeeper03 配置，与 zookeeper01，02 相比，也只有 <code>dataLogDir</code>、<code>dataLogDir</code> 和 <code>clientPort</code> 不同：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/usr/local/zookeeper-cluster/data/03</span><br><span class="line">dataLogDir=/usr/local/zookeeper-cluster/log/03</span><br><span class="line">clientPort=2183</span><br><span class="line"></span><br><span class="line">server.1=127.0.0.1:2287:3387</span><br><span class="line">server.2=127.0.0.1:2288:3388</span><br><span class="line">server.3=127.0.0.1:2289:3389</span><br></pre></td></tr></table></figure>
<blockquote>
<p>配置参数说明：</p>
<ul class="lvl-1">
<li class="lvl-2"><strong>tickTime</strong>：用于计算的基础时间单元。比如 session 超时：N*tickTime；</li>
<li class="lvl-2"><strong>initLimit</strong>：用于集群，允许从节点连接并同步到 master 节点的初始化连接时间，以 tickTime 的倍数来表示；</li>
<li class="lvl-2"><strong>syncLimit</strong>：用于集群， master 主节点与从节点之间发送消息，请求和应答时间长度（心跳机制）；</li>
<li class="lvl-2"><strong>dataDir</strong>：数据存储位置；</li>
<li class="lvl-2"><strong>dataLogDir</strong>：日志目录；</li>
<li class="lvl-2"><strong>clientPort</strong>：用于客户端连接的端口，默认 2181</li>
</ul>
</blockquote>
<h3 id="1-3-标识节点">1.3 标识节点</h3>
<p>分别在三个节点的数据存储目录下新建 <code>myid</code> 文件,并写入对应的节点标识。Zookeeper 集群通过 <code>myid</code> 文件识别集群节点，并通过上文配置的节点通信端口和选举端口来进行节点通信，选举出 leader 节点。</p>
<p>创建存储目录：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">dataDir</span></span><br><span class="line">mkdir -vp  /usr/local/zookeeper-cluster/data/01</span><br><span class="line"><span class="meta"># </span><span class="language-bash">dataDir</span></span><br><span class="line">mkdir -vp  /usr/local/zookeeper-cluster/data/02</span><br><span class="line"><span class="meta"># </span><span class="language-bash">dataDir</span></span><br><span class="line">mkdir -vp  /usr/local/zookeeper-cluster/data/03</span><br></pre></td></tr></table></figure>
<p>创建并写入节点标识到 <code>myid</code> 文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="language-bash">server1</span></span><br><span class="line">echo &quot;1&quot; &gt; /usr/local/zookeeper-cluster/data/01/myid</span><br><span class="line"><span class="meta">#</span><span class="language-bash">server2</span></span><br><span class="line">echo &quot;2&quot; &gt; /usr/local/zookeeper-cluster/data/02/myid</span><br><span class="line"><span class="meta">#</span><span class="language-bash">server3</span></span><br><span class="line">echo &quot;3&quot; &gt; /usr/local/zookeeper-cluster/data/03/myid</span><br></pre></td></tr></table></figure>
<h3 id="1-4-启动集群">1.4 启动集群</h3>
<p>分别启动三个节点：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">启动节点1</span></span><br><span class="line">/usr/app/zookeeper-cluster/zookeeper01/bin/zkServer.sh start</span><br><span class="line"><span class="meta"># </span><span class="language-bash">启动节点2</span></span><br><span class="line">/usr/app/zookeeper-cluster/zookeeper02/bin/zkServer.sh start</span><br><span class="line"><span class="meta"># </span><span class="language-bash">启动节点3</span></span><br><span class="line">/usr/app/zookeeper-cluster/zookeeper03/bin/zkServer.sh start</span><br></pre></td></tr></table></figure>
<h3 id="1-5-集群验证">1.5 集群验证</h3>
<p>使用 jps 查看进程，并且使用 <code>zkServer.sh status</code> 查看集群各个节点状态。如图三个节点进程均启动成功，并且两个节点为 follower 节点，一个节点为 leader 节点。</p>
<div align="center"> <img  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/zookeeper-cluster.png"/> </div>
<h2 id="二、Kafka集群搭建">二、Kafka集群搭建</h2>
<h3 id="2-1-下载解压">2.1 下载解压</h3>
<p>Kafka 安装包官方下载地址：<a href="http://kafka.apache.org/downloads">http://kafka.apache.org/downloads</a> ，本用例下载的版本为 <code>2.2.0</code>，下载命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">下载</span></span><br><span class="line">wget https://www-eu.apache.org/dist/kafka/2.2.0/kafka_2.12-2.2.0.tgz</span><br><span class="line"><span class="meta"># </span><span class="language-bash">解压</span></span><br><span class="line">tar -xzf kafka_2.12-2.2.0.tgz</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里 j 解释一下 kafka 安装包的命名规则：以 <code>kafka_2.12-2.2.0.tgz</code> 为例，前面的 2.12 代表 Scala 的版本号（Kafka 采用 Scala 语言进行开发），后面的 2.2.0 则代表 Kafka 的版本号。</p>
</blockquote>
<h3 id="2-2-拷贝配置文件">2.2 拷贝配置文件</h3>
<p>进入解压目录的 <code> config</code> 目录下 ，拷贝三份配置文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">cp</span> server.properties server-1.properties</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">cp</span> server.properties server-2.properties</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">cp</span> server.properties server-3.properties</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-修改配置">2.3 修改配置</h3>
<p>分别修改三份配置文件中的部分配置，如下：</p>
<p>server-1.properties：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The id of the broker. 集群中每个节点的唯一标识</span></span><br><span class="line"><span class="attr">broker.id</span>=<span class="string">0</span></span><br><span class="line"><span class="comment"># 监听地址</span></span><br><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://hadoop001:9092</span></span><br><span class="line"><span class="comment"># 数据的存储位置</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/usr/local/kafka-logs/00</span></span><br><span class="line"><span class="comment"># Zookeeper连接地址</span></span><br><span class="line"><span class="attr">zookeeper.connect</span>=<span class="string">hadoop001:2181,hadoop001:2182,hadoop001:2183</span></span><br></pre></td></tr></table></figure>
<p>server-2.properties：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">broker.id</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://hadoop001:9093</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/usr/local/kafka-logs/01</span></span><br><span class="line"><span class="attr">zookeeper.connect</span>=<span class="string">hadoop001:2181,hadoop001:2182,hadoop001:2183</span></span><br></pre></td></tr></table></figure>
<p>server-3.properties：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">broker.id</span>=<span class="string">2</span></span><br><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://hadoop001:9094</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/usr/local/kafka-logs/02</span></span><br><span class="line"><span class="attr">zookeeper.connect</span>=<span class="string">hadoop001:2181,hadoop001:2182,hadoop001:2183</span></span><br></pre></td></tr></table></figure>
<p>这里需要说明的是 <code>log.dirs</code> 指的是数据日志的存储位置，确切的说，就是分区数据的存储位置，而不是程序运行日志的位置。程序运行日志的位置是通过同一目录下的 <code>log4j.properties</code> 进行配置的。</p>
<h3 id="2-4-启动集群">2.4 启动集群</h3>
<p>分别指定不同配置文件，启动三个 Kafka 节点。启动后可以使用 jps 查看进程，此时应该有三个 zookeeper 进程和三个 kafka 进程。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server-1.properties</span><br><span class="line">bin/kafka-server-start.sh config/server-2.properties</span><br><span class="line">bin/kafka-server-start.sh config/server-3.properties</span><br></pre></td></tr></table></figure>
<h3 id="2-5-创建测试主题">2.5 创建测试主题</h3>
<p>创建测试主题：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --bootstrap-server hadoop001:9092 \</span><br><span class="line">					--replication-factor 3 \</span><br><span class="line">					--partitions 1 --topic my-replicated-topic</span><br></pre></td></tr></table></figure>
<p>创建后可以使用以下命令查看创建的主题信息：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --bootstrap-server hadoop001:9092 --topic my-replicated-topic</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-cluster-shell.png"/> </div>
<p>可以看到分区 0 的有 0,1,2 三个副本，且三个副本都是可用副本，都在 ISR(in-sync Replica 同步副本) 列表中，其中 1 为首领副本，此时代表集群已经搭建成功。</p>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>消息队列易错指南</title>
    <url>/2021/10/13/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E6%98%93%E9%94%99%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[<h2 id="1-消息队列的坑之非幂等">1.消息队列的坑之非幂等</h2>
<h3 id="（1）幂等性概念">（1）幂等性概念</h3>
<p>所谓幂等性就是无论多少次操作和第一次的操作结果一样。如果消息被多次消费，很有可能造成数据的不一致。而如果消息不可避免地被消费多次，如果我们开发人员能通过技术手段保证数据的前后一致性，那也是可以接受的 。</p>
<p><code>RabbitMQ</code>、<code>RocketMQ</code>、<code>Kafka</code> 消息队列中间件都有可能出现消息重复消费问题。这种问题并不是 MQ 自己保证的，而是需要开发人员来保证。</p>
<p>这几款消息队列中间都是是全球最牛的分布式消息队列，那肯定考虑到了消息的幂等性。我们以 Kafka 为例，看看 Kafka 是怎么保证消息队列的幂等性。</p>
<p>Kafka 有一个 <code>偏移量</code> 的概念，代表着消息的序号，每条消息写到消息队列都会有一个偏移量，消费者消费了数据之后，每过一段固定的时间，就会把消费过的消息的偏移量提交一下，表示已经消费过了，下次消费就从偏移量后面开始消费。</p>
<h3 id="（2）避坑指南">（2）避坑指南</h3>
<p>微信支付结果通知场景</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>微信官方文档上提到微信支付通知结果可能会推送多次，需要开发者自行保证幂等性。第一次我们可以直接修改订单状态（如支付中 -&gt; 支付成功），第二次就根据订单状态来判断，如果不是支付中，则不进行订单处理逻辑。</p>
</li>
</ul>
<p>插入数据库场景</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>每次插入数据时，先检查下数据库中是否有这条数据的主键 id，如果有，则进行更新操作。</p>
</li>
</ul>
<p>写 Redis 场景</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>Redis 的 <code>Set</code> 操作天然幂等性，所以不用考虑 Redis 写数据的问题。</p>
</li>
</ul>
<p>其他场景方案</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>生产者发送每条数据时，增加一个全局唯一 id，类似订单 id。每次消费时，先去 Redis 查下是否有这个 id，如果没有，则进行正常处理消息，且将 id 存到 Redis。如果查到有这个 id，说明之前消费过，则不要进行重复处理这条消息。</p>
</li>
<li class="lvl-2">
<p>不同业务场景，可能会有不同的幂等性方案，大家选择合适的即可，上面的几种方案只是提供常见的解决思路。</p>
</li>
</ul>
<h2 id="2-消息队列的坑之消息丢失">2.消息队列的坑之消息丢失</h2>
<blockquote>
<p>消息丢失会带来什么问题？如果是订单下单、支付结果通知、扣费相关的消息丢失，则可能造成财务损失，如果量很大，就会给甲方带来巨大损失</p>
</blockquote>
<h3 id="（1）生产者存放消息的过程中丢失消息">（1）生产者存放消息的过程中丢失消息</h3>
<h4 id="解决方案">解决方案</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>事务机制（不推荐，异步方式）</p>
</li>
</ul>
<p>对于 RabbitMQ 来说，生产者发送数据之前开启 RabbitMQ 的<strong>事务机制</strong><code>channel.txselect</code> ，如果消息没有进队列，则生产者受到异常报错，并进行回滚 <code>channel.txRollback</code>，然后重试发送消息；如果收到了消息，则可以提交事务 <code>channel.txCommit</code>。但这是一个同步的操作，会影响性能。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>confirm 机制（推荐，异步方式）</p>
</li>
</ul>
<p>我们可以采用另外一种模式： <code>confirm</code> 模式来解决同步机制的性能问题。每次生产者发送的消息都会分配一个唯一的 id，如果写入到了 RabbitMQ 队列中，则 RabbitMQ 会回传一个 <code>ack</code> 消息，说明这个消息接收成功。如果 RabbitMQ 没能处理这个消息，则回调 <code>nack</code> 接口。说明需要重试发送消息。</p>
<p>也可以自定义超时时间 + 消息 id 来实现超时等待后重试机制。但可能出现的问题是调用 ack 接口时失败了，所以会出现消息被发送两次的问题，这个时候就需要保证消费者消费消息的幂等性。</p>
<h4 id="事务模式-和-confirm-模式的区别："><code>事务模式</code> 和 <code>confirm</code> 模式的区别：</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>事务机制是同步的，提交事务后悔被<strong>阻塞</strong>直到提交事务完成后。</p>
</li>
<li class="lvl-2">
<p>confirm 模式异步接收通知，但可能<strong>接收不到通知</strong>。需要考虑接收不到通知的场景。</p>
</li>
</ul>
<h3 id="（2）消息队列丢失消息">（2）消息队列丢失消息</h3>
<p>消息队列的消息可以放到内存中，或将内存中的消息转到硬盘（比如数据库）中，一般都是内存和硬盘中都存有消息。如果只是放在内存中，那么当机器重启了，消息就全部丢失了。如果是硬盘中，则可能存在一种极端情况，就是将内存中的数据转换到硬盘的期间中，消息队列出问题了，未能将消息持久化到硬盘。</p>
<p><strong>解决方案</strong></p>
<ul class="lvl-0">
<li class="lvl-2">
<p>创建 <code>Queue</code> 的时候将其设置为持久化。这个地方没搞懂，欢迎探讨解答。</p>
</li>
<li class="lvl-2">
<p>发送消息的时候将消息的 <code>deliveryMode</code> 设置为 2 。</p>
</li>
<li class="lvl-2">
<p>开启生产者 <code>confirm</code> 模式，可以重试发送消息。</p>
</li>
</ul>
<h3 id="（3）消费者丢失消息">（3）消费者丢失消息</h3>
<p>消费者刚拿到数据，还没开始处理消息，结果进程因为异常退出了，消费者没有机会再次拿到消息。</p>
<p><strong>解决方案</strong></p>
<ul class="lvl-0">
<li class="lvl-2">
<p>关闭 RabbitMQ 的自动 <code>ack</code>，每次生产者将消息写入消息队列后，就自动回传一个 <code>ack</code> 给生产者。</p>
</li>
<li class="lvl-2">
<p>消费者处理完消息再主动 <code>ack</code>，告诉消息队列我处理完了。</p>
</li>
</ul>
<p><strong>问题：</strong> 那这种主动 <code>ack</code> 有什么漏洞了？如果 主动 <code>ack</code> 的时候挂了，怎么办？</p>
<p>则可能会被再次消费，这个时候就需要幂等处理了。</p>
<p><strong>问题：</strong> 如果这条消息一直被重复消费怎么办？</p>
<p>则需要有加上重试次数的监测，如果超过一定次数则将消息丢失，记录到异常表或发送异常通知给值班人员。</p>
<h3 id="（4）Kafka-消息丢失">（4）Kafka 消息丢失</h3>
<p><strong>场景：</strong><code>Kafka</code> 的某个 broker（节点）宕机了，重新选举 leader （写入的节点）。如果 leader 挂了，follower 还有些数据未同步完，则 follower 成为 leader 后，消息队列会丢失一部分数据。</p>
<p><strong>解决方案</strong></p>
<ul class="lvl-0">
<li class="lvl-2">
<p>给 topic 设置 <code>replication.factor</code> 参数，值必须大于 1，要求每个 partition 必须有至少 2 个副本。</p>
</li>
<li class="lvl-2">
<p>给 kafka 服务端设置 <code>min.insyc.replicas</code> 必须大于 1，表示一个 leader 至少一个 follower 还跟自己保持联系。</p>
</li>
</ul>
<h2 id="3-消息队列的坑之消息乱序">3. 消息队列的坑之消息乱序</h2>
<blockquote>
<p>用户先下单成功，然后取消订单，如果顺序颠倒，则最后数据库里面会有一条下单成功的订单。</p>
</blockquote>
<p><strong>RabbitMQ 场景：</strong></p>
<ul class="lvl-0">
<li class="lvl-2">
<p>生产者向消息队列按照顺序发送了 2 条消息，消息1：增加数据 A，消息2：删除数据 A。</p>
</li>
<li class="lvl-2">
<p>期望结果：数据 A 被删除。</p>
</li>
<li class="lvl-2">
<p>但是如果有两个消费者，消费顺序是：消息2、消息 1。则最后结果是增加了数据 A。</p>
</li>
</ul>
<p><strong>RabbitMQ 解决方案：</strong></p>
<ul class="lvl-0">
<li class="lvl-2">
<p>将 Queue 进行拆分，创建多个内存 Queue，消息 1 和 消息 2 进入同一个 Queue。</p>
</li>
<li class="lvl-2">
<p>创建多个消费者，每一个消费者对应一个 Queue。</p>
</li>
</ul>
<p><strong>Kafka 场景：</strong></p>
<ul class="lvl-0">
<li class="lvl-2">
<p>创建了 topic，有 3 个 partition。</p>
</li>
<li class="lvl-2">
<p>创建一条订单记录，订单 id 作为 key，订单相关的消息都丢到同一个 partition 中，同一个生产者创建的消息，顺序是正确的。</p>
</li>
<li class="lvl-2">
<p>为了快速消费消息，会创建多个消费者去处理消息，而为了提高效率，每个消费者可能会创建多个线程来并行的去拿消息及处理消息，处理消息的顺序可能就乱序了。</p>
</li>
</ul>
<p><strong>Kafka 解决方案：</strong></p>
<ul class="lvl-0">
<li class="lvl-2">
<p>解决方案和 RabbitMQ 类似，利用多个 内存 Queue，每个线程消费 1个 Queue。</p>
</li>
<li class="lvl-2">
<p>具有相同 key 的消息 进同一个 Queue。</p>
</li>
</ul>
<h2 id="4-消息队列的坑之消息积压">4. 消息队列的坑之消息积压</h2>
<p>消息积压：消息队列里面有很多消息来不及消费。</p>
<p><strong>场景 1：</strong> 消费端出了问题，比如消费者都挂了，没有消费者来消费了，导致消息在队列里面不断积压。</p>
<p><strong>场景 2：</strong> 消费端出了问题，比如消费者消费的速度太慢了，导致消息不断积压。</p>
<blockquote>
<p>比如线上正在做订单活动，下单全部走消息队列，如果消息不断积压，订单都没有下单成功 ，那么会造成很大的损失</p>
</blockquote>
<p>解决方案：<strong>解铃还须系铃人</strong></p>
<ul class="lvl-0">
<li class="lvl-2">
<p>修复代码层面消费者的问题，确保后续消费速度恢复或尽可能加快消费的速度。</p>
</li>
<li class="lvl-2">
<p>停掉现有的消费者。</p>
</li>
<li class="lvl-2">
<p>临时建立好原先 5 倍的 Queue 数量。</p>
</li>
<li class="lvl-2">
<p>临时建立好原先 5 倍数量的 消费者。</p>
</li>
<li class="lvl-2">
<p>将堆积的消息全部转入临时的 Queue，消费者来消费这些 Queue。</p>
</li>
</ul>
<h2 id="5-消息队列的坑之消息过期失效">5. 消息队列的坑之消息过期失效</h2>
<blockquote>
<p>RabbitMQ 可以设置过期时间，如果消息超过一定的时间还没有被消费，则会被 RabbitMQ 给清理掉。消息就丢失了</p>
</blockquote>
<p>解决方案：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>准备好批量重导的程序</p>
</li>
<li class="lvl-2">
<p>手动将消息闲时批量重导</p>
</li>
</ul>
<h2 id="6-消息队列的坑之队列写满">6. 消息队列的坑之队列写满</h2>
<blockquote>
<p>当消息队列因消息积压导致的队列快写满，所以不能接收更多的消息了。生产者生产的消息将被丢弃。</p>
</blockquote>
<p>解决方案：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>判断哪些是无用的消息，RabbitMQ 可以进行 <code>Purge Message</code> 操作。</p>
</li>
<li class="lvl-2">
<p>如果是有用的消息，则需要将消息快速消费，将消息里面的内容转存到数据库。</p>
</li>
<li class="lvl-2">
<p>准备好程序将转存在数据库中的消息再次重导到消息队列。</p>
</li>
<li class="lvl-2">
<p>闲时重导消息到消息队列。</p>
</li>
</ul>
<p>原文链接：<a href="https://www.cnblogs.com/jackson0714/p/fenbushi.html">https://www.cnblogs.com/jackson0714/p/fenbushi.html</a></p>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>深入理解Kafka副本机制</title>
    <url>/2022/03/19/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h2 id="一、Kafka集群">一、Kafka集群</h2>
<p>Kafka 使用 Zookeeper 来维护集群成员 (brokers) 的信息。每个 broker 都有一个唯一标识 <code>broker.id</code>，用于标识自己在集群中的身份，可以在配置文件 <code>server.properties</code> 中进行配置，或者由程序自动生成。下面是 Kafka brokers 集群自动创建的过程：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>每一个 broker 启动的时候，它会在 Zookeeper 的 <code>/brokers/ids</code> 路径下创建一个 <code>临时节点</code>，并将自己的 <code>broker.id</code> 写入，从而将自身注册到集群；</p>
</li>
<li class="lvl-2">
<p>当有多个 broker 时，所有 broker 会竞争性地在 Zookeeper 上创建 <code>/controller</code> 节点，由于 Zookeeper 上的节点不会重复，所以必然只会有一个 broker 创建成功，此时该 broker 称为 controller broker。它除了具备其他 broker 的功能外，<strong>还负责管理主题分区及其副本的状态</strong>。</p>
</li>
<li class="lvl-2">
<p>当 broker 出现宕机或者主动退出从而导致其持有的 Zookeeper 会话超时时，会触发注册在 Zookeeper 上的 watcher 事件，此时 Kafka 会进行相应的容错处理；如果宕机的是 controller broker 时，还会触发新的 controller 选举。</p>
</li>
</ul>
<h2 id="二、副本机制">二、副本机制</h2>
<p>为了保证高可用，kafka 的分区是多副本的，如果一个副本丢失了，那么还可以从其他副本中获取分区数据。但是这要求对应副本的数据必须是完整的，这是 Kafka 数据一致性的基础，所以才需要使用 <code>controller broker</code> 来进行专门的管理。下面将详解介绍 Kafka 的副本机制。</p>
<h3 id="2-1-分区和副本">2.1 分区和副本</h3>
<p>Kafka 的主题被分为多个分区 ，分区是 Kafka 最基本的存储单位。每个分区可以有多个副本 (可以在创建主题时使用 <code> replication-factor</code> 参数进行指定)。其中一个副本是首领副本 (Leader replica)，所有的事件都直接发送给首领副本；其他副本是跟随者副本 (Follower replica)，需要通过复制来保持与首领副本数据一致，当首领副本不可用时，其中一个跟随者副本将成为新首领。</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-cluster.png"/> </div>
<h3 id="2-2-ISR机制">2.2 ISR机制</h3>
<p>每个分区都有一个 ISR(in-sync Replica) 列表，用于维护所有同步的、可用的副本。首领副本必然是同步副本，而对于跟随者副本来说，它需要满足以下条件才能被认为是同步副本：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>与 Zookeeper 之间有一个活跃的会话，即必须定时向 Zookeeper 发送心跳；</p>
</li>
<li class="lvl-3">
<p>在规定的时间内从首领副本那里低延迟地获取过消息。</p>
</li>
</ul>
<p>如果副本不满足上面条件的话，就会被从 ISR 列表中移除，直到满足条件才会被再次加入。</p>
<p>这里给出一个主题创建的示例：使用 <code>--replication-factor</code> 指定副本系数为 3，创建成功后使用 <code>--describe </code> 命令可以看到分区 0 的有 0,1,2 三个副本，且三个副本都在 ISR 列表中，其中 1 为首领副本。</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-分区副本.png"/> </div>
<h3 id="2-3-不完全的首领选举">2.3 不完全的首领选举</h3>
<p>对于副本机制，在 broker 级别有一个可选的配置参数 <code>unclean.leader.election.enable</code>，默认值为 fasle，代表禁止不完全的首领选举。这是针对当首领副本挂掉且 ISR 中没有其他可用副本时，是否允许某个不完全同步的副本成为首领副本，这可能会导致数据丢失或者数据不一致，在某些对数据一致性要求较高的场景 (如金融领域)，这可能无法容忍的，所以其默认值为 false，如果你能够允许部分数据不一致的话，可以配置为 true。</p>
<h3 id="2-4-最少同步副本">2.4 最少同步副本</h3>
<p>ISR 机制的另外一个相关参数是 <code>min.insync.replicas</code> , 可以在 broker 或者主题级别进行配置，代表 ISR 列表中至少要有几个可用副本。这里假设设置为 2，那么当可用副本数量小于该值时，就认为整个分区处于不可用状态。此时客户端再向分区写入数据时候就会抛出异常 <code>org.apache.kafka.common.errors.NotEnoughReplicasExceptoin: Messages are rejected since there are fewer in-sync replicas than required。</code></p>
<h3 id="2-5-发送确认">2.5 发送确认</h3>
<p>Kafka 在生产者上有一个可选的参数 ack，该参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入成功：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>acks=0</strong> ：消息发送出去就认为已经成功了，不会等待任何来自服务器的响应；</p>
</li>
<li class="lvl-2">
<p><strong>acks=1</strong> ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应；</p>
</li>
<li class="lvl-2">
<p><strong>acks=all</strong> ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。</p>
</li>
</ul>
<h2 id="三、数据请求">三、数据请求</h2>
<h3 id="3-1-元数据请求机制">3.1 元数据请求机制</h3>
<p>在所有副本中，只有领导副本才能进行消息的读写处理。由于不同分区的领导副本可能在不同的 broker 上，如果某个 broker 收到了一个分区请求，但是该分区的领导副本并不在该 broker 上，那么它就会向客户端返回一个 <code>Not a Leader for Partition</code> 的错误响应。 为了解决这个问题，Kafka 提供了元数据请求机制。</p>
<p>首先集群中的每个 broker 都会缓存所有主题的分区副本信息，客户端会定期发送发送元数据请求，然后将获取的元数据进行缓存。定时刷新元数据的时间间隔可以通过为客户端配置 <code>metadata.max.age.ms</code> 来进行指定。有了元数据信息后，客户端就知道了领导副本所在的 broker，之后直接将读写请求发送给对应的 broker 即可。</p>
<p>如果在定时请求的时间间隔内发生的分区副本的选举，则意味着原来缓存的信息可能已经过时了，此时还有可能会收到 <code>Not a Leader for Partition</code> 的错误响应，这种情况下客户端会再次求发出元数据请求，然后刷新本地缓存，之后再去正确的 broker 上执行对应的操作，过程如下图：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-元数据请求.png"/> </div>
<h3 id="3-2-数据可见性">3.2 数据可见性</h3>
<p>需要注意的是，并不是所有保存在分区首领上的数据都可以被客户端读取到，为了保证数据一致性，只有被所有同步副本 (ISR 中所有副本) 都保存了的数据才能被客户端读取到。</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-数据可见性.png"/> </div>
<h3 id="3-3-零拷贝">3.3 零拷贝</h3>
<p>Kafka 所有数据的写入和读取都是通过零拷贝来实现的。传统拷贝与零拷贝的区别如下：</p>
<h4 id="传统模式下的四次拷贝与四次上下文切换">传统模式下的四次拷贝与四次上下文切换</h4>
<p>以将磁盘文件通过网络发送为例。传统模式下，一般使用如下伪代码所示的方法先将文件数据读入内存，然后通过 Socket 将内存中的数据发送出去。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">buffer = File.read</span><br><span class="line">Socket.send(buffer)</span><br></pre></td></tr></table></figure>
<p>这一过程实际上发生了四次数据拷贝。首先通过系统调用将文件数据读入到内核态 Buffer（DMA 拷贝），然后应用程序将内存态 Buffer 数据读入到用户态 Buffer（CPU 拷贝），接着用户程序通过 Socket 发送数据时将用户态 Buffer 数据拷贝到内核态 Buffer（CPU 拷贝），最后通过 DMA 拷贝将数据拷贝到 NIC Buffer。同时，还伴随着四次上下文切换，如下图所示：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-BIO.png"/> </div>
<h4 id="sendfile和transferTo实现零拷贝">sendfile和transferTo实现零拷贝</h4>
<p>Linux 2.4+ 内核通过 <code>sendfile</code> 系统调用，提供了零拷贝。数据通过 DMA 拷贝到内核态 Buffer 后，直接通过 DMA 拷贝到 NIC Buffer，无需 CPU 拷贝。这也是零拷贝这一说法的来源。除了减少数据拷贝外，因为整个读文件到网络发送由一个 <code>sendfile</code> 调用完成，整个过程只有两次上下文切换，因此大大提高了性能。零拷贝过程如下图所示：</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-零拷贝.png"/> </div>
<p>从具体实现来看，Kafka 的数据传输通过 TransportLayer 来完成，其子类 <code>PlaintextTransportLayer</code> 的 <code>transferFrom</code> 方法通过调用 Java NIO 中 FileChannel 的 <code>transferTo</code> 方法实现零拷贝，如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">long</span> <span class="title function_">transferFrom</span><span class="params">(FileChannel fileChannel, <span class="type">long</span> position, <span class="type">long</span> count)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="keyword">return</span> fileChannel.transferTo(position, count, socketChannel);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>注：</strong> <code>transferTo</code> 和 <code>transferFrom</code> 并不保证一定能使用零拷贝。实际上是否能使用零拷贝与操作系统相关，如果操作系统提供 <code>sendfile</code> 这样的零拷贝系统调用，则这两个方法会通过这样的系统调用充分利用零拷贝的优势，否则并不能通过这两个方法本身实现零拷贝。</p>
<h2 id="四、物理存储">四、物理存储</h2>
<h3 id="4-1-分区分配">4.1 分区分配</h3>
<p>在创建主题时，Kafka 会首先决定如何在 broker 间分配分区副本，它遵循以下原则：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>在所有 broker 上均匀地分配分区副本；</p>
</li>
<li class="lvl-2">
<p>确保分区的每个副本分布在不同的 broker 上；</p>
</li>
<li class="lvl-2">
<p>如果使用了 <code>broker.rack</code> 参数为 broker 指定了机架信息，那么会尽可能的把每个分区的副本分配到不同机架的 broker 上，以避免一个机架不可用而导致整个分区不可用。</p>
</li>
</ul>
<p>基于以上原因，如果你在一个单节点上创建一个 3 副本的主题，通常会抛出下面的异常：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">Error</span> <span class="string">while executing topic command : org.apache.kafka.common.errors.InvalidReplicationFactor   </span></span><br><span class="line"><span class="attr">Exception</span>: <span class="string">Replication factor: 3 larger than available brokers: 1.</span></span><br></pre></td></tr></table></figure>
<h3 id="4-2-分区数据保留规则">4.2 分区数据保留规则</h3>
<p>保留数据是 Kafka 的一个基本特性， 但是 Kafka 不会一直保留数据，也不会等到所有消费者都读取了消息之后才删除消息。相反， Kafka 为每个主题配置了数据保留期限，规定数据被删除之前可以保留多长时间，或者清理数据之前可以保留的数据量大小。分别对应以下四个参数：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><code>log.retention.bytes</code> ：删除数据前允许的最大数据量；默认值-1，代表没有限制；</p>
</li>
<li class="lvl-2">
<p><code>log.retention.ms</code>：保存数据文件的毫秒数，如果未设置，则使用 <code>log.retention.minutes</code> 中的值，默认为 null；</p>
</li>
<li class="lvl-2">
<p><code>log.retention.minutes</code>：保留数据文件的分钟数，如果未设置，则使用 <code>log.retention.hours</code> 中的值，默认为 null；</p>
</li>
<li class="lvl-2">
<p><code>log.retention.hours</code>：保留数据文件的小时数，默认值为 168，也就是一周。</p>
</li>
</ul>
<p>因为在一个大文件里查找和删除消息是很费时的，也很容易出错，所以 Kafka 把分区分成若干个片段，当前正在写入数据的片段叫作活跃片段。活动片段永远不会被删除。如果按照默认值保留数据一周，而且每天使用一个新片段，那么你就会看到，在每天使用一个新片段的同时会删除一个最老的片段，所以大部分时间该分区会有 7 个片段存在。</p>
<h3 id="4-3-文件格式">4.3 文件格式</h3>
<p>通常保存在磁盘上的数据格式与生产者发送过来消息格式是一样的。 如果生产者发送的是压缩过的消息，那么同一个批次的消息会被压缩在一起，被当作“包装消息”进行发送 (格式如下所示) ，然后保存到磁盘上。之后消费者读取后再自己解压这个包装消息，获取每条消息的具体信息。</p>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/kafka-compress-message.png"/> </div>]]></content>
      <categories>
        <category>大数据组件</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>集群资源管理器—YARN</title>
    <url>/2021/10/15/%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%99%A8%E2%80%94YARN/</url>
    <content><![CDATA[<h2 id="一、hadoop-yarn-简介">一、hadoop yarn 简介</h2>
<p><strong>Apache YARN</strong> (Yet Another Resource Negotiator)  是 hadoop 2.0 引入的集群资源管理系统。用户可以将各种服务框架部署在 YARN 上，由 YARN 进行统一地管理和资源分配。</p>
<div align="center"> <img width="600px"  src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/yarn-base.png"/> </div>
<h2 id="二、YARN架构">二、YARN架构</h2>
<div align="center"> <img width="600px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/Figure3Architecture-of-YARN.png"/> </div>
<h3 id="1-ResourceManager">1. ResourceManager</h3>
<p><code>ResourceManager</code> 通常在独立的机器上以后台进程的形式运行，它是整个集群资源的主要协调者和管理者。<code>ResourceManager</code> 负责给用户提交的所有应用程序分配资源，它根据应用程序优先级、队列容量、ACLs、数据位置等信息，做出决策，然后以共享的、安全的、多租户的方式制定分配策略，调度集群资源。</p>
<h3 id="2-NodeManager">2. NodeManager</h3>
<p><code>NodeManager</code> 是 YARN 集群中的每个具体节点的管理者。主要负责该节点内所有容器的生命周期的管理，监视资源和跟踪节点健康。具体如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>启动时向 <code>ResourceManager</code> 注册并定时发送心跳消息，等待 <code>ResourceManager</code> 的指令；</p>
</li>
<li class="lvl-2">
<p>维护 <code>Container</code> 的生命周期，监控 <code>Container</code> 的资源使用情况；</p>
</li>
<li class="lvl-2">
<p>管理任务运行时的相关依赖，根据 <code>ApplicationMaster</code> 的需要，在启动 <code>Container</code> 之前将需要的程序及其依赖拷贝到本地。</p>
</li>
</ul>
<h3 id="3-ApplicationMaster">3. ApplicationMaster</h3>
<p>在用户提交一个应用程序时，YARN 会启动一个轻量级的进程 <code>ApplicationMaster</code>。<code>ApplicationMaster</code> 负责协调来自 <code>ResourceManager</code> 的资源，并通过 <code>NodeManager</code> 监视容器内资源的使用情况，同时还负责任务的监控与容错。具体如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>根据应用的运行状态来决定动态计算资源需求；</p>
</li>
<li class="lvl-2">
<p>向 <code>ResourceManager</code> 申请资源，监控申请的资源的使用情况；</p>
</li>
<li class="lvl-2">
<p>跟踪任务状态和进度，报告资源的使用情况和应用的进度信息；</p>
</li>
<li class="lvl-2">
<p>负责任务的容错。</p>
</li>
</ul>
<h3 id="4-Contain">4. Contain</h3>
<p><code>Container</code> 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。当 AM 向 RM 申请资源时，RM 为 AM 返回的资源是用 <code>Container</code> 表示的。YARN 会为每个任务分配一个 <code>Container</code>，该任务只能使用该 <code>Container</code> 中描述的资源。<code>ApplicationMaster</code> 可在 <code>Container</code> 内运行任何类型的任务。例如，<code>MapReduce ApplicationMaster</code> 请求一个容器来启动 map 或 reduce 任务，而 <code>Giraph ApplicationMaster</code> 请求一个容器来运行 Giraph 任务。</p>
<h2 id="三、YARN工作原理简述">三、YARN工作原理简述</h2>
<div align="center"> <img src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/yarn工作原理简图.png"/> </div>
<ol>
<li class="lvl-3">
<p><code>Client</code> 提交作业到 YARN 上；</p>
</li>
<li class="lvl-3">
<p><code>Resource Manager</code> 选择一个 <code>Node Manager</code>，启动一个 <code>Container</code> 并运行 <code>Application Master</code> 实例；</p>
</li>
<li class="lvl-3">
<p><code>Application Master</code> 根据实际需要向 <code>Resource Manager</code> 请求更多的 <code>Container</code> 资源（如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务）；</p>
</li>
<li class="lvl-3">
<p><code>Application Master</code> 通过获取到的 <code>Container</code> 资源执行分布式计算。</p>
</li>
</ol>
<h2 id="四、YARN工作原理详述">四、YARN工作原理详述</h2>
<div align="center"> <img width="600px" src="https://oicio-picgo.oss-cn-hangzhou.aliyuncs.com/ihadu/img/yarn工作原理.png"/> </div>
<h4 id="1-作业提交">1. 作业提交</h4>
<p>client 调用 job.waitForCompletion 方法，向整个集群提交 MapReduce 作业 (第 1 步) 。新的作业 ID(应用 ID) 由资源管理器分配 (第 2 步)。作业的 client 核实作业的输出, 计算输入的 split, 将作业的资源 (包括 Jar 包，配置文件, split 信息) 拷贝给 HDFS(第 3 步)。 最后, 通过调用资源管理器的 submitApplication() 来提交作业 (第 4 步)。</p>
<h4 id="2-作业初始化">2. 作业初始化</h4>
<p>当资源管理器收到 submitApplciation() 的请求时, 就将该请求发给调度器 (scheduler), 调度器分配 container, 然后资源管理器在该 container 内启动应用管理器进程, 由节点管理器监控 (第 5 步)。</p>
<p>MapReduce 作业的应用管理器是一个主类为 MRAppMaster 的 Java 应用，其通过创造一些 bookkeeping 对象来监控作业的进度,  得到任务的进度和完成报告 (第 6 步)。然后其通过分布式文件系统得到由客户端计算好的输入 split(第 7 步)，然后为每个输入 split 创建一个 map 任务, 根据 mapreduce.job.reduces 创建 reduce 任务对象。</p>
<h4 id="3-任务分配">3. 任务分配</h4>
<p>如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务。</p>
<p>如果不是小作业,  那么应用管理器向资源管理器请求 container 来运行所有的 map 和 reduce 任务 (第 8 步)。这些请求是通过心跳来传输的,  包括每个 map 任务的数据位置，比如存放输入 split 的主机名和机架 (rack)，调度器利用这些信息来调度任务，尽量将任务分配给存储数据的节点, 或者分配给和存放输入 split 的节点相同机架的节点。</p>
<h4 id="4-任务运行">4. 任务运行</h4>
<p>当一个任务由资源管理器的调度器分配给一个 container 后，应用管理器通过联系节点管理器来启动 container(第 9 步)。任务由一个主类为 YarnChild 的 Java 应用执行， 在运行任务之前首先本地化任务需要的资源，比如作业配置，JAR 文件,  以及分布式缓存的所有文件 (第 10 步。 最后, 运行 map 或 reduce 任务 (第 11 步)。</p>
<p>YarnChild 运行在一个专用的 JVM 中, 但是 YARN 不支持 JVM 重用。</p>
<h4 id="5-进度和状态更新">5. 进度和状态更新</h4>
<p>YARN 中的任务将其进度和状态 (包括 counter) 返回给应用管理器, 客户端每秒 (通 mapreduce.client.progressmonitor.pollinterval 设置) 向应用管理器请求进度更新, 展示给用户。</p>
<h4 id="6-作业完成">6. 作业完成</h4>
<p>除了向应用管理器请求作业进度外,  客户端每 5 分钟都会通过调用 waitForCompletion() 来检查作业是否完成，时间间隔可以通过 mapreduce.client.completion.pollinterval 来设置。作业完成之后,  应用管理器和 container 会清理工作状态， OutputCommiter 的作业清理方法也会被调用。作业的信息会被作业历史服务器存储以备之后用户核查。</p>
<h2 id="五、提交作业到YARN上运行">五、提交作业到YARN上运行</h2>
<p>这里以提交 Hadoop Examples 中计算 Pi 的 MApReduce 程序为例，相关 Jar 包在 Hadoop 安装目录的 <code>share/hadoop/mapreduce</code> 目录下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">提交格式: hadoop jar jar包路径 主类名称 主类参数</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.15.2.jar pi 3 3</span></span><br></pre></td></tr></table></figure>
<h2 id="参考资料-22">参考资料</h2>
<ol>
<li class="lvl-3">
<p><a href="https://www.cnblogs.com/codeOfLife/p/5492740.html">初步掌握 Yarn 的架构及原理</a></p>
</li>
<li class="lvl-3">
<p><a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html">Apache Hadoop 2.9.2 &gt; Apache Hadoop YARN</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>大数据组件</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
</search>
