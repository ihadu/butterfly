<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>文本生成 | ihadu</title><meta name="author" content="ihadu"><meta name="copyright" content="ihadu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="第5章  文本生成 基于Transformers的语言模型最不可思议的特点之一是它们能够生成与人类所写的文本几乎没有区别的文本。一个著名的例子是OpenAI的GPT-2，它在给出以下提示时: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored"><meta property="og:type" content="article"><meta property="og:title" content="文本生成"><meta property="og:url" content="https://nivbi.com/posts/3cd0dfc0.html"><meta property="og:site_name" content="ihadu"><meta property="og:description" content="第5章  文本生成 基于Transformers的语言模型最不可思议的特点之一是它们能够生成与人类所写的文本几乎没有区别的文本。一个著名的例子是OpenAI的GPT-2，它在给出以下提示时: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-k7jklm_1920x1080.webp"><meta property="article:published_time" content="2024-01-22T07:31:38.000Z"><meta property="article:modified_time" content="2024-03-15T02:47:04.445Z"><meta property="article:author" content="ihadu"><meta property="article:tag" content="blog"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-k7jklm_1920x1080.webp"><link rel="shortcut icon" href="/images/dog.png"><link rel="canonical" href="https://nivbi.com/posts/3cd0dfc0.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="google-site-verification" content="h-TVRSWVCnbA8LDyXZkYIrIJ4pr1OUAgVfR5h2H4Eok"><meta name="baidu-site-verification" content="codeva-V4Yhtugi3b"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/node-snackbar/0.1.16/snackbar.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.19/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?546017bccd857b94cdaba2806f36802a",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!1,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:{chs_to_cht:"你已切换为繁体",cht_to_chs:"你已切换为简体",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#49b1f5",bgDark:"#1f1f1f",position:"bottom-left"},source:{justifiedGallery:{js:"https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.js",css:"https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.css"}},isPhotoFigcaption:!1,islazyload:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"文本生成",isPost:!0,isHome:!1,isHighlightShrink:!0,isToc:!0,postUpdate:"2024-03-15 10:47:04"}</script><noscript><style type="text/css">#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,a){0!==a&&(a=864e5*a,t={value:t,expiry:(new Date).getTime()+a},localStorage.setItem(e,JSON.stringify(t)))},get:function(e){var t=localStorage.getItem(e);if(t){t=JSON.parse(t);if(!((new Date).getTime()>t.expiry))return t.value;localStorage.removeItem(e)}}},e.getScript=o=>new Promise((t,e)=>{const a=document.createElement("script");a.src=o,a.async=!0,a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.getCSS=(o,n=!1)=>new Promise((t,e)=>{const a=document.createElement("link");a.rel="stylesheet",a.href=o,n&&(a.id=n),a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};e=saveToLocal.get("theme"),"dark"===e?activateDarkMode():"light"===e&&activateLightMode(),e=saveToLocal.get("aside-status");void 0!==e&&("hide"===e?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><meta name="generator" content="Hexo 6.0.0"></head><body><script>window.paceOptions={restartOnPushState:!1},document.addEventListener("pjax:send",()=>{Pace.restart()})</script><link rel="stylesheet" href="/assets/css/flash.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://oss.kiscloud.net/image/user/anime/Chainsaw/qiu.webp" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">247</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">32</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">36</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/nav/"><i class="fa-fw fa fa-paper-plane"></i><span> 导航</span></a></div><div class="menus_item"><a class="site-page" href="/resume/"><i class="fa-fw fa fa-archive"></i><span> 简历</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-book"></i><span> 教程</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/python/"><i class="fa-fw fa-brands fa-python"></i><span> python</span></a></li><li><a class="site-page child" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"><i class="fa-fw fa-solid fa-database"></i><span> 大数据</span></a></li><li><a class="site-page child" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><i class="fa-fw fa-solid fa-desktop"></i><span> 机器学习</span></a></li><li><a class="site-page child" href="/DeepLearning/"><i class="fa-fw fas fa-video"></i><span> 深度学习</span></a></li><li><a class="site-page child" href="/categories/Transformers/"><i class="fa-fw fas fa-book-open"></i><span> Transformers</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="ihadu"><img class="site-icon" src="/images/sports.webp"></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/nav/"><i class="fa-fw fa fa-paper-plane"></i><span> 导航</span></a></div><div class="menus_item"><a class="site-page" href="/resume/"><i class="fa-fw fa fa-archive"></i><span> 简历</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-book"></i><span> 教程</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/python/"><i class="fa-fw fa-brands fa-python"></i><span> python</span></a></li><li><a class="site-page child" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"><i class="fa-fw fa-solid fa-database"></i><span> 大数据</span></a></li><li><a class="site-page child" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><i class="fa-fw fa-solid fa-desktop"></i><span> 机器学习</span></a></li><li><a class="site-page child" href="/DeepLearning/"><i class="fa-fw fas fa-video"></i><span> 深度学习</span></a></li><li><a class="site-page child" href="/categories/Transformers/"><i class="fa-fw fas fa-book-open"></i><span> Transformers</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">文本生成</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-22T07:31:38.000Z" title="发表于 2024-01-22 15:31:38">2024-01-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-15T02:47:04.445Z" title="更新于 2024-03-15 10:47:04">2024-03-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Transformers/">Transformers</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>27分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="文本生成"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/3cd0dfc0.html#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/posts/3cd0dfc0.html" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div><article class="post-content" id="article-container"><h1>第5章 文本生成</h1><p>基于Transformers的语言模型最不可思议的特点之一是它们能够生成与人类所写的文本几乎没有区别的文本。一个著名的例子是OpenAI的GPT-2，它在给出以下提示时:</p><p><em>In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.</em></p><p>能够生成一篇关于独角兽的新闻：</p><ul class="lvl-0"><li class="lvl-2"></li></ul><p>The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science. Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved. Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow. Pérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez. Pérez and his friends were astonished to see the unicorn herd. These creatures could be seen from the air without having to move too much to see them—they were so close they could touch their horns. While examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English …</p><ul class="lvl-0"><li class="lvl-2"></li></ul><p>这个例子之所以如此引人注目，是因为它是在没有任何明确监督的情况下产生的! 通过简单地学习预测数以百万计的网页文本中的下一个词，GPT-2和它更强大的后代，如GPT-3，能够获得一套广泛的技能和模式识别能力。能被不同类型的输入提示激活的能力。图5-1显示了语言模型在预训练期间有时会接触到一些任务序列，在这些任务中，它们需要仅仅根据上下文来预测下面的标记，如加法、解词和翻译。这使得它们在微调期间或（如果模型足够大）在推理时间有效地转移这些知识。这些任务不是提前选择的，而是在用于训练十亿参数语言模型的巨大语料库中自然出现的。</p><p><img src="/images/transformers/chapter5/image-20220214201135577.png" alt="image-20220214201135577"></p><p>Transformers生成现实文本的能力导致了多样化的应用，如InferKit、Write With Transformer、AI Dungeon，以及像谷歌的Meena这样的对话代理，它甚至可以讲出老套的笑话，如图5-2所示!</p><p><img src="/images/transformers/chapter5/image-20220214201207915.png" alt="image-20220214201207915"></p><p>在本章中，我们将使用GPT-2来说明语言模型的文本生成方式，并探讨不同的解码策略如何影响生成的文本。</p><h2 id="产生连贯性文本的挑战">产生连贯性文本的挑战</h2><p>在本书中，到目前为止，我们一直专注于通过预训练和监督微调的组合来处理NLP任务。正如我们所看到的，对于像序列或标记分类这样的特定任务，产生预测是相当直接的；模型产生一些对数，我们或者取最大值来获得预测的类别，或者应用softmax函数来获得每个类别的预测概率。相比之下，将模型的概率输出转换为文本需要一种解码方法，这就引入了一些文本生成所特有的挑战：</p><ul class="lvl-0"><li class="lvl-2"><p>解码是反复进行的，因此比简单地将输入通过模型的前向传递一次涉及到更多的计算。</p></li><li class="lvl-2"><p>生成文本的质量和多样性取决于解码方法和相关超参数的选择。</p></li></ul><p>为了理解这个解码过程是如何进行的，让我们先来看看GPT-2是如何进行预训练并随后应用于生成文本的。</p><p>像其他自回归或因果语言模型一样，GPT-2被预训练来估计文本中出现的标记序列y = y1, y2, … yt的概率P（y|x），给定一些初始提示或语境序列x = x1, x2, … xk。由于获得足够的训练数据来直接估计P(y|x)是不切实际的，所以通常使用概率链规则来将其分解为条件概率的乘积。</p><p><img src="/images/transformers/chapter5/image-20220214201402795.png" alt="image-20220214201402795"></p><p>其中y&lt;t是序列y1, …, yt-1的速记符号。正是从这些条件概率中，我们获得了这样的直觉：自回归语言建模相当于在一个句子中给定前面的词来预测每个词；这正是前面方程中右边的概率所描述的。请注意，这个预训练目标与BERT的预训练目标完全不同，BERT利用过去和未来的语境来预测一个被掩盖的标记。</p><p>现在你可能已经猜到我们如何调整这个下一个标记的预测任务，以生成任意长度的文本序列。如图5-3所示，我们从 &quot;变形金刚是 &quot;这样的提示开始，用模型来预测下一个标记。一旦我们确定了下一个标记，我们就把它附加到提示上，然后用新的输入序列来生成另一个标记。我们这样做，直到我们达到一个特殊的序列结束符号或预先定义的最大长度。</p><p><img src="/images/transformers/chapter5/image-20220214201440669.png" alt="image-20220214201440669"></p><p><strong>注意事项</strong></p><p>由于输出序列以输入提示的选择为条件，这种类型的文本生成通常被称为条件性文本生成。</p><p>这个过程的核心是一个解码方法，决定在每个时间段选择哪个标记。由于语言模型头在每个步骤中对词汇中的每个标记产生一个logit zt,i，我们可以通过采取softmax得到下一个可能的标记wi的概率分布。</p><p><img src="/images/transformers/chapter5/image-20220214201532451.png" alt="image-20220214201532451"></p><p>大多数解码方法的目标是通过挑选一个ˆy来搜索最可能的整体序列，从而使之成为。</p><p><img src="/images/transformers/chapter5/image-20220214201627488.png" alt="image-20220214201627488"></p><p>直接找到ˆy将涉及到用语言模型评估每一个可能的序列。由于不存在一种能在合理时间内完成这一工作的算法，我们只能依靠近似值来代替。在本章中，我们将探索其中的一些近似方法，并逐步建立起 逐步建立起更智能、更复杂的算法，以用于生成高质量的文本。</p><h2 id="贪婪搜索解码">贪婪搜索解码</h2><p>从模型的连续输出中获得离散标记的最简单的解码方法是贪婪地选择每个时间点上概率最大的标记：</p><p><img src="/images/transformers/chapter5/image-20220214201712067.png" alt="image-20220214201712067"></p><p>为了了解贪婪搜索是如何工作的，让我们先用语言建模头加载15亿参数版本的GPT-2：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch </span><br><span class="line">from transformers import AutoTokenizer, AutoModelForCausalLM </span><br><span class="line">device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; </span><br><span class="line">model_name = &quot;gpt2-xl&quot; </span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name) </span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_name).to(device)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在让我们来生成一些文本! 尽管Transformers为GPT-2这样的自回归模型提供了一个generate()函数，但我们将自己实现这个解码方法，看看引擎盖下发生了什么。为了热身，我们将采取图5-3所示的相同的迭代方法：我们将使用 &quot;变形金刚是 &quot;作为输入提示，并运行八个时间段的解码。在每个时间步骤中，我们为提示中的最后一个符号挑选出模型的对数，并用softmax包起来，得到一个概率分布。然后，我们挑选出概率最高的下一个符号，将其添加到输入序列中，并再次运行该过程。下面的代码完成了这项工作，并且还在每个时间段存储了五个最有可能的标记，这样我们就可以直观地看到备选方案：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">input_txt = &quot;Transformers are the&quot; </span><br><span class="line">input_ids = tokenizer(input_txt, return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;].to(device) </span><br><span class="line">iterations = [] </span><br><span class="line">n_steps = 8 </span><br><span class="line">choices_per_step = 5</span><br><span class="line">with torch.no_grad(): </span><br><span class="line">	for _ in range(n_steps): </span><br><span class="line">		iteration = dict() </span><br><span class="line">		iteration[&quot;Input&quot;] = tokenizer.decode(input_ids[0]) </span><br><span class="line">		output = model(input_ids=input_ids) </span><br><span class="line">		# Select logits of the first batch and the last token and apply softmax 			</span><br><span class="line">		next_token_logits = output.logits[0, -1, :]</span><br><span class="line">		next_token_probs = torch.softmax(next_token_logits, dim=-1) </span><br><span class="line">		sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True) </span><br><span class="line">		# Store tokens with highest probabilities </span><br><span class="line">		for choice_idx in range(choices_per_step): </span><br><span class="line">			token_id = sorted_ids[choice_idx] </span><br><span class="line">			token_prob = next_token_probs[token_id].cpu().numpy() </span><br><span class="line">			token_choice = ( f&quot;&#123;tokenizer.decode(token_id)&#125; (&#123;100 * token_prob:.2f&#125;%)&quot; ) </span><br><span class="line">			iteration[f&quot;Choice &#123;choice_idx+1&#125;&quot;] = token_choice </span><br><span class="line">			# Append predicted next token to input </span><br><span class="line">			input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1) </span><br><span class="line">			iterations.append(iteration) pd.DataFrame(iterations)</span><br><span class="line">pd.DataFrame(iterations)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter5/image-20220214202106391.png" alt="image-20220214202106391"></p><p>通过这种简单的方法，我们能够生成 &quot;变形金刚是世界上最受欢迎的玩具系列 &quot;的句子。有趣的是，这表明GPT-2已经内化了一些关于变形金刚媒体系列的知识，它是由两家玩具公司（Hasbro和Takara Tomy）创造的。我们还可以看到每一步都有其他可能的延续，这显示了文本生成的迭代性质。与序列分类等其他任务不同的是，在序列分类中，单一的前向传递就足以产生预测，而在文本生成中，我们需要一次一次地对输出标记进行解码。</p><p>实现贪婪搜索并不难，但我们要使用Transformers内置的generate()函数来探索更复杂的解码方法。为了重现我们的简单例子，让我们 确保采样被关闭（默认情况下是关闭的，除非你加载检查点的模型的具体配置另有规定），并为新生成的标记数量指定max_new_tokens：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">input_ids = tokenizer(input_txt, return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;].to(device) </span><br><span class="line">output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False) </span><br><span class="line">print(tokenizer.decode(output[0])) </span><br><span class="line"></span><br><span class="line">Transformers are the most popular toy line in the world,</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在让我们尝试一些更有趣的东西：我们能重现OpenAI的独角兽故事吗？正如我们之前所做的，我们将用标记器对提示进行编码，并且我们将为max_length指定一个较大的值，以生成一个较长的文本序列：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">max_length = 128 </span><br><span class="line">input_txt = &quot;&quot;&quot;In a shocking finding, scientist discovered \ a herd of unicorns living in a remote, previously unexplored \ valley, in the Andes Mountains. Even more surprising to the \ researchers was the fact that the unicorns spoke perfect English.\n\n &quot;&quot;&quot; </span><br><span class="line">input_ids = tokenizer(input_txt, return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;].to(device) </span><br><span class="line">output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False) </span><br><span class="line">print(tokenizer.decode(output_greedy[0])) </span><br><span class="line"></span><br><span class="line">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees. The researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans</span><br><span class="line">The researchers were surprised to find that the unicorns were able</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>好吧，前几句与OpenAI的例子完全不同，而且有趣的是，不同的大学都被归功于这一发现！我们还可以看到贪婪搜索解码的一个主要缺点：往往会产生重复的输出序列，这在新闻报道中当然不可取。我们还可以看到贪婪搜索解码的一个主要缺点：它倾向于产生重复的输出序列，这在一篇新闻文章中当然是不可取的。这是贪婪搜索算法的一个常见问题，它可能无法给你提供最佳解决方案；在解码的背景下，它们可能会错过整体概率较高的单词序列，只是因为高概率的单词刚好在低概率的单词之前。</p><p>幸运的是，我们可以做得更好–让我们研究一种被称为beam搜索解码的流行方法。</p><p><strong>注意事项</strong></p><p>虽然贪婪搜索解码很少用于需要多样性的文本生成任务，但它对于生成像算术这样的短序列是很有用的，在这种情况下，人们更喜欢确定性的和事实正确的输出。对于这些任务，你可以通过提供一些行间分隔的例子作为输入提示 &quot;5 + 8 =&gt; 13\n 7 + 2 =&gt; 9\n 1 + 0 =&gt;&quot;的格式来调节GPT-2。</p><h2 id="Beam搜索解码-（beam-search-decoding">Beam搜索解码 （beam search decoding)</h2><p>Beam搜索不是在每一步解码概率最高的标记，而是跟踪前b个最有可能的下一个标记，其中b被称为波束或部分假说的数量。下一组波束的选择是考虑现有波束的所有可能的下一个标记的扩展，并选择b个最可能的扩展。这个过程重复进行，直到我们达到最大长度或EOS标记，然后根据对数概率对b个波束进行排序，选择最可能的序列。图5-4中显示了一个Beam搜索的例子。</p><p><img src="/images/transformers/chapter5/image-20220214202454948.png" alt="image-20220214202454948"></p><p>为什么我们要用对数概率而不是概率本身对序列进行评分？计算一个序列的总体概率P（y1，y2，…，yt|x）涉及计算条件概率P（yt|y&lt;t，x）的乘积是一个原因。由于每个条件概率通常是[0，1]范围内的一个小数字，取它们的乘积会导致总的概率很容易出现下溢。这意味着计算机不能再精确地表示计算的结果。例如，假设我们有一个由t = 1024个标记组成的序列，并慷慨地假设每个标记的概率为0.5。这个序列的总体概率是一个极小的数字：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0.5 ** 1024 </span><br><span class="line">5.562684646268003e-309</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>导致数值不稳定，因为我们遇到了下溢。我们可以通过计算一个相关项，即对数概率来避免这种情况。如果我们将对数应用于联合概率和条件概率，那么在对数的乘积规则的帮助下，我们可以得到：</p><p><img src="/images/transformers/chapter5/image-20220214202552207.png" alt="image-20220214202552207"></p><p>换句话说，我们之前看到的概率乘积变成了对数概率之和，这就更不可能遇到数字不稳定的情况。例如，计算之前同一个例子的对数概率，可以得到：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np </span><br><span class="line">sum([np.log(0.5)] * 1024) </span><br><span class="line">-709.7827128933695</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这是一个我们可以轻松处理的数字，而且这种方法对更小的数字仍然有效。由于我们只想比较相对概率，我们可以直接用对数概率来做。</p><p>让我们计算并比较贪婪和Beam搜索产生的文本的对数概率，看看Beam搜索是否能提高整体概率。由于Transformers模型返回的是给定输入标记的下一个标记的未归一化对数，我们首先需要将对数归一化，以便为序列中的每个标记创建整个词汇的概率分布。然后，我们需要只选择序列中存在的标记概率。下面的函数实现了这些步骤。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.nn.functional as F </span><br><span class="line">def log_probs_from_logits(logits, labels): </span><br><span class="line">	logp = F.log_softmax(logits, dim=-1) </span><br><span class="line">	logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1) </span><br><span class="line">	return logp_label</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这给我们提供了单个标记的对数概率，所以要得到一个序列的总对数概率，我们只需要将每个标记的对数概率相加：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def sequence_logprob(model, labels, input_len=0): </span><br><span class="line">	with torch.no_grad(): </span><br><span class="line">		output = model(labels) </span><br><span class="line">		log_probs = log_probs_from_logits( output.logits[:, :-1, :], labels[:, 1:])</span><br><span class="line">        seq_log_prob = torch.sum(log_probs[:, input_len:]) </span><br><span class="line">    return seq_log_prob.cpu().numpy()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>注意，我们忽略了输入序列的对数概率，因为它们不是由模型生成的。我们还可以看到，将对数和标签对齐是很重要的；因为模型预测了下一个标记，所以我们没有得到第一个标签的对数，我们也不需要最后一个对数，因为我们没有它的地面真相标记。</p><p>让我们用这些函数来首先计算OpenAI提示上的贪婪解码器的序列对数概率。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0])) </span><br><span class="line">print(tokenizer.decode(output_greedy[0])) </span><br><span class="line">print(f&quot;\nlog-prob: &#123;logp:.2f&#125;&quot;) </span><br><span class="line"></span><br><span class="line">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. </span><br><span class="line"></span><br><span class="line">The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees. </span><br><span class="line">The researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans. </span><br><span class="line">The researchers were surprised to find that the unicorns were able </span><br><span class="line">log-prob: -87.43</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在让我们把它与用Beam搜索生成的序列进行比较。要用generate()函数激活Beam搜索，我们只需要用num_beams参数指定波束的数量。我们选择的波束越多，可能得到的结果就越好；然而，生成过程会变得更慢，因为我们为每个波束生成平行序列：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False) </span><br><span class="line">logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))</span><br><span class="line">print(tokenizer.decode(output_beam[0])) </span><br><span class="line">print(f&quot;\nlog-prob: &#123;logp:.2f&#125;&quot;) </span><br><span class="line"></span><br><span class="line">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.</span><br><span class="line"></span><br><span class="line">The discovery of the unicorns was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society. </span><br><span class="line">The scientists were conducting a study of the Andes Mountains when they discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English </span><br><span class="line"></span><br><span class="line">log-prob: -55.23</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们可以看到，我们用Beam搜索得到的对数概率（越高越好）比用简单的贪婪解码得到的要好。然而，我们可以看到，Beam搜索也受到重复文本的影响。解决这个问题的一个方法是用no_repeat_ngram_size参数施加一个n-gram惩罚，跟踪哪些n-gram已经被看到，并将下一个token的概率设置为零，如果它将产生一个以前看到的n-gram：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2) </span><br><span class="line">logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0])</span><br><span class="line"></span><br><span class="line">print(tokenizer.decode(output_beam[0]))</span><br><span class="line">print(f&quot;\nlog-prob: &#123;logp:.2f&#125;&quot;) </span><br><span class="line"></span><br><span class="line">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. The discovery was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society. </span><br><span class="line"></span><br><span class="line">According to a press release, the scientists were conducting a survey of the area when they came across the herd. They were surprised to find that they were able to converse with the animals in English, even though they had never seen a unicorn in person before. The researchers were </span><br><span class="line">log-prob: -93.12</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这还不算太糟！我们已经设法停止了重复，而且我们可以看到，尽管产生了较低的分数，但文本仍然是连贯的。带n-gram惩罚的Beam搜索是一种很好的方法，可以在关注高概率的标记（用Beam搜索）和减少重复（用n-gram惩罚）之间找到一个平衡点，它通常用于总结或机器翻译等事实正确性很重要的应用中。当事实的正确性不如生成的输出的多样性重要时，例如在开放领域的闲聊或故事生成中，另一种减少重复同时提高多样性的方法是使用抽样。让我们通过研究几种最常见的抽样方法来完成我们对文本生成的探索。</p><h2 id="采样方法">采样方法</h2><p>最简单的抽样方法是在每个时间点上从模型输出的概率分布中随机抽样：</p><p><img src="/images/transformers/chapter5/image-20220214203122780.png" alt="image-20220214203122780"></p><p>其中|V|表示词汇的cardinality。我们可以通过添加一个温度参数T来轻松控制输出的多样性，该参数在采取softmax之前重新调整对数：</p><p><img src="/images/transformers/chapter5/image-20220214203149542.png" alt="image-20220214203149542"></p><p>通过调整T，我们可以控制概率分布的形状。当T≪1≫时，分布在原点周围变得尖锐，罕见的标记被压制。另一方面，当T≫1时，分布变得平缓，每个令牌的可能性相同。温度对标记概率的影响见图5-5。</p><p><img src="/images/transformers/chapter5/image-20220214203209475.png" alt="image-20220214203209475"></p><p>为了看看我们如何利用温度来影响生成的文本，让我们通过在generate()函数中设置温度参数，以T=2为例进行采样（我们将在下一节解释top_k参数的含义）：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=2.0, top_k=0) </span><br><span class="line">print(tokenizer.decode(output_temp[0])) </span><br><span class="line"></span><br><span class="line">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. </span><br><span class="line"></span><br><span class="line">While the station aren protagonist receive Pengala nostalgiates tidbitRegarding </span><br><span class="line"></span><br><span class="line">Jenny loclonju AgreementCON irrational �rite Continent seaf A jer Turner Dorbecue WILL Pumpkin mere Thatvernuildagain YoAniamond disse * Runewitingkusstemprop&#125;);b zo coachinginventorymodules deflation press Vaticanpres Wrestling chargesThingsctureddong Ty physician PET KimBi66 graz Oz at aff da temporou MD6 radi iter</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们可以清楚地看到，高温产生了大部分的胡言乱语；通过强调罕见的标记，我们使模型产生了奇怪的语法和相当多的生造词！让我们看看如果我们把温度降下来会发生什么？让我们看看如果我们降低温度会发生什么：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=0.5, top_k=0) </span><br><span class="line">print(tokenizer.decode(output_temp[0])) </span><br><span class="line"></span><br><span class="line">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. The scientists were searching for the source of the mysterious sound, which was making the animals laugh and cry.</span><br><span class="line"></span><br><span class="line">The unicorns were living in a remote valley in the Andes mountains &#x27;When we first heard the noise of the animals, we thought it was a lion or a tiger,&#x27; said Luis Guzman, a researcher from the University of Buenos Aires, Argentina.</span><br><span class="line"></span><br><span class="line">&#x27;But when</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这明显更有连贯性，甚至还包括了另一所大学因这一发现而被引用的一段话 我们可以从温度中得出的主要教训是，它允许我们控制样本的质量，但在一致性（低温）和多样性（高温）之间总有一个权衡，人们必须根据手头的使用情况进行调整。</p><p>调整一致性和多样性之间权衡的另一种方法是截断词汇的分布。这使我们能够随着温度自由地调整多样性，但在一个更有限的范围内，排除那些在语境中过于奇怪的词（即低概率词）。有两种主要的方法：top-k和nucleus（或top-p）采样。我们来看看。</p><h2 id="Top-k和核抽样">Top-k和核抽样</h2><p>Top-k和nucleus（top-p）抽样是两种流行的替代方法或使用温度的扩展。在这两种情况下，其基本思想是限制我们在每个时间步长中可以取样的可能标记的数量。为了了解这一点，首先让我们把模型在T=1时的累积概率分布可视化，如图5-6所示。</p><p>让我们把这些图分开，因为它们包含了大量的信息。在上面的图中，我们可以看到令牌概率的柱状图。它在10-8左右有一个峰值，在10-4左右有一个较小的峰值，然后是急剧下降，只有少数几个概率在10-2和10-1之间的标记出现。看这张图，我们可以看到，选择概率最高的标记的 挑选概率最高的标记（10-1处的孤立条）的概率是1/10。</p><p><img src="/images/transformers/chapter5/image-20220214203526334.png" alt="image-20220214203526334"></p><p>在下图中，我们按概率降序排列标记，并计算前10,000个标记的累积总和（GPT-2的词汇中总共有50,257个标记）。弧线代表挑选前面任何一个标记的概率。例如，在概率最高的1,000个标记中，大约有96%的机会挑选任何一个标记。我们看到，该概率迅速上升到90%以上，但在几千个标记之后才饱和，接近100%。该图 显示，有1/100的概率没有选到任何甚至不在前2000名的标记。</p><p>虽然这些数字乍看之下可能很小，但它们变得很重要，因为在生成文本时，我们对每个标记取样一次。因此，即使只有1/100或1/1000的机会，如果我们取样数百次，就有很大的机会在某一时刻选到一个不可能的标记，而且在取样时选到这样的标记会严重影响生成文本的质量。出于这个原因，我们通常希望避免这些非常不可能的标记。这就是top-k和top-p采样发挥作用的地方。</p><p>top-k抽样背后的想法是通过只从概率最高的k个标记中抽样来避免低概率的选择。这就在分布的长尾上设置了一个固定的切口，确保我们只从可能的选择中取样。回到图5-6，top-k抽样相当于定义一条垂直线并从左边的标记中抽样。同样，generate()函数通过top_k参数提供了一个简单的方法来实现这一点:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">output_topk = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=50) </span><br><span class="line">print(tokenizer.decode(output_topk[0])) </span><br><span class="line"></span><br><span class="line">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.</span><br><span class="line"></span><br><span class="line">The wild unicorns roam the Andes Mountains in the region of Cajamarca, on the border with Argentina (Picture: Alamy/Ecole Nationale Supérieure d&#x27;Histoire Naturelle) </span><br><span class="line"></span><br><span class="line">The researchers came across about 50 of the animals in the valley. They had lived in such a remote and isolated area at that location for nearly a thousand </span><br><span class="line">years that</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这可以说是我们迄今为止生成的最像人类的文本。但是我们如何选择k呢？k的值是手动选择的，对序列中的每个选择都是一样的，与实际的输出分布无关。序列中的每个选择都是一样的，与实际的输出分布无关。我们可以通过查看一些文本质量指标来找到一个好的k值，我们将在下一章探讨这个问题–但这个固定的截止值可能并不十分令人满意。</p><p>另一种方法是使用动态截断。在核抽样或顶抽样中，我们不是选择一个固定的截断值，而是设定一个截断的时间条件。这个条件就是在选择中达到一定的概率质量时。比方说，我们把这个值设定为95%。然后我们按概率降序排列所有标记，并从列表的顶部开始一个接一个地添加标记，直到所选标记的概率之和达到95%。回到图5-6，p的值在概率累积总和图上定义了一条水平线，我们只从该线以下的标记中取样。根据输出分布，这可能只是一个（非常可能的）标记，也可能是一百个（同样可能的）标记。在这一点上，你可能对generate()函数也提供了一个激活top-p抽样的参数而不感到惊讶。让我们来试试吧:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">output_topp = model.generate(input_ids, max_length=max_length, do_sample=True, top_p=0.90) </span><br><span class="line">print(tokenizer.decode(output_topp[0])) </span><br><span class="line"></span><br><span class="line">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. </span><br><span class="line"></span><br><span class="line">The scientists studied the DNA of the animals and came to the conclusion that the herd are descendants of a prehistoric herd that lived in Argentina about 50,000 years ago.</span><br><span class="line"></span><br><span class="line">According to the scientific analysis, the first humans who migrated to South America migrated into the Andes Mountains from South Africa and Australia, after the last ice age had ended. </span><br><span class="line"></span><br><span class="line">Since their migration, the animals have been adapting to</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Top-p采样也产生了一个连贯的故事，而且这次有一个新的转折点，关于从澳大利亚到南美洲的移民。你甚至可以把这两种抽样方法结合起来，以获得两个世界的最佳效果。设置top_k=50和top_p=0.9，相当于从最多50个标记的池子里选择概率质量为90%的标记的规则。</p><p><strong>注意事项</strong></p><p>当我们使用抽样时，我们也可以应用Beam搜索。与其贪婪地选择下一批候选标记，我们可以对它们进行抽样，并以同样的方式建立起波束。</p><h2 id="哪种解码方法是最好的？">哪种解码方法是最好的？</h2><p>不幸的是，没有一个普遍的 &quot;最佳 &quot;解码方法。哪种方法最好，取决于你生成文本的任务的性质。如果你想让你的模型执行一个精确的任务，如进行算术运算或提供一个特定问题的答案，那么你应该降低温度或使用确定性的方法，如贪婪搜索与Beam搜索相结合，以保证得到最可能的答案。如果你想让模型生成更长的文本，甚至有点创造性，那么你应该改用抽样方法，并提高温度，或者使用top-k和核抽样的混合方法。</p><h2 id="结论-2">结论</h2><p>在这一章中，我们研究了文本生成，这是一项与我们之前遇到的NLU任务截然不同的任务。生成文本需要对每个生成的标记进行至少一次前向传递，如果我们使用Beam搜索，则需要更多。这使得文本生成对计算的要求很高，人们需要合适的基础设施来大规模地运行文本生成模型。此外，一个好的解码策略，将模型的输出概率转化为离散的标记，可以提高文本质量。找到 最好的解码策略需要进行一些实验和对生成的文本进行主观评价。然而，在实践中，我们不希望仅凭直觉来做这些决定。和其他NLP任务一样，我们应该选择一个能反映我们想要解决的问题的模型性能指标。不出所料，选择的范围很广，我们将在下一章中遇到最常见的选择，在这一章中我们将看看如何训练和评估文本总结的模型。或者，如果你迫不及待地想学习如何从头开始训练一个GPT类型的模型，你可以直接跳到第10章，在那里我们收集一个大型的代码数据集，然后在上面训练一个自回归语言模型。</p></article><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-k7jklm_1920x1080.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/images/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/images/wechat.jpg" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/images/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/images/alipay.jpg" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/c992bf78.html" title="文本摘要"><img class="cover" src="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-j563gq_1920x1080.webp" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">文本摘要</div></div></a></div><div class="next-post pull-right"><a href="/posts/bf838bc3.html" title="多语言命名实体识别"><img class="cover" src="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-gp8553_1920x1080.webp" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">多语言命名实体识别</div></div></a></div></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://oss.kiscloud.net/image/user/anime/Chainsaw/qiu.webp" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">ihadu</div><div class="author-info__description">寄蜉蝣于天地，渺沧海之一粟。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">247</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">32</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">36</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/ihadu"><i class="fab fa-github"></i><span>look Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ihadu" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:ihadyou@qq.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://weibo.com/u/5992742619" rel="external nofollow noreferrer" target="_blank"><i class="fa-brands fa-weibo"></i></a><a class="social-icon" href="https://www.cnblogs.com/ihadu/post-categories/#/c/subject/category/default.html" rel="external nofollow noreferrer" target="_blank"><i class="fa-solid fa-blog"></i></a><a class="social-icon" href="/nav/" target="_blank"><i class="fa fa-paper-plane"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Have you found your way home yet</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">第5章 文本生成</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%A7%E7%94%9F%E8%BF%9E%E8%B4%AF%E6%80%A7%E6%96%87%E6%9C%AC%E7%9A%84%E6%8C%91%E6%88%98"><span class="toc-number">1.1.</span> <span class="toc-text">产生连贯性文本的挑战</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%AA%E5%A9%AA%E6%90%9C%E7%B4%A2%E8%A7%A3%E7%A0%81"><span class="toc-number">1.2.</span> <span class="toc-text">贪婪搜索解码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Beam%E6%90%9C%E7%B4%A2%E8%A7%A3%E7%A0%81-%EF%BC%88beam-search-decoding"><span class="toc-number">1.3.</span> <span class="toc-text">Beam搜索解码 （beam search decoding)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">采样方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Top-k%E5%92%8C%E6%A0%B8%E6%8A%BD%E6%A0%B7"><span class="toc-number">1.5.</span> <span class="toc-text">Top-k和核抽样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%93%AA%E7%A7%8D%E8%A7%A3%E7%A0%81%E6%96%B9%E6%B3%95%E6%98%AF%E6%9C%80%E5%A5%BD%E7%9A%84%EF%BC%9F"><span class="toc-number">1.6.</span> <span class="toc-text">哪种解码方法是最好的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-2"><span class="toc-number">1.7.</span> <span class="toc-text">结论</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/bf3a593a.html" title="未来发展方向"><img src="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-e7d1q8_1920x1080.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="未来发展方向"></a><div class="content"><a class="title" href="/posts/bf3a593a.html" title="未来发展方向">未来发展方向</a><time datetime="2024-01-22T07:35:24.000Z" title="发表于 2024-01-22 15:35:24">2024-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/cc3455bf.html" title="从头训练 Transformers 模型"><img src="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-85xm2j_1920x1080.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="从头训练 Transformers 模型"></a><div class="content"><a class="title" href="/posts/cc3455bf.html" title="从头训练 Transformers 模型">从头训练 Transformers 模型</a><time datetime="2024-01-22T07:34:57.000Z" title="发表于 2024-01-22 15:34:57">2024-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3359ec3c.html" title="处理少标签或零标签情形"><img src="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-rr7ldm_1920x1080.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="处理少标签或零标签情形"></a><div class="content"><a class="title" href="/posts/3359ec3c.html" title="处理少标签或零标签情形">处理少标签或零标签情形</a><time datetime="2024-01-22T07:34:26.000Z" title="发表于 2024-01-22 15:34:26">2024-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/910a108.html" title="优化Transformers性能"><img src="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-jxy6vp_1920x1080.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="优化Transformers性能"></a><div class="content"><a class="title" href="/posts/910a108.html" title="优化Transformers性能">优化Transformers性能</a><time datetime="2024-01-22T07:33:50.000Z" title="发表于 2024-01-22 15:33:50">2024-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/c0b5c215.html" title="问答系统"><img src="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-m3gl2m_1920x1080.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="问答系统"></a><div class="content"><a class="title" href="/posts/c0b5c215.html" title="问答系统">问答系统</a><time datetime="2024-01-22T07:32:45.000Z" title="发表于 2024-01-22 15:32:45">2024-01-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2025 By ihadu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><img src="https://haiyong.site/img/icp.png"><a href="https://beian.miit.gov.cn/#/Integrated/index" rel="external nofollow noreferrer" style="color:#fff" target="_blank">皖ICP备19024061号-4</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.19/fancybox/fancybox.umd.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.2.0/instantpage.min.js" type="module"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/node-snackbar/0.1.16/snackbar.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js").then(()=>{pangu.autoSpacingPage()})}function panguInit(){GLOBAL_CONFIG_SITE.isPost&&panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>function loadValine(){function n(){new Valine(Object.assign({el:"#vcomment",appId:"kriIEcS9Jj1uTpNoV1sFu2Rk-gzGzoHsz",appKey:"mnmRNJ7HtlpE7664tpn7Kfmy",avatar:"monsterid",serverURLs:"",emojiMaps:"",path:window.location.pathname,visitor:!1},null))}"function"==typeof Valine?n():getScript("https://cdnjs.cloudflare.com/ajax/libs/valine/1.5.1/Valine.min.js").then(n)}function loadOtherComment(){loadValine()}setTimeout(loadValine,0)</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.3/fireworks.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.3/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!0,POWERMODE.mobile=!0,document.body.addEventListener("input",POWERMODE)</script><script src="https://cdnjs.cloudflare.com/ajax/libs/pjax/0.2.8/pjax.min.js"></script><script>let pjaxSelectors=["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"];var pjax=new Pjax({elements:'a:not([target="_blank"]):not([href="/music/"])',selectors:pjaxSelectors,cacheBust:!1,analytics:!1,scrollRestoration:!1});document.addEventListener("pjax:send",function(){if(window.tocScrollFn&&window.removeEventListener("scroll",window.tocScrollFn),window.scrollCollect&&window.removeEventListener("scroll",scrollCollect),document.getElementById("rightside").style.cssText="opacity: ''; transform: ''",window.aplayers)for(let e=0;e<window.aplayers.length;e++)window.aplayers[e].options.fixed||window.aplayers[e].destroy();"object"==typeof typed&&typed.destroy();var e=document.body.classList;e.contains("read-mode")&&e.remove("read-mode"),"object"==typeof disqusjs&&disqusjs.destroy()}),document.addEventListener("pjax:complete",function(){window.refreshFn(),document.querySelectorAll("script[data-pjax]").forEach(e=>{const t=document.createElement("script");var o=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach(e=>t.setAttribute(e.name,e.value)),t.appendChild(document.createTextNode(o)),e.parentNode.replaceChild(t,e)}),GLOBAL_CONFIG.islazyload&&window.lazyLoadInstance.update(),"function"==typeof panguInit&&panguInit(),"function"==typeof gtag&&gtag("config","",{page_path:window.location.pathname}),"object"==typeof _hmt&&_hmt.push(["_trackPageview",window.location.pathname]),"function"==typeof loadMeting&&document.getElementsByClassName("aplayer").length&&loadMeting(),"object"==typeof Prism&&Prism.highlightAll()}),document.addEventListener("pjax:error",e=>{404===e.request.status&&pjax.loadUrl("/404.html")})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{scale:1,hHeadPos:.5,vHeadPos:.618,jsonPath:"/live2d_models/gun/95type_405/normal/model.json"},display:{superSample:2,position:"left",width:200,height:400,hOffset:30,vOffset:-80},mobile:{show:!1,scale:1},react:{opacityDefault:.3,opacityOnHover:.3,opacity:.95},dialog:{enable:!0,hitokoto:!0},log:!1})</script></body></html>