<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>处理少标签或零标签情形 | ihadu</title><meta name="author" content="ihadu"><meta name="copyright" content="ihadu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="第9章  处理少标签或零标签情形 有一个问题在每个数据科学家的脑海中根深蒂固，通常是他们在新项目开始时问的第一件事：是否有任何标注的数据？更多的时候，答案是 &quot;没有 &quot;或 “有一点”，然后客户的期望是，你的团队花哨的机器学习模型应该仍然表现良好。由于在非常小的数据集上训练模型通常不会产生好的结果，一个明显的解决方案是标注更多的数据。然而，这需要时间，而且可能非常昂贵，特别是如果"><meta property="og:type" content="article"><meta property="og:title" content="处理少标签或零标签情形"><meta property="og:url" content="https://nivbi.com/posts/3359ec3c.html"><meta property="og:site_name" content="ihadu"><meta property="og:description" content="第9章  处理少标签或零标签情形 有一个问题在每个数据科学家的脑海中根深蒂固，通常是他们在新项目开始时问的第一件事：是否有任何标注的数据？更多的时候，答案是 &quot;没有 &quot;或 “有一点”，然后客户的期望是，你的团队花哨的机器学习模型应该仍然表现良好。由于在非常小的数据集上训练模型通常不会产生好的结果，一个明显的解决方案是标注更多的数据。然而，这需要时间，而且可能非常昂贵，特别是如果"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-rr7ldm_1920x1080.webp"><meta property="article:published_time" content="2024-01-22T07:34:26.000Z"><meta property="article:modified_time" content="2024-03-15T02:47:04.441Z"><meta property="article:author" content="ihadu"><meta property="article:tag" content="blog"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-rr7ldm_1920x1080.webp"><link rel="shortcut icon" href="/images/dog.png"><link rel="canonical" href="https://nivbi.com/posts/3359ec3c.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="google-site-verification" content="h-TVRSWVCnbA8LDyXZkYIrIJ4pr1OUAgVfR5h2H4Eok"><meta name="baidu-site-verification" content="codeva-V4Yhtugi3b"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/node-snackbar/0.1.16/snackbar.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.19/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?546017bccd857b94cdaba2806f36802a",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!1,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:{chs_to_cht:"你已切换为繁体",cht_to_chs:"你已切换为简体",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#49b1f5",bgDark:"#1f1f1f",position:"bottom-left"},source:{justifiedGallery:{js:"https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.js",css:"https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.css"}},isPhotoFigcaption:!1,islazyload:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"处理少标签或零标签情形",isPost:!0,isHome:!1,isHighlightShrink:!0,isToc:!0,postUpdate:"2024-03-15 10:47:04"}</script><noscript><style type="text/css">#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,a){0!==a&&(a=864e5*a,t={value:t,expiry:(new Date).getTime()+a},localStorage.setItem(e,JSON.stringify(t)))},get:function(e){var t=localStorage.getItem(e);if(t){t=JSON.parse(t);if(!((new Date).getTime()>t.expiry))return t.value;localStorage.removeItem(e)}}},e.getScript=o=>new Promise((t,e)=>{const a=document.createElement("script");a.src=o,a.async=!0,a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.getCSS=(o,n=!1)=>new Promise((t,e)=>{const a=document.createElement("link");a.rel="stylesheet",a.href=o,n&&(a.id=n),a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};e=saveToLocal.get("theme"),"dark"===e?activateDarkMode():"light"===e&&activateLightMode(),e=saveToLocal.get("aside-status");void 0!==e&&("hide"===e?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><meta name="generator" content="Hexo 6.0.0"></head><body><script>window.paceOptions={restartOnPushState:!1},document.addEventListener("pjax:send",()=>{Pace.restart()})</script><link rel="stylesheet" href="/assets/css/flash.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://oss.kiscloud.net/image/user/anime/Chainsaw/qiu.webp" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">247</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">32</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">36</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/nav/"><i class="fa-fw fa fa-paper-plane"></i><span> 导航</span></a></div><div class="menus_item"><a class="site-page" href="/resume/"><i class="fa-fw fa fa-archive"></i><span> 简历</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-book"></i><span> 教程</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/python/"><i class="fa-fw fa-brands fa-python"></i><span> python</span></a></li><li><a class="site-page child" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"><i class="fa-fw fa-solid fa-database"></i><span> 大数据</span></a></li><li><a class="site-page child" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><i class="fa-fw fa-solid fa-desktop"></i><span> 机器学习</span></a></li><li><a class="site-page child" href="/DeepLearning/"><i class="fa-fw fas fa-video"></i><span> 深度学习</span></a></li><li><a class="site-page child" href="/categories/Transformers/"><i class="fa-fw fas fa-book-open"></i><span> Transformers</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="ihadu"><img class="site-icon" src="/images/sports.webp"></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/nav/"><i class="fa-fw fa fa-paper-plane"></i><span> 导航</span></a></div><div class="menus_item"><a class="site-page" href="/resume/"><i class="fa-fw fa fa-archive"></i><span> 简历</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-book"></i><span> 教程</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/python/"><i class="fa-fw fa-brands fa-python"></i><span> python</span></a></li><li><a class="site-page child" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"><i class="fa-fw fa-solid fa-database"></i><span> 大数据</span></a></li><li><a class="site-page child" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><i class="fa-fw fa-solid fa-desktop"></i><span> 机器学习</span></a></li><li><a class="site-page child" href="/DeepLearning/"><i class="fa-fw fas fa-video"></i><span> 深度学习</span></a></li><li><a class="site-page child" href="/categories/Transformers/"><i class="fa-fw fas fa-book-open"></i><span> Transformers</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">处理少标签或零标签情形</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-22T07:34:26.000Z" title="发表于 2024-01-22 15:34:26">2024-01-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-15T02:47:04.441Z" title="更新于 2024-03-15 10:47:04">2024-03-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Transformers/">Transformers</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">19.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>68分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="处理少标签或零标签情形"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/3359ec3c.html#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/posts/3359ec3c.html" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div><article class="post-content" id="article-container"><h1>第9章 处理少标签或零标签情形</h1><p>有一个问题在每个数据科学家的脑海中根深蒂固，通常是他们在新项目开始时问的第一件事：是否有任何标注的数据？更多的时候，答案是 &quot;没有 &quot;或 “有一点”，然后客户的期望是，你的团队花哨的机器学习模型应该仍然表现良好。由于在非常小的数据集上训练模型通常不会产生好的结果，一个明显的解决方案是标注更多的数据。然而，这需要时间，而且可能非常昂贵，特别是如果每个标注都需要领域的专业知识来验证。</p><p>幸运的是，有几种方法很适合处理很少甚至没有标签的情况！你可能已经熟悉了一些方法。你可能已经熟悉了其中的一些方法，比如零样本学习或少样本学习，GPT-3仅用几十个例子就能完成各种不同的任务，这一点令人印象深刻。</p><p>一般来说，表现最好的方法将取决于任务、可用的数据量以及这些数据中被标注的部分。图9-1所示的决策树可以帮助指导我们完成挑选最合适的方法的过程。</p><p><img src="/images/transformers/chapter9/image-20220215072355730.png" alt="image-20220215072355730"></p><p>让我们一步步走过这个决策树：</p><p>1.你有标记的数据吗？</p><p>​ 在哪种方法最有效的问题上，即使是少数几个有标签的样本也会产生差异。如果你根本没有标记的数据，你可以从零点学习方法开始，这通常会设定一个强大的基线来工作。</p><ol start="2"><li class="lvl-3"><p>有多少标签？</p><p>如果有标记的数据，决定性的因素是有多少。如果你有大量的训练数据可用，你可以使用第2章中讨论的标准微调方法。</p></li><li class="lvl-4"><p>你有未标注的数据吗？</p></li></ol><p>​ 如果你只有少量的标记样本，那么如果你能获得大量的无标记数据，就会有极大的帮助。如果你能获得未标记的数据，你可以在训练分类器之前用它来微调领域的语言模型，或者你可以使用更复杂的方法，如无监督数据增强（UDA）或不确定性感知自我训练（UST）。如果你没有任何未标记的数据可用，你就不能选择注释更多的数据。在这种情况下，你可以使用少量学习或使用来自预训练语言模型的嵌入，用最近的邻居搜索来进行查找。</p><p>在这一章中，我们将通过解决许多使用Jira或GitHub等问题跟踪器来帮助用户的支持团队所面临的一个常见问题来完成这个决策树：根据问题的描述给问题加上元数据的标签。这些标签可以定义问题的类型，引起问题的产品，或者哪个团队负责处理报告的问题。自动化这一过程可以对生产力产生很大的影响，使支持团队能够专注于帮助他们的用户。作为一个运行的例子，我们将使用与一个流行的开源项目相关的GitHub问题, Transforerms! 现在让我们来看看这些问题中包含哪些信息，如何构建任务，以及如何获得数据。</p><p><strong>注意事项</strong></p><p>本章介绍的方法在文本分类方面效果很好，但在处理命名实体识别、问题回答或总结等更复杂的任务时，可能需要其他技术，如数据扩充。</p><h2 id="构建一个GitHub-Issue标注程序">构建一个GitHub Issue标注程序</h2><p>如果你导航到Transformers资源库的Issues标签，你会发现像图9-2所示的问题，其中包含一个标题、一个描述和一组标签或描述该问题的标签。这表明有一种自然的方式来构建监督学习任务：给定一个问题的标题和描述，预测一个或多个标签。由于每个问题可以被分配不同数量的标签，这意味着我们正在处理一个多标签文本分类问题。这通常比我们在第二章中遇到的多类问题更具挑战性，在第2章中，每条推文只被分配给一种情绪。</p><p><img src="/images/transformers/chapter9/image-20220215072824787.png" alt="image-20220215072824787"></p><p>现在我们已经看到了GitHub问题的模样，让我们看看如何下载它们来创建我们的数据集。</p><h3 id="获取数据">获取数据</h3><p>为了抓取仓库中的所有问题，我们将使用GitHub REST API来轮询问题端点。这个端点会返回一个 JSON 对象的列表，每个对象都包含大量关于当前问题的字段，包括其状态（打开或关闭）、谁打开了这个问题，以及我们在图 9-2 中看到的标题、正文和标签。</p><p>由于获取所有问题需要一段时间，我们在本书的GitHub仓库中包含了一个github-issuestransformers.jsonl文件，以及一个fetch_issues()函数，你可以用来自己下载它们。</p><p><strong>注意事项</strong></p><p>GitHub的REST API将pull请求视为问题，因此我们的数据集包含了两者的混合。为了保持简单，我们将为这两种类型的问题开发分类器，尽管在实践中你可能会考虑建立两个独立的分类器，以便对模型的性能进行更精细的控制。</p><p>现在我们知道了如何抓取数据，让我们来看看如何清理它。</p><h3 id="准备数据">准备数据</h3><p>一旦我们下载了所有的问题，我们就可以用Pandas加载它们:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd </span><br><span class="line">dataset_url = &quot;https://git.io/nlp-with-transformers&quot; </span><br><span class="line">df_issues = pd.read_json(dataset_url, lines=True) </span><br><span class="line">print(f&quot;DataFrame shape: &#123;df_issues.shape&#125;&quot;) </span><br><span class="line"></span><br><span class="line">DataFrame shape: (9930, 26)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们的数据集中有近一万个问题，通过查看单行，我们可以看到从GitHub API中检索的信息包含许多字段，如URL、ID、日期、用户、标题、正文以及标签:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cols = [&quot;url&quot;, &quot;id&quot;, &quot;title&quot;, &quot;user&quot;, &quot;labels&quot;, &quot;state&quot;, &quot;created_at&quot;, &quot;body&quot;] </span><br><span class="line"></span><br><span class="line">df_issues.loc[2, cols].to_frame()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215073115724.png" alt="image-20220215073115724"></p><p>标签列是我们感兴趣的东西，每一行都包含一个JSON对象的列表，其中有关于每个标签的元数据:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[ </span><br><span class="line">    &#123; </span><br><span class="line">        &quot;id&quot;:2659267025, &quot;node_id&quot;:&quot;MDU6TGFiZWwyNjU5MjY3MDI1&quot;, </span><br><span class="line">        &quot;url&quot;:&quot;https://api.github.com/repos/huggingface...&quot;, </span><br><span class="line">        &quot;name&quot;:&quot;DeepSpeed&quot;, </span><br><span class="line">        &quot;color&quot;:&quot;4D34F7&quot;, </span><br><span class="line">        &quot;default&quot;:false, </span><br><span class="line">        &quot;description&quot;:&quot;&quot; </span><br><span class="line">    &#125; </span><br><span class="line">]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>为了我们的目的，我们只对每个标签对象的名称字段感兴趣，所以让我们用标签名称覆盖标签列:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df_issues[&quot;labels&quot;] = (df_issues[&quot;labels&quot;] .apply(lambda x: [meta[&quot;name&quot;] for meta in x])) </span><br><span class="line">df_issues[[&quot;labels&quot;]].head()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215073235513.png" alt="image-20220215073235513"></p><p>现在，标签栏中的每一行都是GitHub的标签列表，所以我们可以计算每一行的长度，以找出每个问题的标签数量:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df_issues[&quot;labels&quot;].apply(lambda x : len(x)).value_counts().to_frame().T</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215073257537.png" alt="image-20220215073257537"></p><p>这表明，大多数问题都有零个或一个标签，而有一个以上标签的则少得多。接下来，让我们看看数据集中最频繁出现的前10个标签。在Pandas中，我们可以通过 &quot;explode (展开)&quot;标签列来实现，这样列表中的每个标签都会成为一行，然后简单地计算每个标签的出现次数:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df_counts = df_issues[&quot;labels&quot;].explode().value_counts() </span><br><span class="line">print(f&quot;Number of labels: &#123;len(df_counts)&#125;&quot;) </span><br><span class="line"># Display the top-8 label categories df_counts.to_frame().head(8).T</span><br><span class="line"></span><br><span class="line">Number of labels: 65</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215073332797.png" alt="image-20220215073332797"></p><p>我们可以看到，数据集中有65个独特的标签，这些类别非常不平衡，其中wontfix和model card是最常见的标签。为了使分类问题更容易解决，我们将专注于为标签的一个子集建立一个标签器。例如，一些标签，如Good First Issue或Help Wanted，有可能很难从问题的描述中预测出来，而另一些标签，如model card，可以用一个简单的规则来分类，即检测Hugging Face Hub上何时添加了model card。</p><p>下面的代码对数据集进行过滤，以获得我们要处理的标签子集，同时对名称进行标准化处理，使其更容易阅读：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">label_map = &#123;&quot;Core: Tokenization&quot;: &quot;tokenization&quot;, &quot;New model&quot;: &quot;new model&quot;, &quot;Core: Modeling&quot;: &quot;model training&quot;, &quot;Usage&quot;: &quot;usage&quot;, &quot;Core: Pipeline&quot;: &quot;pipeline&quot;, &quot;TensorFlow&quot;: &quot;tensorflow or tf&quot;, &quot;PyTorch&quot;: &quot;pytorch&quot;, &quot;Examples&quot;: &quot;examples&quot;, &quot;Documentation&quot;: &quot;documentation&quot;&#125; </span><br><span class="line"></span><br><span class="line">def filter_labels(x): </span><br><span class="line">	return [label_map[label] for label in x if label in label_map] </span><br><span class="line"></span><br><span class="line">df_issues[&quot;labels&quot;] = df_issues[&quot;labels&quot;].apply(filter_labels) </span><br><span class="line">all_labels = list(label_map.values())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在我们来看看新标签的分布情况：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df_counts = df_issues[&quot;labels&quot;].explode().value_counts() </span><br><span class="line">df_counts.to_frame().T</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215073556232.png" alt="image-20220215073556232"></p><p>在本章的后面，我们会发现将未标记的问题作为一个单独的训练分割来处理是很有用的，所以我们创建一个新的列，表示该问题是否是未标记的:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df_issues[&quot;split&quot;] = &quot;unlabeled&quot; </span><br><span class="line">mask = df_issues[&quot;labels&quot;].apply(lambda x: len(x)) &gt; 0 </span><br><span class="line">df_issues.loc[mask, &quot;split&quot;] = &quot;labeled&quot;</span><br><span class="line">df_issues[&quot;split&quot;].value_counts().to_frame()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215073654838.png" alt="image-20220215073654838"></p><p>现在让我们来看一个例子:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for column in [&quot;title&quot;, &quot;body&quot;, &quot;labels&quot;]: </span><br><span class="line">	print(f&quot;&#123;column&#125;: &#123;df_issues[column].iloc[26][:500]&#125;\n&quot;) </span><br><span class="line"></span><br><span class="line">title: Add new CANINE model </span><br><span class="line">body: # New model addition </span><br><span class="line">## Model description </span><br><span class="line">Google recently proposed a new **C**haracter **A**rchitecture with **N**o tokenization **I**n **N**eural **E**ncoders architecture (CANINE). Not only the title is exciting: </span><br><span class="line"></span><br><span class="line">Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly-used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually en </span><br><span class="line"></span><br><span class="line">labels: [&#x27;new model&#x27;]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这个例子中，我们提出了一个新的模型架构，所以新模型标签是有意义的。我们还可以看到，标题包含了对我们的分类器有用的信息，所以让我们把它与正文字段中的问题描述连接起来:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df_issues[&quot;text&quot;] = (df_issues .apply(lambda x: x[&quot;title&quot;] + &quot;\n\n&quot; + x[&quot;body&quot;], axis=1))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在我们看其余的数据之前，让我们检查一下数据中是否有重复的部分，并通过drop_duplicates()方法将它们删除:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">len_before = len(df_issues) </span><br><span class="line">df_issues = df_issues.drop_duplicates(subset=&quot;text&quot;) </span><br><span class="line">print(f&quot;Removed &#123;(len_before-len(df_issues))/len_before:.2%&#125; duplicates.&quot;)</span><br><span class="line"></span><br><span class="line">Removed 1.88% duplicates.</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们可以看到，在我们的数据集中有一些重复的问题，但它们只占很小的比例。正如我们在其他章节中所做的那样，快速看一下我们文本中的字数也是一个好主意，看看当我们截断到每个模型的上下文大小时，我们是否会失去很多信息:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np import matplotlib.pyplot as plt </span><br><span class="line"></span><br><span class="line">(df_issues[&quot;text&quot;].str.split().apply(len) .hist(bins=np.linspace(0, 500, 50), grid=False, edgecolor=&quot;C0&quot;)) </span><br><span class="line">plt.title(&quot;Words per issue&quot;) </span><br><span class="line">plt.xlabel(&quot;Number of words&quot;) </span><br><span class="line">plt.ylabel(&quot;Number of issues&quot;) </span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215073927742.png" alt="image-20220215073927742"></p><p>该分布具有许多文本数据集的长尾特征。大多数文本都相当短，但也有超过500字的问题。有一些很长的问题是很常见的，特别是当错误信息和代码片段一起发布时。鉴于大多数转化器模型的上下文大小为512个标记或更大，截断少数长问题不可能影响整体性能。现在我们已经探索并清理了我们的数据集，最后要做的是定义我们的训练集和验证集，以便为我们的分类器设定基准。让我们来看看如何做到这一点。</p><h3 id="创建训练集">创建训练集</h3><p>对于多标签问题来说，创建训练集和验证集是比较麻烦的，因为没有保证所有标签的平衡。然而，它可以被近似，我们可以使用Scikit-multilearn库，它是专门为此目的而设置的。我们需要做的第一件事是将我们的标签集，如pytorch和标记化，转换成模型可以处理的格式。在这里，我们可以使用Scikit-learn的MultiLabel Binarizer类，它接收一个标签名称的列表，并创建一个矢量，其中零代表没有的标签，一代表现在的标签。我们可以通过在all_labels上拟合MultiLabelBinarizer来测试，学习标签名称到ID的映射，如下所示:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.preprocessing import MultiLabelBinarizer </span><br><span class="line">mlb = MultiLabelBinarizer() </span><br><span class="line">mlb.fit([all_labels]) </span><br><span class="line">mlb.transform([[&quot;tokenization&quot;, &quot;new model&quot;], [&quot;pytorch&quot;]])</span><br><span class="line"></span><br><span class="line">array([[0, 0, 0, 1, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这个简单的例子中，我们可以看到第一行有两个对应于标记化和新模型标签的，而第二行只有一个pytorch的命中。</p><p>为了创建分割，我们可以使用Scikit-multilearn的iterative_train_test_split()函数，它可以迭代地创建训练/测试分割以实现平衡标签。我们把它包装成一个可以应用于DataFrames的函数。由于该函数期望有一个二维特征矩阵，我们需要在进行分割之前给可能的索引增加一个维度。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from skmultilearn.model_selection import iterative_train_test_split </span><br><span class="line">def balanced_split(df, test_size=0.5): </span><br><span class="line">	ind = np.expand_dims(np.arange(len(df)), axis=1) </span><br><span class="line">	labels = mlb.transform(df[&quot;labels&quot;]) </span><br><span class="line">	ind_train, _, ind_test, _ = iterative_train_test_split(ind, labels, test_size)</span><br><span class="line">    return df.iloc[ind_train[:, 0]], df.iloc[ind_test[:,0]]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>有了balanced_split()函数，我们可以把数据分成有监督和无监督的数据集，然后为有监督的部分创建平衡的训练、验证和测试集:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split </span><br><span class="line">df_clean = df_issues[[&quot;text&quot;, &quot;labels&quot;, &quot;split&quot;]].reset_index(drop=True).copy() </span><br><span class="line">df_unsup = df_clean.loc[df_clean[&quot;split&quot;] == &quot;unlabeled&quot;, [&quot;text&quot;, &quot;labels&quot;]] </span><br><span class="line">df_sup = df_clean.loc[df_clean[&quot;split&quot;] == &quot;labeled&quot;, [&quot;text&quot;, &quot;labels&quot;]] </span><br><span class="line"></span><br><span class="line">np.random.seed(0) </span><br><span class="line">df_train, df_tmp = balanced_split(df_sup, test_size=0.5) </span><br><span class="line">df_valid, df_test = balanced_split(df_tmp, test_size=0.5)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>最后，让我们创建一个包含所有分片的DatasetDict，这样我们就可以轻松地对数据集进行标记，并与Trainer整合。在这里，我们将使用漂亮的from_pandas()方法，直接从相应的Pandas DataFrame中加载每个分片:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from datasets import Dataset, DatasetDict </span><br><span class="line"></span><br><span class="line">ds = DatasetDict(&#123; &quot;train&quot;: Dataset.from_pandas(df_train.reset_index(drop=True)), &quot;valid&quot;: Dataset.from_pandas(df_valid.reset_index(drop=True)), &quot;test&quot;: Dataset.from_pandas(df_test.reset_index(drop=True)), &quot;unsup&quot;: Dataset.from_pandas(df_unsup.reset_index(drop=True))&#125;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这看起来不错，所以最后要做的是创建一些训练片，以便我们可以评估每个分类器的性能与训练集大小的关系。</p><h3 id="创建训练切片">创建训练切片</h3><p>该数据集具有我们在本章要研究的两个特点：稀疏的标记数据和多标签分类。训练集只有220个例子可供训练，即使是迁移学习，这也是一个挑战。为了深入研究本章中每种方法在很少的标签数据下的表现，我们还将创建样本更少的训练数据片。然后，我们可以将样本的数量与性能作对比，研究各种制度。我们将从每个标签只有8个样本开始，然后使用iterative_train_test_split()函数建立切片，直到覆盖全部的训练集:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">np.random.seed(0) </span><br><span class="line">all_indices = np.expand_dims(list(range(len(ds[&quot;train&quot;]))), axis=1) </span><br><span class="line">indices_pool = all_indices </span><br><span class="line">labels = mlb.transform(ds[&quot;train&quot;][&quot;labels&quot;]) </span><br><span class="line">train_samples = [8, 16, 32, 64, 128] </span><br><span class="line">train_slices, last_k = [], 0 </span><br><span class="line">for i, k in enumerate(train_samples):</span><br><span class="line">	# Split off samples necessary to fill the gap to the next split size </span><br><span class="line">	indices_pool, labels, new_slice, _ = iterative_train_test_split( indices_pool, labels, (k-last_k)/len(labels)) </span><br><span class="line">	last_k = k </span><br><span class="line">	if i==0: </span><br><span class="line">		train_slices.append(new_slice) </span><br><span class="line">	else: </span><br><span class="line">		train_slices.append(np.concatenate((train_slices[-1], new_slice))) </span><br><span class="line"># Add full dataset as last slice </span><br><span class="line">train_slices.append(all_indices), train_samples.append(len(ds[&quot;train&quot;])) </span><br><span class="line">train_slices = [np.squeeze(train_slice) for train_slice in train_slices]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>请注意，这种迭代方法只是将样本近似地分割成所需的大小，因为在给定的分割大小下并不总是能够找到平衡的分割:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">print(&quot;Target split sizes:&quot;) </span><br><span class="line">print(train_samples) </span><br><span class="line">print(&quot;Actual split sizes:&quot;) </span><br><span class="line">print([len(x) for x in train_slices]) </span><br><span class="line"></span><br><span class="line">Target split sizes: [8, 16, 32, 64, 128, 223] Actual split sizes: [10, 19, 36, 68, 134, 223]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们将使用指定的分割大小作为以下图表的标签。很好，我们终于准备好了我们的数据集的训练分片–接下来让我们看看如何训练一个强大的基线模型!</p><h2 id="实现朴素贝叶斯基线">实现朴素贝叶斯基线</h2><p>每当你开始一个新的NLP项目时，实施一套强有力的基线总是一个好主意。这有两个主要原因:</p><ol><li class="lvl-3"><p>一个基于正则表达式、手工制作的规则或非常简单的模型的基线可能已经能很好地解决这个问题。在这些情况下，没有理由拿出像Transformers这样的大炮，因为在生产环境中，Transformers的部署和维护通常比较复杂。</p></li><li class="lvl-3"><p>当你探索更复杂的模型时，基线提供快速检查。例如，假设你训练BERT-large，并在验证集上获得80%的准确率。你可能会把它作为一个硬数据集来处理，然后就此打住。但是，如果你知道像逻辑回归这样的简单分类器可以得到95%的准确率呢？这就会引起你的怀疑，并促使你去调试你的模型。</p></li></ol><p>因此，让我们通过训练一个基线模型来开始我们的分析。对于文本分类来说，一个很好的基线是Naive Bayes分类器，因为它非常简单，可以快速训练，并且对输入的扰动相当稳健。Naive Bayes的Scikit-learn实现并不支持开箱即用的多标签分类，但幸运的是，我们可以再次使用Scikitmultilearn库，将这个问题作为一个单对单的分类任务，我们为L标签训练L个二进制分类器。首先，让我们使用一个多标签二值化器，在我们的训练集中创建一个新的标签_ids列。我们可以使用map()函数来一次性搞定所有的处理:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ef prepare_labels(batch): </span><br><span class="line">	batch[&quot;label_ids&quot;] = mlb.transform(batch[&quot;labels&quot;]) </span><br><span class="line">	return batch </span><br><span class="line">ds = ds.map(prepare_labels, batched=True)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>为了衡量我们的分类器的性能，我们将使用微观和宏观的F-scores，其中前者跟踪频繁标签的性能，后者跟踪不考虑频率的所有标签。由于我们将在不同大小的训练片段中评估每个模型，让我们创建一个defaultdict，用一个列表来存储每个片段的得分:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from collections import defaultdict</span><br><span class="line"></span><br><span class="line">macro_scores, micro_scores = defaultdict(list), defaultdict(list)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在我们终于准备好训练我们的基线了! 下面是训练模型的代码，并在不断增加的训练集规模中评估我们的分类器:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.naive_bayes import MultinomialNB </span><br><span class="line">from sklearn.metrics import classification_report </span><br><span class="line">from skmultilearn.problem_transform import BinaryRelevance </span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer </span><br><span class="line">for train_slice in train_slices: </span><br><span class="line">	# Get training slice and test data </span><br><span class="line">	ds_train_sample = ds[&quot;train&quot;].select(train_slice) </span><br><span class="line">	y_train = np.array(ds_train_sample[&quot;label_ids&quot;]) </span><br><span class="line">	y_test = np.array(ds[&quot;test&quot;][&quot;label_ids&quot;]) </span><br><span class="line">	# Use a simple count vectorizer to encode our texts as token counts </span><br><span class="line">	count_vect = CountVectorizer() </span><br><span class="line">	X_train_counts = count_vect.fit_transform(ds_train_sample[&quot;text&quot;]) </span><br><span class="line">	X_test_counts = count_vect.transform(ds[&quot;test&quot;][&quot;text&quot;]) </span><br><span class="line">	# Create and train our model! </span><br><span class="line">	classifier = BinaryRelevance(classifier=MultinomialNB()) classifier.fit(X_train_counts, y_train) </span><br><span class="line">	# Generate predictions and evaluate </span><br><span class="line">	y_pred_test = classifier.predict(X_test_counts) </span><br><span class="line">	clf_report = classification_report( y_test, y_pred_test, target_names=mlb.classes_, zero_division=0, output_dict=True) </span><br><span class="line">	# Store metrics </span><br><span class="line">	macro_scores[&quot;Naive Bayes&quot;].append(clf_report[&quot;macro avg&quot;][&quot;f1-score&quot;]) </span><br><span class="line">	micro_scores[&quot;Naive Bayes&quot;].append(clf_report[&quot;micro avg&quot;][&quot;f1-score&quot;])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这段代码中，有很多事情要做，所以让我们来解开它。首先，我们得到训练片并对标签进行编码。然后，我们使用计数向量器对文本进行编码，简单地创建一个词汇量大小的向量，其中每个条目对应于文本中某一标记出现的频率。这被称为 &quot;词包 &quot;方法，因为所有关于词的顺序的信息都会丢失。然后，我们训练分类器，并在测试集上使用预测结果，通过分类报告获得微观和宏观的F-scores。</p><p>通过下面的辅助函数，我们可以绘制这个实验的结果:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt </span><br><span class="line">def plot_metrics(micro_scores, macro_scores, sample_sizes, current_model): </span><br><span class="line">	fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4), sharey=True) </span><br><span class="line">	for run in micro_scores.keys(): </span><br><span class="line">		if run == current_model: </span><br><span class="line">			ax0.plot(sample_sizes, micro_scores[run], label=run, linewidth=2) </span><br><span class="line">			ax1.plot(sample_sizes, macro_scores[run], label=run, linewidth=2) </span><br><span class="line">		else: </span><br><span class="line">			ax0.plot(sample_sizes, micro_scores[run], label=run, linestyle=&quot;dashed&quot;) </span><br><span class="line">			ax1.plot(sample_sizes, macro_scores[run], label=run, linestyle=&quot;dashed&quot;) </span><br><span class="line">	ax0.set_title(&quot;Micro F1 scores&quot;) </span><br><span class="line">	ax1.set_title(&quot;Macro F1 scores&quot;) </span><br><span class="line">	ax0.set_ylabel(&quot;Test set F1 score&quot;) </span><br><span class="line">	ax0.legend(loc=&quot;lower right&quot;) </span><br><span class="line">	for ax in [ax0, ax1]: </span><br><span class="line">		ax.set_xlabel(&quot;Number of training samples&quot;) </span><br><span class="line">		ax.set_xscale(&quot;log&quot;) </span><br><span class="line">		ax.set_xticks(sample_sizes) </span><br><span class="line">		ax.set_xticklabels(sample_sizes) </span><br><span class="line">		ax.minorticks_off() </span><br><span class="line">	plt.tight_layout() </span><br><span class="line">	plt.show() </span><br><span class="line">plot_metrics(micro_scores, macro_scores, train_samples, &quot;Naive Bayes&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215075403491.png" alt="image-20220215075403491"></p><p>请注意，我们将样本的数量绘制在对数刻度上。从图中我们可以看到，随着训练样本数量的增加，微观和宏观的F-scores都有所提高。由于可以训练的样本太少，结果也略显嘈杂，因为每个片断都可能有不同的类别分布。尽管如此，这里最重要的是趋势，所以现在让我们看看这些结果与基于Transformers的方法相比如何吧!</p><h2 id="零样本学习">零样本学习</h2><p>我们要考虑的第一个技术是零样本分类，它适用于你完全没有标签数据的环境。这种情况在工业界出奇地普遍，可能是因为没有带标签的历史数据，或者因为获取数据的标签很困难。在这一节中，我们会有一点欺骗，因为我们仍然会使用测试数据来衡量性能，但我们不会使用任何数据来训练模型（否则与以下方法的比较会很困难）。</p><p>零样本分类的目标是利用预先训练好的模型，而不需要对你的特定任务语料进行任何额外的微调。为了更好地了解这一点，请回顾一下，像BERT这样的语言模型是经过预训练的，以预测成千上万的书籍和大型维基百科转储的文本中的掩蔽标记。为了成功地预测一个缺失的标记，该模型需要意识到上下文中的主题。我们可以尝试通过提供这样一个句子来欺骗模型为我们分类文件：</p><p>“This section was about the topic [MASK].”</p><p>然后，该模型应该对该文件的主题给出一个合理的建议，因为这是一个在数据集中自然出现的文本。</p><p>让我们用下面这个玩具问题来进一步说明：假设你有两个孩子，其中一个喜欢有汽车的电影，而另一个更喜欢有动物的电影。不幸的是，他们已经看过所有你知道的电影，所以你想建立一个函数，告诉你新电影是关于什么主题的。自然而然地，你转向Transformers来完成这项任务。首先要尝试的是将BERT-base加载到fill-mask流水线中，该流水线使用掩码语言模型来预测掩码标记的内容：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from transformers import pipeline </span><br><span class="line">pipe = pipeline(&quot;fill-mask&quot;, model=&quot;bert-base-uncased&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>接下来，让我们构建一个小的电影描述，并在其中添加一个带有遮蔽词的提示。提示的目的是引导模型，帮助我们进行分类。填充-屏蔽流水线会返回最可能的标记来填充被屏蔽的地方：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">movie_desc = &quot;The main characters of the movie madacascar \ are a lion, a zebra, a giraffe, and a hippo. &quot; </span><br><span class="line">prompt = &quot;The movie is about [MASK].&quot; </span><br><span class="line">output = pipe(movie_desc + prompt) </span><br><span class="line">for element in output: </span><br><span class="line">	print(f&quot;Token &#123;element[&#x27;token_str&#x27;]&#125;:\t&#123;element[&#x27;score&#x27;]:.3f&#125;%&quot;) </span><br><span class="line">	</span><br><span class="line">Token animals: 0.103% </span><br><span class="line">Token lions: 0.066% </span><br><span class="line">Token birds: 0.025% </span><br><span class="line">Token love: 0.015% </span><br><span class="line">Token hunting: 0.013%</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>显然，该模型只预测了与动物有关的标记。我们也可以把这个问题转过来，我们可以向流水线查询几个给定标记的概率，而不是获得最可能的标记。对于这个任务，我们可能会选择汽车和动物，所以我们可以把它们作为目标传递给流水线：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">output = pipe(movie_desc + prompt, targets=[&quot;animals&quot;, &quot;cars&quot;]) </span><br><span class="line">for element in output: </span><br><span class="line">	print(f&quot;Token &#123;element[&#x27;token_str&#x27;]&#125;:\t&#123;element[&#x27;score&#x27;]:.3f&#125;%&quot;) </span><br><span class="line">	</span><br><span class="line">Token animals: 0.103% </span><br><span class="line">Token cars: 0.001%</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>不出所料，对象征性的汽车的预测概率要比对动物的预测概率小得多。让我们看看这是否也适用于更接近于汽车的描述：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">movie_desc = &quot;In the movie transformers aliens \ can morph into a wide range of vehicles.&quot; </span><br><span class="line">output = pipe(movie_desc + prompt, targets=[&quot;animals&quot;, &quot;cars&quot;]) </span><br><span class="line">for element in output: </span><br><span class="line">	print(f&quot;Token &#123;element[&#x27;token_str&#x27;]&#125;:\t&#123;element[&#x27;score&#x27;]:.3f&#125;%&quot;) </span><br><span class="line"></span><br><span class="line">Token cars: 0.139% </span><br><span class="line">Token animals: 0.006%</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>它是这样的! 这只是一个简单的例子，如果我们想确保它运行良好，我们应该对它进行彻底的测试，但它说明了本章所讨论的许多方法的关键思想：找到一种方法，使一个预训练的模型适应另一个任务，而不需要训练它。在这种情况下，我们设置了一个带有掩码的提示，这样我们就可以直接使用掩码的语言模型进行分类。让我们来看看我们是否可以通过调整一个已经在一个更接近文本分类的任务上进行微调的模型来做得更好：自然语言推理（NLI）。</p><p>使用屏蔽语言模型进行分类是一个很好的技巧，但我们还可以通过使用一个在更接近分类的任务上训练出来的模型来做得更好。有一个整洁的代理任务，叫做文本连带关系，符合这个条件。在文本连带关系中，模型需要确定两个文本段落是否有可能相互跟随或相互矛盾。模型通常被训练来检测多流派NLI语料库（MNLI）或跨语言NLI语料库（XNLI）等数据集的衔接和矛盾。</p><p>这些数据集中的每个样本都由三部分组成：一个前提、一个假设和一个标签，标签可以是连带、中性或矛盾的。当假设文本在前提下必然为真时，就会被分配到entailment标签。当假设在前提下必然是假的或不合适时，就会使用矛盾标签。</p><p>如果这两种情况都不适用，那么就会分配中性标签。每种情况的例子见表9-1。</p><p><img src="/images/transformers/chapter9/image-20220215080152836.png" alt="image-20220215080152836"></p><p>现在，事实证明，我们可以劫持一个在MNLI数据集上训练的模型来建立一个分类器，而根本不需要任何标签！关键的想法是将我们希望分类的文本视为前提。关键的想法是将我们希望分类的文本作为前提，然后将假设表述为：</p><p>“This example is about {label}.”</p><p>其中我们插入了标签的类别名称。然后，连带分数告诉我们，该前提与该主题有关的可能性有多大，我们可以对任何数量的类依次运行这个方法。这种方法的缺点是，我们需要对每个类别执行一个前向传递，这使得它的效率低于标准分类器。另一个稍显棘手的方面是，标签名称的选择会对准确性产生很大的影响，选择具有语义的标签通常是最好的方法。例如，如果标签只是简单的第1类，那么模型就没有提示这可能意味着什么，以及这是否构成了矛盾或包含关系。</p><p>Transformers有一个内置的MNLI模型用于零样本分类。我们可以通过一个流水线来初始化它，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from transformers import pipeline </span><br><span class="line">pipe = pipeline(&quot;zero-shot-classification&quot;, device=0)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>device=0的设置可以确保模型在GPU上运行，而不是默认的CPU，以加快推理速度。要对一个文本进行分类，我们只需要把它和标签名称一起传递给流水线。此外，我们可以设置multi_label=True，以确保返回所有的分数，而不是只返回单标签分类的最大分数：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sample = ds[&quot;train&quot;][0] </span><br><span class="line">print(f&quot;Labels: &#123;sample[&#x27;labels&#x27;]&#125;&quot;) </span><br><span class="line">output = pipe(sample[&quot;text&quot;], all_labels, multi_label=True) </span><br><span class="line">print(output[&quot;sequence&quot;][:400]) </span><br><span class="line">print(&quot;\nPredictions:&quot;) </span><br><span class="line">for label, score in zip(output[&quot;labels&quot;], output[&quot;scores&quot;]): </span><br><span class="line">	print(f&quot;&#123;label&#125;, &#123;score:.2f&#125;&quot;) </span><br><span class="line"></span><br><span class="line">Labels: [&#x27;new model&#x27;] </span><br><span class="line">Add new CANINE model </span><br><span class="line"># New model addition </span><br><span class="line">## Model description </span><br><span class="line"></span><br><span class="line">Google recently proposed a new **C**haracter **A**rchitecture with **N**o tokenization **I**n **N**eural **E**ncoders architecture (CANINE). Not only the title is exciting: </span><br><span class="line"></span><br><span class="line">&gt; Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly-used models still require an explicit tokeni </span><br><span class="line">Predictions: </span><br><span class="line">new model, 0.98 </span><br><span class="line">tensorflow or tf, 0.37 </span><br><span class="line">examples, 0.34 </span><br><span class="line">usage, 0.30 </span><br><span class="line">pytorch, 0.25 </span><br><span class="line">documentation, 0.25 </span><br><span class="line">model training, 0.24 </span><br><span class="line">tokenization, 0.17 </span><br><span class="line">pipeline, 0.16</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>注意事项</strong></p><p>由于我们使用的是子词标记器，我们甚至可以将代码传递给模型 标记化可能不是很有效，因为只有一小部分零样本流水线的预训练数据集是由代码片段组成的，但由于代码也是由很多自然词组成的，这不是一个大问题。另外，代码块可能包含重要的信息，如框架（PyTorch或TensorFlow）。</p><p>我们可以看到，该模型对这个文本是关于一个新模型非常有信心，但它对其他标签也产生了相对较高的分数。零点分类的一个重要方面是我们所处的领域。我们在这里处理的文本是非常技术性的，大部分是关于编码的，这使得它们与MNLI数据集中的原始文本分布有很大不同。因此，这对模型来说是一项具有挑战性的任务也就不足为奇了；它对某些领域的效果可能比其他领域好得多，这取决于它们与训练数据的接近程度。</p><p>让我们写一个函数，通过零拍流水线送入一个例子，然后通过运行map()将其扩展到整个验证集：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def zero_shot_pipeline(example): </span><br><span class="line">	output = pipe(example[&quot;text&quot;], all_labels, multi_label=True) </span><br><span class="line">	example[&quot;predicted_labels&quot;] = output[&quot;labels&quot;] </span><br><span class="line">	example[&quot;scores&quot;] = output[&quot;scores&quot;] </span><br><span class="line">	return example </span><br><span class="line">ds_zero_shot = ds[&quot;valid&quot;].map(zero_shot_pipeline)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在我们有了我们的分数，下一步是确定应该给每个例子分配哪一组标签。有几个选项我们可以进行试验：</p><ul class="lvl-0"><li class="lvl-2"><p>定义一个阈值并选择所有高于阈值的标签。</p></li><li class="lvl-2"><p>挑选分数最高的k个标签。</p></li></ul><p>为了帮助我们确定哪种方法最好，让我们写一个get_preds()函数，应用其中一种方法来检索预测结果：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_preds(example, threshold=None, topk=None): </span><br><span class="line">	preds = [] </span><br><span class="line">	if threshold: </span><br><span class="line">		for label, score in zip(example[&quot;predicted_labels&quot;], example[&quot;scores&quot;]): </span><br><span class="line">			if score &gt;= threshold: </span><br><span class="line">				preds.append(label) </span><br><span class="line">	elif topk: </span><br><span class="line">		for i in range(topk):</span><br><span class="line">            preds.append(example[&quot;predicted_labels&quot;][i]) </span><br><span class="line">    else: </span><br><span class="line">         raise ValueError(&quot;Set either `threshold` or `topk`.&quot;) </span><br><span class="line">    return &#123;&quot;pred_label_ids&quot;: list(np.squeeze(mlb.transform([preds])))&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>接下来，让我们写第二个函数，get_clf_report()，它从数据集中返回带有预测标签的Scikit-learn分类报告：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_clf_report(ds): </span><br><span class="line">	y_true = np.array(ds[&quot;label_ids&quot;]) </span><br><span class="line">	y_pred = np.array(ds[&quot;pred_label_ids&quot;]) </span><br><span class="line">	return classification_report( y_true, y_pred, target_names=mlb.classes_, zero_division=0, output_dict=True)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>有了这两个函数，让我们从top-k方法开始，增加k的几个值，然后绘制整个验证集的微观和宏观F分数：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">macros, micros = [], [] </span><br><span class="line">topks = [1, 2, 3, 4] </span><br><span class="line">for topk in topks: </span><br><span class="line">	ds_zero_shot = ds_zero_shot.map(get_preds, batched=False, fn_kwargs=&#123;&#x27;topk&#x27;: topk&#125;) </span><br><span class="line">	clf_report = get_clf_report(ds_zero_shot) </span><br><span class="line">	micros.append(clf_report[&#x27;micro avg&#x27;][&#x27;f1-score&#x27;]) </span><br><span class="line">	macros.append(clf_report[&#x27;macro avg&#x27;][&#x27;f1-score&#x27;]) </span><br><span class="line">plt.plot(topks, micros, label=&#x27;Micro F1&#x27;) </span><br><span class="line">plt.plot(topks, macros, label=&#x27;Macro F1&#x27;) </span><br><span class="line">plt.xlabel(&quot;Top-k&quot;) </span><br><span class="line">plt.ylabel(&quot;F1-score&quot;) </span><br><span class="line">plt.legend(loc=&#x27;best&#x27;) </span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215081026792.png" alt="image-20220215081026792"></p><p>从图中我们可以看出，通过选择每个例子中得分最高的标签（顶部1），可以获得最佳结果。考虑到我们的数据集中的大多数例子只有一个标签，这也许并不令人惊讶。现在让我们来比较一下这与设置阈值的区别，这样我们就有可能预测每个例子有一个以上的标签：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">macros, micros = [], [] </span><br><span class="line">thresholds = np.linspace(0.01, 1, 100) </span><br><span class="line">for threshold in thresholds: </span><br><span class="line">	ds_zero_shot = ds_zero_shot.map(get_preds, fn_kwargs=&#123;&quot;threshold&quot;: threshold&#125;) </span><br><span class="line">	clf_report = get_clf_report(ds_zero_shot)</span><br><span class="line">    micros.append(clf_report[&quot;micro avg&quot;][&quot;f1-score&quot;]) </span><br><span class="line">    macros.append(clf_report[&quot;macro avg&quot;][&quot;f1-score&quot;]) </span><br><span class="line">plt.plot(thresholds, micros, label=&quot;Micro F1&quot;) </span><br><span class="line">plt.plot(thresholds, macros, label=&quot;Macro F1&quot;) </span><br><span class="line">plt.xlabel(&quot;Threshold&quot;) </span><br><span class="line">plt.ylabel(&quot;F1-score&quot;) </span><br><span class="line">plt.legend(loc=&quot;best&quot;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215081127519.png" alt="image-20220215081127519"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">best_t, best_micro = thresholds[np.argmax(micros)], np.max(micros) </span><br><span class="line">print(f&#x27;Best threshold (micro): &#123;best_t&#125; with F1-score &#123;best_micro:.2f&#125;.&#x27;) best_t, best_macro = thresholds[np.argmax(macros)], np.max(macros) </span><br><span class="line">print(f&#x27;Best threshold (micro): &#123;best_t&#125; with F1-score &#123;best_macro:.2f&#125;.&#x27;)</span><br><span class="line"></span><br><span class="line">Best threshold (micro): 0.75 with F1-score 0.46. </span><br><span class="line">Best threshold (micro): 0.72 with F1-score 0.42.</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这种方法的表现比top-1的结果要差一些，但是我们可以在这个图中清楚地看到精度/召回率的权衡。如果我们把阈值设置得太低，那么就会有太多的预测，这导致了低精度。如果我们把阈值设置得太高，那么我们几乎不会做出任何预测，这就产生了低召回率。从图中我们可以看出，0.8左右的阈值是这两者之间的甜蜜点。</p><p>由于top-1方法表现最好，让我们用它来比较零样本分类与测试集上的Naive Bayes：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ds_zero_shot = ds[&#x27;test&#x27;].map(zero_shot_pipeline) </span><br><span class="line">ds_zero_shot = ds_zero_shot.map(get_preds, fn_kwargs=&#123;&#x27;topk&#x27;: 1&#125;)</span><br><span class="line">clf_report = get_clf_report(ds_zero_shot) </span><br><span class="line">for train_slice in train_slices: </span><br><span class="line">	macro_scores[&#x27;Zero Shot&#x27;].append(clf_report[&#x27;macro avg&#x27;][&#x27;f1-score&#x27;]) </span><br><span class="line">	micro_scores[&#x27;Zero Shot&#x27;].append(clf_report[&#x27;micro avg&#x27;][&#x27;f1-score&#x27;]) </span><br><span class="line">plot_metrics(micro_scores, macro_scores, train_samples, &quot;Zero Shot&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215081325505.png" alt="image-20220215081325505"></p><p>对比零次拍摄的流水线和基线，我们观察到两件事:</p><ol><li class="lvl-3"><p>如果我们有少于50个标记的样本，那么零点拍摄流水线的表现就会大大超过基线。</p></li><li class="lvl-3"><p>即使超过50个样本，在考虑微观和宏观的F-scores时，零拍流水线的性能也很优越。微观F-score的结果告诉我们，基线在频繁的类别上表现良好，而零点拍摄流水线在这些方面表现出色，因为它不需要任何例子来学习。</p></li></ol><p><strong>注意事项</strong></p><p>你可能会注意到本节中的一个小矛盾：虽然我们谈到了处理无标签的问题，但我们仍然使用验证集和测试集。我们用它们来展示不同的技术，并使它们之间的结果具有可比性。即使在一个真实的使用案例中，收集一些有标签的例子来进行一些快速评估也是有意义的。重要的一点是，我们没有用数据来调整模型的参数；相反，我们只是调整了一些超参数。</p><p>如果你发现在你自己的数据集上很难得到好的结果，这里有几件事你可以做，以改善零样本流水线：</p><ul class="lvl-0"><li class="lvl-2"><p>该流水线的工作方式使其对标签的名称非常敏感。如果这些名字没有什么意义，或者不容易与文本联系起来，那么流水线很可能会表现不佳。要么尝试使用不同的名字，要么平行使用几个名字，并在一个额外的步骤中聚合它们。</p></li><li class="lvl-2"><p>你可以改进的另一件事是假设的形式。默认情况下，它是hypothesis=“这个例子是关于{}的”，但你可以向流水线传递任何其他文本。根据不同的用例，这可能会提高性能。</p></li></ul><p>现在让我们转向制度，我们有一些标记的例子，我们可以用来训练一个模型。</p><h2 id="少样本学习">少样本学习</h2><p>在大多数NLP项目中，你至少会有机会接触到一些有标签的例子。这些标签可能直接来自客户或跨公司团队，或者你可能决定自己坐下来注释几个例子。即使是以前的方法，我们也需要一些有标签的例子来评估零拍方法的效果如何。在这一节中，我们将看看我们如何能够最好地利用我们所拥有的少数珍贵的标注过的例子。让我们先来看看一种被称为数据增强的技术，它可以帮助我们增加我们所拥有的少量标记的数据。</p><h3 id="数据增强">数据增强</h3><p>在小数据集上提高文本分类器性能的一个简单而有效的方法是应用数据增强技术，从现有的数据中产生新的训练实例。这是计算机视觉中的一种常见策略，即在不改变数据意义的情况下对图像进行随机扰动（例如，一只稍微旋转的猫仍然是一只猫）。对于文本来说，数据增强有些棘手，因为扰动单词或字符会完全改变其含义。例如，&quot;大象比老鼠重吗？&quot;和 &quot;老鼠比大象重吗？&quot;这两个问题只有一个词的互换，但答案却相反。然而，如果文本由几个句子组成（就像我们的GitHub问题那样），那么这些类型的转换所引入的噪音一般不会影响标签。在实践中，有两种类型的数据增强技术是常用的：</p><p><strong>回译</strong></p><p>将源语言的文本，用机器翻译成一种或多种目标语言，然后再将其翻译回源语言。逆向翻译往往对高资源语言或不包含太多特定领域词汇的语料库效果最好。</p><p><strong>标记扰动</strong><br>从训练集中给定一个文本，随机选择并执行简单的转换，如随机同义词替换、词的插入、交换或删除等</p><p>这些转换的例子见表9-2。关于NLP的其他数据增强技术的详细列表，我们建议阅读Amit Chaudhary的博文 “<a target="_blank" rel="noopener external nofollow noreferrer" href="https://oreil.ly/j6euX">A Visual Survey of Data Augmentation in NLP</a>”。</p><p><img src="/images/transformers/chapter9/image-20220215082024458.png" alt="image-20220215082024458"></p><p>你可以使用M2M100这样的机器翻译模型来实现回译，而NlpAug和TextAttack这样的库提供了各种标记扰动的配方。在本节中，我们将重点讨论使用同义词替换，因为它实现起来很简单，并能体现出数据增强背后的主要理念。</p><p>我们将使用NlpAug的ContextualWordEmbsAug增强器来利用DistilBERT的上下文词嵌入来替换同义词。让我们从一个简单的例子开始:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from transformers import set_seed </span><br><span class="line">import nlpaug.augmenter.word as naw</span><br><span class="line"></span><br><span class="line">set_seed(3) </span><br><span class="line">aug = naw.ContextualWordEmbsAug(model_path=&quot;distilbert-base-uncased&quot;, device=&quot;cpu&quot;, action=&quot;substitute&quot;) </span><br><span class="line">text = &quot;Transformers are the most popular toys&quot; </span><br><span class="line">print(f&quot;Original text: &#123;text&#125;&quot;) </span><br><span class="line">print(f&quot;Augmented text: &#123;aug.augment(text)&#125;&quot;) </span><br><span class="line"></span><br><span class="line">Original text: Transformers are the most popular toys </span><br><span class="line">Augmented text: transformers&#x27;the most popular toys</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这里，我们可以看到 &quot;are &quot;这个词是如何被替换成一撇的，以生成一个新的合成训练例子。我们可以用一个简单的函数来包装这个增量，如下所示:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def augment_text(batch, transformations_per_example=1): </span><br><span class="line">	text_aug, label_ids = [], [] </span><br><span class="line">	for text, labels in zip(batch[&quot;text&quot;], batch[&quot;label_ids&quot;]): </span><br><span class="line">		text_aug += [text] </span><br><span class="line">		label_ids += [labels] </span><br><span class="line">		for _ in range(transformations_per_example): </span><br><span class="line">			text_aug += [aug.augment(text)] </span><br><span class="line">			label_ids += [labels]</span><br><span class="line">    return &#123;&quot;text&quot;: text_aug, &quot;label_ids&quot;: label_ids&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在，当我们把这个函数传递给map()方法时，我们可以用transformations_per_example这个参数生成任意数量的新例子。我们可以在我们的代码中使用这个函数来训练Naive Bayes分类器，只需在我们选择分片后添加一行:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ds_train_sample = ds_train_sample.map(augment_text, batched=True, remove_columns=ds_train_sample.column_names).shuffle(seed=42)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>包括这一点并重新运行分析，产生了这里所示的图表:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plot_metrics(micro_scores, macro_scores, train_samples, &quot;Naive Bayes + Aug&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215082322966.png" alt="image-20220215082322966"></p><p>从图中我们可以看到，少量的数据扩充就能使Naive Bayes分类器的F-score提高5分左右，而且一旦我们有了170个左右的训练样本，它就会在宏观分数上超过零分流水线。现在让我们来看看基于使用大型语言模型的嵌入的方法。</p><h3 id="使用嵌入向量作为查询表">使用嵌入向量作为查询表</h3><p>像GPT-3这样的大型语言模型已经被证明在解决有限数据的任务方面表现出色。原因是这些模型学习了有用的文本表征，这些表征编码了许多维度的信息，如情感、主题、文本结构等等。为此，大型语言模型的嵌入可用于开发语义搜索引擎，寻找类似的文件或评论，甚至是对文本进行分类。</p><p>在本节中，我们将创建一个仿照OpenAI API分类端点的文本分类器。这个想法遵循一个三步骤的过程:</p><ol><li class="lvl-3"><p>使用语言模型来嵌入所有标记的文本。</p></li><li class="lvl-3"><p>对存储的嵌入文本进行近邻搜索。</p></li><li class="lvl-3"><p>汇总最近邻居的标签，以获得预测结果。</p></li></ol><p>这个过程如图9-3所示，它显示了标签数据是如何被嵌入模型并与标签一起存储的。当一个新的文本需要被分类时，它也被嵌入，并根据最近的邻居的标签给出标签。校准要搜索的邻居的数量是很重要的，因为太少可能会有噪音，太多可能会混入邻居组。</p><p><img src="/images/transformers/chapter9/image-20220215082521642.png" alt="image-20220215082521642"></p><p>这种方法的好处是，不需要对模型进行微调来利用少数可用的标记数据点。相反，使这种方法发挥作用的主要决定是选择一个适当的模型，最好是在与你的数据集类似的领域中进行预训练。</p><p>由于GPT-3只能通过OpenAI的API获得，我们将使用GPT-2来测试该技术。具体来说，我们将使用GPT-2的一个变体，该变体是在Python代码上训练的，这将有望捕捉到我们的GitHub问题中包含的一些背景。</p><p>让我们写一个辅助函数，它接收一个文本列表，并使用模型为每个文本创建一个单向量表示。我们必须处理的一个问题是，像GPT-2这样的转化器模型实际上将返回每个标记的一个嵌入向量。例如，给定句子 “我带着我的狗去散步”，我们可以期待几个嵌入向量，每个标记一个。但我们真正想要的是整个句子（或我们应用中的GitHub问题）的单一嵌入向量。为了处理这个问题，我们可以使用一种叫做池化的技术。最简单的集合方法之一是对标记嵌入进行平均，这被称为平均集合。使用均值池，我们唯一需要注意的是，在平均值中不包括填充标记，所以我们可以使用注意力掩码来处理这个问题。</p><p>为了看看这是如何工作的，让我们加载一个GPT-2标记器和模型，定义平均池操作，并将整个过程包裹在一个简单的embed_text()函数中:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch </span><br><span class="line">from transformers import AutoTokenizer, AutoModel </span><br><span class="line">model_ckpt = &quot;miguelvictor/python-gpt2-large&quot; </span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_ckpt)</span><br><span class="line">model = AutoModel.from_pretrained(model_ckpt) </span><br><span class="line">def mean_pooling(model_output, attention_mask): </span><br><span class="line">	# Extract the token embeddings </span><br><span class="line">	token_embeddings = model_output[0] </span><br><span class="line">	# Compute the attention mask </span><br><span class="line">	input_mask_expanded = (attention_mask .unsqueeze(-1) .expand(token_embeddings.size()) .float()) </span><br><span class="line">	# Sum the embeddings, but ignore masked tokens </span><br><span class="line">	sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) </span><br><span class="line">	sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9) </span><br><span class="line">	# Return the average as a single vector </span><br><span class="line">	return sum_embeddings / sum_mask </span><br><span class="line">def embed_text(examples): </span><br><span class="line">	inputs = tokenizer(examples[&quot;text&quot;], padding=True, truncation=True, max_length=128, return_tensors=&quot;pt&quot;) </span><br><span class="line">	with torch.no_grad():</span><br><span class="line">		model_output = model(**inputs) </span><br><span class="line">	pooled_embeds = mean_pooling(model_output, inputs[&quot;attention_mask&quot;]) </span><br><span class="line">	return &#123;&quot;embedding&quot;: pooled_embeds.cpu().numpy()&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在我们可以得到每个分割的嵌入。请注意，GPT风格的模型没有填充标记，因此我们需要添加一个，然后才能像前面的代码中实现的那样以分批的方式获得嵌入。我们将为此目的回收字符串末尾的标记:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tokenizer.pad_token = tokenizer.eos_token </span><br><span class="line">embs_train = ds[&quot;train&quot;].map(embed_text, batched=True, batch_size=16) </span><br><span class="line">embs_valid = ds[&quot;valid&quot;].map(embed_text, batched=True, batch_size=16) </span><br><span class="line">embs_test = ds[&quot;test&quot;].map(embed_text, batched=True, batch_size=16)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在我们有了所有的嵌入，我们需要建立一个系统来搜索它们。我们可以写一个函数来计算，比如说，我们要查询的一个新的文本嵌入和训练集中现有的嵌入之间的余弦相似度。另外，我们也可以使用数据集的一个内置结构，叫做FAISS索引。我们已经在第七章中遇到了FAISS。你可以把它看作是一个嵌入的搜索引擎，我们一会儿会仔细看看它是如何工作的。我们可以通过add_faiss_index()使用数据集的一个现有字段来创建一个FAISS索引，也可以通过add_faiss_index_from_external_arrays()将新的嵌入数据加载到数据集。让我们使用前一个函数将我们的训练嵌入物添加到数据集中，如下所示:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">embs_train.add_faiss_index(&quot;embedding&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这创建了一个新的FAISS索引，称为嵌入。我们现在可以通过调用函数get_nearest_examples()来进行最近的邻居查询。它返回最近的邻居，以及每个邻居的匹配分数。我们需要指定查询的嵌入以及要检索的最近的邻居的数量。让我们转一转，看一看与一个例子最接近的文件:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">i, k = 0, 3</span><br><span class="line"># Select the first query and 3 nearest neighbors </span><br><span class="line">rn, nl = &quot;\r\n\r\n&quot;, &quot;\n&quot;</span><br><span class="line"># Used to remove newlines in text for compact display </span><br><span class="line">query = np.array(embs_valid[i][&quot;embedding&quot;], dtype=np.float32) </span><br><span class="line">scores, samples = embs_train.get_nearest_examples(&quot;embedding&quot;, query, k=k) </span><br><span class="line">print(f&quot;QUERY LABELS: &#123;embs_valid[i][&#x27;labels&#x27;]&#125;&quot;) </span><br><span class="line">print(f&quot;QUERY TEXT:\n&#123;embs_valid[i][&#x27;text&#x27;][:200].replace(rn, nl)&#125; [...]\n&quot;) </span><br><span class="line">print(&quot;=&quot;*50) </span><br><span class="line">print(f&quot;Retrieved documents:&quot;) </span><br><span class="line">for score, label, text in zip(scores, samples[&quot;labels&quot;], samples[&quot;text&quot;]): </span><br><span class="line">	print(&quot;=&quot;*50) </span><br><span class="line">	print(f&quot;TEXT:\n&#123;text[:200].replace(rn, nl)&#125; [...]&quot;) </span><br><span class="line">	print(f&quot;SCORE: &#123;score:.2f&#125;&quot;) </span><br><span class="line">	print(f&quot;LABELS: &#123;label&#125;&quot;) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">QUERY LABELS: [&#x27;new model&#x27;] </span><br><span class="line">QUERY TEXT: </span><br><span class="line">Implementing efficient self attention in T5 # New model addition My teammates and I (including @ice-americano) would like to use efficient self attention methods such as Linformer, Performer and [...]</span><br><span class="line">================================================== </span><br><span class="line">Retrieved documents: </span><br><span class="line">================================================== </span><br><span class="line">TEXT: Add Linformer model </span><br><span class="line"></span><br><span class="line"># New model addition </span><br><span class="line">## Model description </span><br><span class="line">### Linformer: Self-Attention with Linear Complexity </span><br><span class="line"></span><br><span class="line">Paper published June 9th on ArXiv: https://arxiv.org/abs/2006.04768 La [...] SCORE: 54.92 LABELS: [&#x27;new model&#x27;] </span><br><span class="line">================================================== </span><br><span class="line">TEXT: Add FAVOR+ / Performer attention </span><br><span class="line"># FAVOR+ / Performer attention addition Are there any plans to add this new attention approximation block to </span><br><span class="line">Transformers library?</span><br><span class="line">## Model description The n [...] </span><br><span class="line">SCORE: 57.90 </span><br><span class="line">LABELS: [&#x27;new model&#x27;]</span><br><span class="line">================================================== </span><br><span class="line">TEXT: </span><br><span class="line">Implement DeLighT: Very Deep and Light-weight Transformers </span><br><span class="line"></span><br><span class="line"># New model addition ## Model description DeLight, that delivers similar or better performance than transformer-based models with sign [...] </span><br><span class="line">SCORE: 60.12 </span><br><span class="line">LABELS: [&#x27;new model&#x27;]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>很好! 这正是我们所希望的：我们通过嵌入查找得到的三个检索文件都有相同的标签，我们已经可以从标题中看到它们都非常相似。查询以及检索到的文件都是围绕着增加新的、高效的转化器模型进行的。然而，问题仍然是，什么是k的最佳值？同样地，我们应该如何汇总检索到的文件的标签？例如，我们是否应该检索三个文档，并分配所有至少出现两次的标签？或者我们应该检索20个，使用所有至少出现5次的标签？让我们系统地研究一下这个问题：我们将尝试几个k的值，然后用一个辅助函数改变标签分配的阈值m &lt; k。我们将记录每个设置的宏观和微观性能，这样我们就可以在以后决定哪一个运行表现最好。我们可以利用函数get_nearest_examples_batch()，它接受一批查询，而不是在验证集中的每个样本上循环:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_sample_preds(sample, m): </span><br><span class="line">	return (np.sum(sample[&quot;label_ids&quot;], axis=0) &gt;= m).astype(int) </span><br><span class="line">def find_best_k_m(ds_train, valid_queries, valid_labels, max_k=17):</span><br><span class="line">	max_k = min(len(ds_train), max_k) </span><br><span class="line">	perf_micro = np.zeros((max_k, max_k)) </span><br><span class="line">	perf_macro = np.zeros((max_k, max_k)) </span><br><span class="line">	for k in range(1, max_k): </span><br><span class="line">		for m in range(1, k + 1):</span><br><span class="line">        	_, samples = ds_train.get_nearest_examples_batch(&quot;embedding&quot;, valid_queries, k=k) </span><br><span class="line">        	y_pred = np.array([get_sample_preds(s, m) for s in samples]) </span><br><span class="line">        	clf_report = classification_report(valid_labels, y_pred</span><br><span class="line">target_names=mlb.classes_, zero_division=0, output_dict=True) </span><br><span class="line">			perf_micro[k, m] = clf_report[&quot;micro avg&quot;][&quot;f1-score&quot;] </span><br><span class="line">			perf_macro[k, m] = clf_report[&quot;macro avg&quot;][&quot;f1-score&quot;] </span><br><span class="line">	return perf_micro, perf_macro</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>让我们检查一下所有训练样本的最佳值是多少，并将所有k和m配置的分数可视化:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">valid_labels = np.array(embs_valid[&quot;label_ids&quot;]) </span><br><span class="line">valid_queries = np.array(embs_valid[&quot;embedding&quot;], dtype=np.float32) </span><br><span class="line">perf_micro, perf_macro = find_best_k_m(embs_train, valid_queries, valid_labels) </span><br><span class="line">fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True) </span><br><span class="line">ax0.imshow(perf_micro)</span><br><span class="line">ax1.imshow(perf_macro) </span><br><span class="line">ax0.set_title(&quot;micro scores&quot;)</span><br><span class="line">ax0.set_ylabel(&quot;k&quot;) </span><br><span class="line">ax1.set_title(&quot;macro scores&quot;) </span><br><span class="line">for ax in [ax0, ax1]: </span><br><span class="line">	ax.set_xlim([0.5, 17 - 0.5]) </span><br><span class="line">	ax.set_ylim([17 - 0.5, 0.5]) </span><br><span class="line">	ax.set_xlabel(&quot;m&quot;) </span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215101419621.png" alt="image-20220215101419621"></p><p>从图中我们可以看出，有一个模式：对于给定的k，选择m过大或过小都会产生次优结果。当选择大约m/k=1/3的比例时，会产生最佳的性能。让我们看看哪一个k和m在整体上能产生最好的结果:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">k, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape) </span><br><span class="line">print(f&quot;Best k: &#123;k&#125;, best m: &#123;m&#125;&quot;) </span><br><span class="line">Best k: 15, best m: 5</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>当我们选择k=15和m=5时，或者换句话说，当我们检索15个最近的邻居，然后分配至少出现5次的标签时，表现最好。现在我们有了一个很好的方法来寻找嵌入查找的最佳值，我们可以玩和Naive Bayes分类器一样的游戏，我们通过训练集的切片来评估性能。在我们对数据集进行切片之前，我们需要删除索引，因为我们不能像数据集那样对FAISS索引进行切片。循环的其余部分完全保持不变，只是增加了使用验证集来获得最佳的k和m值:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">embs_train.drop_index(&quot;embedding&quot;) </span><br><span class="line">test_labels = np.array(embs_test[&quot;label_ids&quot;]) </span><br><span class="line">test_queries = np.array(embs_test[&quot;embedding&quot;], dtype=np.float32) </span><br><span class="line">for train_slice in train_slices: </span><br><span class="line"># Create a Faiss index from training slice </span><br><span class="line">	embs_train_tmp = embs_train.select(train_slice) 	</span><br><span class="line">	embs_train_tmp.add_faiss_index(&quot;embedding&quot;)</span><br><span class="line">    # Get best k, m values with validation set </span><br><span class="line">    perf_micro, _ = find_best_k_m(embs_train_tmp, valid_queries, valid_labels) </span><br><span class="line">    k, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape) </span><br><span class="line">    # Get predictions on test set </span><br><span class="line">    _, samples = embs_train_tmp.get_nearest_examples_batch(&quot;embedding&quot;, test_queries, k=int(k)) </span><br><span class="line">    y_pred = np.array([get_sample_preds(s, m) for s in samples]) </span><br><span class="line">    # Evaluate predictions </span><br><span class="line">    clf_report = classification_report(test_labels, y_pred, target_names=mlb.classes_, zero_division=0, output_dict=True,) </span><br><span class="line">    macro_scores[&quot;Embedding&quot;].append(clf_report[&quot;macro avg&quot;][&quot;f1-score&quot;]) </span><br><span class="line">    micro_scores[&quot;Embedding&quot;].append(clf_report[&quot;micro avg&quot;][&quot;f1-score&quot;]</span><br><span class="line">    </span><br><span class="line">plot_metrics(micro_scores, macro_scores, train_samples, &quot;Embedding&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215101642477.png" alt="image-20220215101642477"></p><p>嵌入查找法在微观分数上与以前的方法有竞争性，同时只有两个 &quot;可学习 &quot;的参数，即k和m，但在宏观分数上的表现略差。</p><p>对这些结果持谨慎态度；哪种方法最有效在很大程度上取决于领域。零拍流水线的训练数据与我们正在使用的GitHub问题数据集有很大不同，后者包含了大量模型以前可能没有遇到过的代码。对于一个更常见的任务，如评论的情感分析，该流水线可能工作得更好。同样地，嵌入的质量也取决于模型和它所训练的数据。我们尝试了半打模型，如sentence-transformers/stsb-roberta-large，它被训练为提供高质量的句子嵌入，以及microsoft/codebertbase和dbernsohn/roberta-python，它们被训练为代码和文档。对于这个特定的用例，在Python代码上训练的GPT-2效果最好。</p><p>因为除了替换模型检查点名称以测试另一个模型外，你实际上不需要改变你的代码中的任何东西，一旦你建立了评估流水线，你可以快速尝试一些模型。现在让我们把这个简单的嵌入技巧与在有限的数据上简单地微调一个转化器进行比较。</p><hr><p><strong>用Faiss进行高效的相似性搜索</strong></p><p>我们在第七章中第一次接触到FAISS，在那里我们用它通过DPR嵌入来检索文档。这里我们将简要地解释FAISS库是如何工作的，以及为什么它是ML工具箱中的一个强大工具。我们习惯于在巨大的数据集上进行快速的文本查询，如维基百科或网络上的搜索引擎，如谷歌。当我们从文本转移到嵌入时，我们希望保持这种性能；然而，用于加快文本查询的方法并不适用于嵌入。为了加快文本搜索的速度，我们通常会创建一个倒置的索引，将术语映射到文档。倒置索引的工作原理类似于书末的索引：每个词都被映射到它所出现的页面（或者在我们的例子中，是文档）。当我们以后运行一个查询时，我们可以快速查找搜索词出现在哪些文档中。这对离散的对象（如单词）很有效，但对连续的对象（如矢量）则无效。每个文档都可能有一个独特的向量，因此索引将永远不会与新的向量相匹配。我们不需要寻找精确的匹配，而是需要寻找接近或类似的匹配。当我们想找到数据库中与查询向量最相似的向量时，理论上我们需要将查询向量与数据库中的每个向量进行比较。对于像本章中的小数据库来说，这是没有问题的，但是如果我们将其扩大到几千甚至几百万条，我们就需要等待一段时间来处理每个查询。FAISS通过一些技巧来解决这个问题。主要的想法是对数据集进行分区。如果我们只需要将查询向量与数据库的一个子集进行比较，我们可以大大加快处理速度。但是，如果我们只是随机地对数据集进行分区，我们如何决定搜索哪个分区，以及我们对找到最相似的条目有何保证？显然，一定有一个更好的解决方案：对数据集进行kmeans聚类！这是对数据集的聚类。这将嵌入物按相似度聚成一组。此外，对于每个组，我们得到一个中心点向量，它是该组所有成员的平均值（图9-4）。</p><p><img src="/images/transformers/chapter9/image-20220215101845554.png" alt="image-20220215101845554"></p><p>鉴于这样的分组，在n个向量中搜索就容易多了：我们首先在k个中心点中搜索与我们的查询最相似的那个（k个比较），然后在组内搜索（元素比较）。这就把比较的数量从n减少到k+(n/k) 。那么问题来了，k的最佳选择是什么？如果它太小，每组仍然包含许多样本，我们需要在第二步进行比较，如果k太大，有许多中心点我们需要搜索。寻找函数f（k）=k+ (n/k) 关于k的最小值，我们发现k=√n。事实上，我们可以用下面的图形直观地说明这一点，n=2^20。</p><p><img src="/images/transformers/chapter9/image-20220215101959379.png" alt="image-20220215101959379"></p><p>在图中，你可以看到比较的数量是集群数量的一个函数。我们正在寻找这个函数的最小值，即我们需要做最少的比较的地方。我们可以看到，最小值正是我们期望看到的地方，在</p><p>√(2<sup>20)=2</sup>10=1024 处。</p><p>除了用分区来加快查询速度，FAISS还允许你利用GPU来进一步加快速度。如果内存成为一个问题，也有几个选项可以用高级量化方案压缩向量。如果你想在你的项目中使用FAISS，资源库有一个简单的指南，让你为你的用例选择正确的方法。</p><p>使用FAISS的最大项目之一是Facebook创建的CCMatrix语料库。作者使用多语言嵌入来寻找不同语言的平行句子。这个巨大的语料库随后被用来训练M2M100，这是一个大型的机器翻译模型，能够在100种语言中的任何一种之间进行直接翻译。</p><hr><h3 id="微调一个普通的Transformers">微调一个普通的Transformers</h3><p>如果我们有机会获得标记的数据，我们也可以尝试做一些显而易见的事情：简单地微调预训练的转化器模型。在本节中，我们将使用标准的BERT检查点作为起点。稍后，我们将看到微调语言模型对性能的影响。</p><p><strong>小技巧</strong></p><p>对于许多应用来说，从一个预训练的类似BERT的模型开始是一个好主意。然而，如果你的语料库的领域与预训练语料库（通常是维基百科）有很大的不同，你应该探索在Hugging Face Hub上的许多模型。很有可能有人已经在你的领域上预训练了一个模型!</p><p>让我们从加载预训练的标记器开始，对我们的数据集进行标记，并去掉我们在训练和评估中不需要的列:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch </span><br><span class="line">from transformers import (AutoTokenizer, AutoConfig, AutoModelForSequenceClassification) </span><br><span class="line"></span><br><span class="line">model_ckpt = &quot;bert-base-uncased&quot; </span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_ckpt) </span><br><span class="line">def tokenize(batch): </span><br><span class="line">	return tokenizer(batch[&quot;text&quot;], truncation=True, max_length=128) </span><br><span class="line">ds_enc = ds.map(tokenize, batched=True) </span><br><span class="line">ds_enc = ds_enc.remove_columns([&#x27;labels&#x27;, &#x27;text&#x27;])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>多标签损失函数希望标签的类型是浮动的，因为它也允许类概率而不是离散的标签。因此，我们需要改变列 label_ids 的类型。由于从元素上改变列的格式与Arrow的类型格式不相称，我们将做一个小小的变通。首先，我们创建一个带有标签的新列。该列的格式是由第一个元素推断出来的。然后，我们删除原来的列，重命名新的列以取代原来的列:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ds_enc.set_format(&quot;torch&quot;) </span><br><span class="line">ds_enc = ds_enc.map(lambda x: &#123;&quot;label_ids_f&quot;: x[&quot;label_ids&quot;].to(torch.float)&#125;, remove_columns=[&quot;label_ids&quot;]) </span><br><span class="line">ds_enc = ds_enc.rename_column(&quot;label_ids_f&quot;, &quot;label_ids&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>由于训练数据的规模有限，我们很可能很快过度拟合，所以我们设置load_best_model_at_end=True，并根据微观F - score选择最佳模型:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from transformers import Trainer, TrainingArguments </span><br><span class="line">training_args_fine_tune = TrainingArguments( output_dir=&quot;./results&quot;, num_train_epochs=20, learning_rate=3e-5, lr_scheduler_type=&#x27;constant&#x27;, per_device_train_batch_size=4, per_device_eval_batch_size=32, weight_decay=0.0, evaluation_strategy=&quot;epoch&quot;, save_strategy=&quot;epoch&quot;,logging_strategy=&quot;epoch&quot;, load_best_model_at_end=True, metric_for_best_model=&#x27;micro f1&#x27;, save_total_limit=1, log_level=&#x27;error&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们需要F-score来选择最佳模型，所以我们需要确保在评估期间计算它。因为模型返回的是对数，我们首先需要用sigmoid函数对预测进行归一化，然后可以用一个简单的阈值对其进行二进制化。然后，我们从分类报告中返回我们感兴趣的分数:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from scipy.special import expit as sigmoid </span><br><span class="line">def compute_metrics(pred): </span><br><span class="line">	y_true = pred.label_ids</span><br><span class="line">    y_pred = sigmoid(pred.predictions) </span><br><span class="line">    y_pred = (y_pred&gt;0.5).astype(float) </span><br><span class="line">    clf_dict = classification_report(y_true, y_pred, target_names=all_labels, zero_division=0, output_dict=True) </span><br><span class="line">    return &#123;&quot;micro f1&quot;: clf_dict[&quot;micro avg&quot;][&quot;f1-score&quot;], &quot;macro f1&quot;: clf_dict[&quot;macro avg&quot;][&quot;f1-score&quot;]&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在，我们已经准备好了。对于每个训练集片，我们从头开始训练一个分类器，在训练循环结束时加载最佳模型，并将结果存储在测试集上:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">config = AutoConfig.from_pretrained(model_ckpt) </span><br><span class="line">config.num_labels = len(all_labels) </span><br><span class="line">config.problem_type = &quot;multi_label_classification&quot; </span><br><span class="line">for train_slice in train_slices: </span><br><span class="line">	model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config) </span><br><span class="line">	trainer = Trainer( model=model, tokenizer=tokenizer, args=training_args_fine_tune, compute_metrics=compute_metrics, train_dataset=ds_enc[&quot;train&quot;].select(train_slice), eval_dataset=ds_enc[&quot;valid&quot;],) </span><br><span class="line">	trainer.train() </span><br><span class="line">	pred = trainer.predict(ds_enc[&quot;test&quot;]) </span><br><span class="line">	metrics = compute_metrics(pred) </span><br><span class="line">	macro_scores[&quot;Fine-tune (vanilla)&quot;].append(metrics[&quot;macro f1&quot;]) </span><br><span class="line">	micro_scores[&quot;Fine-tune (vanilla)&quot;].append(metrics[&quot;micro f1&quot;]) </span><br><span class="line"></span><br><span class="line">plot_metrics(micro_scores, macro_scores, train_samples, &quot;Fine-tune (vanilla)&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215103020543.png" alt="image-20220215103020543"></p><p>首先，我们看到，当我们有机会接触到大约64个例子时，在数据集上简单地微调一个虚构的BERT模型就能得到有竞争力的结果。我们还看到，在这之前，行为有点不稳定，这也是由于在小样本上训练模型，其中一些标签可能是不利的不平衡的。在我们利用我们的数据集的无标签部分之前，让我们快速看一下另一种有前途的方法，即在少有的领域中使用语言模型。</p><h3 id="基于提示的上下文学习和少样本学习">基于提示的上下文学习和少样本学习</h3><p>我们在本章前面看到，我们可以使用像BERT或GPT-2这样的语言模型，并通过使用提示语和解析模型的标记预测来使其适应监督任务。这与添加一个特定任务的头并为任务调整模型参数的经典方法不同。从好的方面看，这种方法不需要任何训练数据，但从坏的方面看，如果我们有机会获得标签数据，似乎就不能利用。有一个中间地带，我们有时可以利用它，叫做in-context或sew-shot学习。</p><p>为了说明这个概念，请考虑一个英语到法语的翻译任务。在0-shot范式中，我们将构建一个提示，可能看起来如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">prompt = &quot;&quot;&quot;\ Translate English to French: thanks =&gt; &quot;&quot;&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这希望能促使模型预测 &quot;merci &quot;这个词的标记。我们在第六章中使用GPT-2进行总结时已经看到，在文本中加入 &quot;TL;DR &quot;会促使模型产生一个总结，而不需要明确地进行训练。GPT-3论文的一个有趣的发现是大型语言模型能够有效地从提示中提出的例子中学习–因此，前面的翻译例子可以用几个英译德的例子来增加，这将使模型在这个任务中表现得更好。</p><p>此外，作者发现，模型的规模越大，它们在使用语境中的例子时就越好，从而导致性能的大幅提升。尽管GPT-3大小的模型在生产中的使用具有挑战性，但这是一个令人兴奋的新兴研究领域，人们已经建立了很酷的应用，比如一个自然语言的外壳，用自然语言输入命令并由GPT-3解析为外壳命令。</p><p>使用标记数据的另一种方法是创建提示和预期预测的例子，并在这些例子上继续训练语言模型。一种名为ADAPET的新方法采用了这种方法，并在各种任务上击败了GPT-3，用生成的提示语来调整模型。Hugging Face的研究人员最近的工作表明，这样的方法可以比微调一个自定义的头更具有数据效率。</p><p>在这一节中，我们简要地看了各种方法，以很好地利用我们所拥有的少数标记的例子。很多时候，除了标注的例子之外，我们还可以获得大量未标注的数据；在下一节，我们将讨论如何很好地利用这些数据。</p><h2 id="利用无标签的数据">利用无标签的数据</h2><p>尽管能够获得大量高质量的标记数据是训练分类器的最佳情况，但这并不意味着无标记数据毫无价值。只要想想我们所使用的大多数模型的预训练：即使它们是在互联网上的大部分不相关的数据上训练的，我们也可以利用预训练的权重在各种文本上进行其他任务。这就是NLP中转移学习的核心思想。当然，如果下游任务的文本结构与预训练的文本相似，那么迁移的效果会更好，所以如果我们能使预训练的任务更接近下游的目标，我们就有可能提高迁移的效果。</p><p>让我们从我们的具体用例来思考这个问题。BERT在BookCorpus和英语维基百科上进行了预训练，而包含代码和GitHub问题的文本在这些数据集中绝对是一个小众的。如果我们从头开始预训练BERT，我们可以在抓取GitHub上的所有问题时进行训练，比如说。然而，这将是昂贵的，而且BERT所学到的关于语言的很多方面仍然对GitHub问题有效。那么，在从头开始重新训练和仅仅使用模型进行分类之间是否有一个中间地带？有的，这就是所谓的领域适应（我们在第7章也看到了问题回答的情况）。与其从头开始重新训练语言模型，我们可以在我们领域的数据上继续训练它。在这一步中，我们使用经典的语言模型目标，即预测屏蔽词，这意味着我们不需要任何标记的数据。之后，我们可以将适应的模型加载为分类器，并对其进行微调，从而利用未标记的数据。</p><p>领域适应的好处是，与标记的数据相比，未标记的数据往往是大量可用的。此外，适应的模型可以在许多用例中重复使用。想象一下，你想建立一个电子邮件分类器，并在你所有的历史邮件上应用域适应。你以后可以将同一个模型用于命名实体识别或另一个分类任务，如情感分析，因为该方法对下游任务是不可知的。</p><p>现在让我们看看我们需要采取哪些步骤来微调一个预训练的语言模型。</p><h3 id="对语言模型进行微调">对语言模型进行微调</h3><p>在这一节中，我们将在数据集的未标记部分用屏蔽语言建模来微调预训练的BERT模型。要做到这一点，我们只需要两个新的概念：在对数据进行标记时的一个额外步骤和一个特殊的数据整理器。让我们从标记化开始。</p><p>除了文本中的普通标记外，标记器还将特殊标记添加到序列中，如用于分类和下句预测的[CLS]和[SEP]标记。当我们做屏蔽式语言建模时，我们要确保我们不会训练模型来预测这些标记。出于这个原因，我们将它们从损失中屏蔽掉，我们可以通过设置return_special_tokens_mask=True，在标记化时得到一个屏蔽。让我们用这个设置对文本进行重新标记。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def tokenize(batch):</span><br><span class="line">	return tokenizer(batch[&quot;text&quot;], truncation=True, max_length=128, return_special_tokens_mask=True) </span><br><span class="line">	ds_mlm = ds.map(tokenize, batched=True) </span><br><span class="line">	ds_mlm = ds_mlm.remove_columns([&quot;labels&quot;, &quot;text&quot;, &quot;label_ids&quot;])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在开始进行遮蔽语言建模时，缺少的是在输入序列中遮蔽标记并在输出中拥有目标标记的机制。我们可以采取的一种方法是，设置一个掩蔽随机标记的函数，并为这些序列创建标签。但这将使数据集的大小增加一倍，因为我们还要在数据集中存储目标序列，而且这意味着我们将在每个时代使用相同的序列掩码。</p><p>一个更优雅的解决方案是使用一个数据整理器。记住，数据整理器是在数据集和模型调用之间建立桥梁的函数。一个批次从数据集中取样，数据整理器准备好批次中的元素并将其送入模型。在我们遇到的最简单的情况下，它只是将每个元素的张量连接成一个张量。在我们的案例中，我们可以用它来做屏蔽和标签的生成。这样我们就不需要存储标签，而且每次采样都会得到新的掩码。这个任务的数据整理器被称为DataCollatorForLanguageModeling。我们用模型的标记器和我们想通过mlm_probability参数掩盖的标记的比例来初始化它。我们将使用这个整理器来屏蔽15%的标记，这与BERT论文中的程序一致：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from transformers import DataCollatorForLanguageModeling, set_seed </span><br><span class="line">data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>让我们快速看一下数据整理器的运行情况，看看它到底做了什么。为了在DataFrame中快速显示结果，我们将标记器和数据整理器的返回格式切换为NumPy：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set_seed(3) </span><br><span class="line">data_collator.return_tensors = &quot;np&quot; </span><br><span class="line">inputs = tokenizer(&quot;Transformers are awesome!&quot;, return_tensors=&quot;np&quot;) </span><br><span class="line">outputs = data_collator([&#123;&quot;input_ids&quot;: inputs[&quot;input_ids&quot;][0]&#125;])</span><br><span class="line"></span><br><span class="line">pd.DataFrame(&#123; &quot;Original tokens&quot;: tokenizer.convert_ids_to_tokens(inputs[&quot;input_ids&quot;] [0]), &quot;Masked tokens&quot;: tokenizer.convert_ids_to_tokens(outputs[&quot;input_ids&quot;][0]), &quot;Original input_ids&quot;: original_input_ids, &quot;Masked input_ids&quot;: masked_input_ids, &quot;Labels&quot;: outputs[&quot;labels&quot;][0]&#125;).T</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215103800026.png" alt="image-20220215103800026"></p><p>我们看到对应于感叹号的令牌已经被替换成了一个屏蔽令牌。此外，数据整理器返回了一个标签数组，其中原始令牌为-100，被屏蔽的令牌为令牌ID。正如我们之前看到的，在计算损失时，包含-100的条目被忽略。让我们把数据整理器的格式换回PyTorch：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data_collator.return_tensors = &quot;pt&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>随着标记器和数据整理器的到位，我们准备对屏蔽的语言模型进行微调。我们像往常一样设置TrainingArguments和Trainer：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from transformers import AutoModelForMaskedLM </span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments( output_dir = f&quot;&#123;model_ckpt&#125;-issues-128&quot;, per_device_train_batch_size=32, logging_strategy=&quot;epoch&quot;, evaluation_strategy=&quot;epoch&quot;, save_strategy=&quot;no&quot;, num_train_epochs=16, push_to_hub=True, log_level=&quot;error&quot;, report_to=&quot;none&quot;) </span><br><span class="line"></span><br><span class="line">trainer = Trainer( model=AutoModelForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;), tokenizer=tokenizer, args=training_args, data_collator=data_collator, train_dataset=ds_mlm[&quot;unsup&quot;], eval_dataset=ds_mlm[&quot;train&quot;]) </span><br><span class="line"></span><br><span class="line">trainer.train() </span><br><span class="line">trainer.push_to_hub(&quot;Training complete!&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们可以访问训练器的日志历史，查看模型的训练和验证损失。所有的日志都存储在trainer.state.log_history中，作为一个字典列表，我们可以很容易地加载到Pandas DataFrame中。由于训练和验证损失是在不同的步骤中记录的，所以数据框架中存在缺失值。出于这个原因，我们在绘制指标之前，先把缺失的值删除：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df_log = pd.DataFrame(trainer.state.log_history) </span><br><span class="line"></span><br><span class="line">(df_log.dropna(subset=[&quot;eval_loss&quot;]).reset_index()[&quot;eval_loss&quot;] .plot(label=&quot;Validation&quot;)) </span><br><span class="line"></span><br><span class="line">df_log.dropna(subset=[&quot;loss&quot;]).reset_index()[&quot;loss&quot;].plot(label=&quot;Train&quot;)</span><br><span class="line">plt.xlabel(&quot;Epochs&quot;) </span><br><span class="line">plt.ylabel(&quot;Loss&quot;) </span><br><span class="line">plt.legend(loc=&quot;upper right&quot;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215104139493.png" alt="image-20220215104139493"></p><p>看来，训练和验证损失都大大降低了。因此，让我们检查一下，当我们对基于这个模型的分类器进行微调时，是否也能看到改进。</p><h3 id="对分类器进行微调">对分类器进行微调</h3><p>现在，我们将重复微调程序，但有一点不同的是，我们将加载我们自己的自定义模型：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">model_ckpt = f&#x27;&#123;model_ckpt&#125;-issues-128&#x27; </span><br><span class="line">config = AutoConfig.from_pretrained(model_ckpt) </span><br><span class="line">config.num_labels = len(all_labels) </span><br><span class="line">config.problem_type = &quot;multi_label_classification&quot; </span><br><span class="line">for train_slice in train_slices: </span><br><span class="line">	model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config) </span><br><span class="line">	trainer = Trainer( model=model, tokenizer=tokenizer, args=training_args_fine_tune, compute_metrics=compute_metrics</span><br><span class="line">train_dataset=ds_enc[&quot;train&quot;].select(train_slice), eval_dataset=ds_enc[&quot;valid&quot;], )</span><br><span class="line">	trainer.train() </span><br><span class="line">	pred = trainer.predict(ds_enc[&#x27;test&#x27;])</span><br><span class="line">	 metrics = compute_metrics(pred) </span><br><span class="line">	 # DA refers to domain adaptation </span><br><span class="line">	 macro_scores[&#x27;Fine-tune (DA)&#x27;].append(metrics[&#x27;macro f1&#x27;]) </span><br><span class="line">	 micro_scores[&#x27;Fine-tune (DA)&#x27;].append(metrics[&#x27;micro f1&#x27;])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>将结果与基于vanilla BERT的微调进行比较，我们看到，我们得到了一个优势，特别是在低数据领域。在有更多标记数据的情况下，我们也获得了几个百分点：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plot_metrics(micro_scores, macro_scores, train_samples, &quot;Fine-tune (DA)&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/transformers/chapter9/image-20220215105444900.png" alt="image-20220215105444900"></p><p>这突出表明，领域适应可以通过未标记的数据和少量的努力为模型的性能提供轻微的提升。自然，未标注的数据越多，标注的数据越少，你用这种方法得到的影响就越大。在结束本章之前，我们将向你展示一些利用无标签数据的更多技巧。</p><h3 id="高级方法">高级方法</h3><p>在调整分类头之前对语言模型进行微调是一个简单而可靠的方法，可以提高性能。然而，还有一些复杂的方法比可以进一步利用未标记的数据。我们在此总结了其中的一些方法，如果你需要更多的性能，这些方法应该提供一个很好的起点。</p><h4 id="无监督的数据增强">无监督的数据增强</h4><p>无监督数据增强（UDA）的关键思想是，一个模型的预测对于一个没有标签的例子和一个稍微扭曲的例子应该是一致的。这种扭曲是通过标准的数据增强策略引入的，如标记替换和回译。然后，通过最小化原始例子和扭曲的例子的预测之间的KL分歧来强制实现一致性。这个过程如图9-5所示，其中一致性要求是通过增加交叉熵损失和来自未标记的例子的额外项来实现的。这意味着，我们用标准的监督方法在有标签的数据上训练一个模型，但限制该模型对无标签的数据做出一致的预测。</p><p><img src="/images/transformers/chapter9/image-20220215105735379.png" alt="image-20220215105735379"></p><p>这种方法的性能相当令人印象深刻：用UDA训练的BERT模型在少数有标签的例子中得到的性能与在成千上万的例子中训练的模型相似。缺点是，你需要一个数据增强流水线，而且训练时间更长，因为你需要多个前向通道来生成对未标记和增强的例子的预测分布。</p><h4 id="不确定性感知自训练">不确定性感知自训练</h4><p>另一种有前途的利用无标签数据的方法是不确定性感知自我训练（UST）。这里的想法是在有标签的数据上训练一个教师模型，然后用这个模型在未标记的数据上创建伪标签。然后在伪标签数据上训练一个学生模型，训练结束后它就成为下一次迭代的教师模型。</p><p>这种方法的一个有趣的方面是伪标签是如何产生的：为了得到模型预测的不确定性测量，在开启滤波的情况下，同一输入被多次送入模型。然后，预测的方差可以代表模型对特定样本的确定性。有了这个不确定性的测量，假标签就可以用一种叫做 “基于分歧的贝叶斯主动学习”（BALD）的方法进行采样。图9-6说明了完整的训练流水线。</p><p><img src="/images/transformers/chapter9/image-20220215105844174.png" alt=""></p><p>通过这种迭代方案，教师模型在创建伪标签方面不断变得更好，从而提高了模型的性能。最后，这种方法在数千个样本的完整训练数据上训练出来的模型中只占百分之几，甚至在几个数据集上击败了UDA。</p><p>现在我们已经看到了一些先进的方法，让我们回过头来总结一下我们在本章中所学到的东西。</p><h2 id="小结-4">小结</h2><p>在本章中，我们已经看到，即使我们只有几个甚至没有标签，也不是没有希望。我们可以利用在其他任务上预训练过的模型，比如在Python代码上训练的BERT语言模型或GPT-2，来对GitHub问题分类这个新任务进行预测。此外，我们可以利用领域适应性，在用普通分类头训练模型时获得额外的提升。</p><p>所介绍的哪种方法在特定的用例中效果最好，取决于多个方面：你有多少标记的数据，它有多大的噪声，数据与预训练语料库有多接近，等等。为了找出最有效的方法，建立一个评估流水线，然后快速迭代是个好主意。Transformers灵活的API允许你快速加载少量的模型并进行比较，而不需要修改任何代码。在Hugging Face Hub上有超过10,000个模型，有可能有人在过去曾处理过类似的问题，你可以在此基础上建立。超出本书范围的一个方面是像UDA或UST这样更复杂的方法与获得更多数据之间的权衡。为了评估你的方法，至少在早期建立一个验证和测试集是有意义的。在每一步，你也可以收集更多的标注数据。通常情况下，对几百个例子进行标注是几个小时或几天的工作，有很多工具可以帮助你这样做。根据你想要实现的目标，投入一些时间来创建一个小型的、高质量的数据集，而不是用一个非常复杂的方法来弥补数据集的不足，是有意义的。通过我们在本章介绍的方法，你可以确保从你宝贵的标签数据中获得最大的价值。</p><p>在这里，我们已经冒险进入了低数据系统，并看到即使只有一百个例子，转化器模型仍然是强大的。在下一章中，我们将看看完全相反的情况：我们将看看当我们有数百GB的数据和大量的计算时，我们能做什么。我们将从头开始训练一个大型的转化器模型来为我们自动完成代码。</p></article><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-rr7ldm_1920x1080.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/images/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/images/wechat.jpg" alt="wechat"></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/images/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/images/alipay.jpg" alt="alipay"></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/cc3455bf.html" title="从头训练 Transformers 模型"><img class="cover" src="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-85xm2j_1920x1080.webp" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">从头训练 Transformers 模型</div></div></a></div><div class="next-post pull-right"><a href="/posts/910a108.html" title="优化Transformers性能"><img class="cover" src="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-jxy6vp_1920x1080.webp" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">优化Transformers性能</div></div></a></div></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://oss.kiscloud.net/image/user/anime/Chainsaw/qiu.webp" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">ihadu</div><div class="author-info__description">寄蜉蝣于天地，渺沧海之一粟。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">247</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">32</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">36</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/ihadu"><i class="fab fa-github"></i><span>look Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ihadu" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:ihadyou@qq.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://weibo.com/u/5992742619" rel="external nofollow noreferrer" target="_blank"><i class="fa-brands fa-weibo"></i></a><a class="social-icon" href="https://www.cnblogs.com/ihadu/post-categories/#/c/subject/category/default.html" rel="external nofollow noreferrer" target="_blank"><i class="fa-solid fa-blog"></i></a><a class="social-icon" href="/nav/" target="_blank"><i class="fa fa-paper-plane"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Have you found your way home yet</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">第9章 处理少标签或零标签情形</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AAGitHub-Issue%E6%A0%87%E6%B3%A8%E7%A8%8B%E5%BA%8F"><span class="toc-number">1.1.</span> <span class="toc-text">构建一个GitHub Issue标注程序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-number">1.1.1.</span> <span class="toc-text">获取数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="toc-number">1.1.2.</span> <span class="toc-text">准备数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%AE%AD%E7%BB%83%E9%9B%86"><span class="toc-number">1.1.3.</span> <span class="toc-text">创建训练集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%AE%AD%E7%BB%83%E5%88%87%E7%89%87"><span class="toc-number">1.1.4.</span> <span class="toc-text">创建训练切片</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9F%BA%E7%BA%BF"><span class="toc-number">1.2.</span> <span class="toc-text">实现朴素贝叶斯基线</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.</span> <span class="toc-text">零样本学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%91%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.4.</span> <span class="toc-text">少样本学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">1.4.1.</span> <span class="toc-text">数据增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%B5%8C%E5%85%A5%E5%90%91%E9%87%8F%E4%BD%9C%E4%B8%BA%E6%9F%A5%E8%AF%A2%E8%A1%A8"><span class="toc-number">1.4.2.</span> <span class="toc-text">使用嵌入向量作为查询表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E6%99%AE%E9%80%9A%E7%9A%84Transformers"><span class="toc-number">1.4.3.</span> <span class="toc-text">微调一个普通的Transformers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%8F%90%E7%A4%BA%E7%9A%84%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%B0%91%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.4.4.</span> <span class="toc-text">基于提示的上下文学习和少样本学习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%A9%E7%94%A8%E6%97%A0%E6%A0%87%E7%AD%BE%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="toc-number">1.5.</span> <span class="toc-text">利用无标签的数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83"><span class="toc-number">1.5.1.</span> <span class="toc-text">对语言模型进行微调</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E5%88%86%E7%B1%BB%E5%99%A8%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83"><span class="toc-number">1.5.2.</span> <span class="toc-text">对分类器进行微调</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7%E6%96%B9%E6%B3%95"><span class="toc-number">1.5.3.</span> <span class="toc-text">高级方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">1.5.3.1.</span> <span class="toc-text">无监督的数据增强</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%84%9F%E7%9F%A5%E8%87%AA%E8%AE%AD%E7%BB%83"><span class="toc-number">1.5.3.2.</span> <span class="toc-text">不确定性感知自训练</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-4"><span class="toc-number">1.6.</span> <span class="toc-text">小结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/bf3a593a.html" title="未来发展方向"><img src="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-e7d1q8_1920x1080.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="未来发展方向"></a><div class="content"><a class="title" href="/posts/bf3a593a.html" title="未来发展方向">未来发展方向</a><time datetime="2024-01-22T07:35:24.000Z" title="发表于 2024-01-22 15:35:24">2024-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/cc3455bf.html" title="从头训练 Transformers 模型"><img src="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-85xm2j_1920x1080.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="从头训练 Transformers 模型"></a><div class="content"><a class="title" href="/posts/cc3455bf.html" title="从头训练 Transformers 模型">从头训练 Transformers 模型</a><time datetime="2024-01-22T07:34:57.000Z" title="发表于 2024-01-22 15:34:57">2024-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3359ec3c.html" title="处理少标签或零标签情形"><img src="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-rr7ldm_1920x1080.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="处理少标签或零标签情形"></a><div class="content"><a class="title" href="/posts/3359ec3c.html" title="处理少标签或零标签情形">处理少标签或零标签情形</a><time datetime="2024-01-22T07:34:26.000Z" title="发表于 2024-01-22 15:34:26">2024-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/910a108.html" title="优化Transformers性能"><img src="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-jxy6vp_1920x1080.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="优化Transformers性能"></a><div class="content"><a class="title" href="/posts/910a108.html" title="优化Transformers性能">优化Transformers性能</a><time datetime="2024-01-22T07:33:50.000Z" title="发表于 2024-01-22 15:33:50">2024-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/c0b5c215.html" title="问答系统"><img src="https://oss.kiscloud.net/image/user/anime/Chainsaw/wallhaven-m3gl2m_1920x1080.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="问答系统"></a><div class="content"><a class="title" href="/posts/c0b5c215.html" title="问答系统">问答系统</a><time datetime="2024-01-22T07:32:45.000Z" title="发表于 2024-01-22 15:32:45">2024-01-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2025 By ihadu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><img src="https://haiyong.site/img/icp.png"><a href="https://beian.miit.gov.cn/#/Integrated/index" rel="external nofollow noreferrer" style="color:#fff" target="_blank">皖ICP备19024061号-4</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.19/fancybox/fancybox.umd.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.2.0/instantpage.min.js" type="module"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/node-snackbar/0.1.16/snackbar.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js").then(()=>{pangu.autoSpacingPage()})}function panguInit(){GLOBAL_CONFIG_SITE.isPost&&panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>function loadValine(){function n(){new Valine(Object.assign({el:"#vcomment",appId:"kriIEcS9Jj1uTpNoV1sFu2Rk-gzGzoHsz",appKey:"mnmRNJ7HtlpE7664tpn7Kfmy",avatar:"monsterid",serverURLs:"",emojiMaps:"",path:window.location.pathname,visitor:!1},null))}"function"==typeof Valine?n():getScript("https://cdnjs.cloudflare.com/ajax/libs/valine/1.5.1/Valine.min.js").then(n)}function loadOtherComment(){loadValine()}setTimeout(loadValine,0)</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.3/fireworks.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.3/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!0,POWERMODE.mobile=!0,document.body.addEventListener("input",POWERMODE)</script><script src="https://cdnjs.cloudflare.com/ajax/libs/pjax/0.2.8/pjax.min.js"></script><script>let pjaxSelectors=["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"];var pjax=new Pjax({elements:'a:not([target="_blank"]):not([href="/music/"])',selectors:pjaxSelectors,cacheBust:!1,analytics:!1,scrollRestoration:!1});document.addEventListener("pjax:send",function(){if(window.tocScrollFn&&window.removeEventListener("scroll",window.tocScrollFn),window.scrollCollect&&window.removeEventListener("scroll",scrollCollect),document.getElementById("rightside").style.cssText="opacity: ''; transform: ''",window.aplayers)for(let e=0;e<window.aplayers.length;e++)window.aplayers[e].options.fixed||window.aplayers[e].destroy();"object"==typeof typed&&typed.destroy();var e=document.body.classList;e.contains("read-mode")&&e.remove("read-mode"),"object"==typeof disqusjs&&disqusjs.destroy()}),document.addEventListener("pjax:complete",function(){window.refreshFn(),document.querySelectorAll("script[data-pjax]").forEach(e=>{const t=document.createElement("script");var o=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach(e=>t.setAttribute(e.name,e.value)),t.appendChild(document.createTextNode(o)),e.parentNode.replaceChild(t,e)}),GLOBAL_CONFIG.islazyload&&window.lazyLoadInstance.update(),"function"==typeof panguInit&&panguInit(),"function"==typeof gtag&&gtag("config","",{page_path:window.location.pathname}),"object"==typeof _hmt&&_hmt.push(["_trackPageview",window.location.pathname]),"function"==typeof loadMeting&&document.getElementsByClassName("aplayer").length&&loadMeting(),"object"==typeof Prism&&Prism.highlightAll()}),document.addEventListener("pjax:error",e=>{404===e.request.status&&pjax.loadUrl("/404.html")})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{scale:1,hHeadPos:.5,vHeadPos:.618,jsonPath:"/live2d_models/gun/95type_405/normal/model.json"},display:{superSample:2,position:"left",width:200,height:400,hOffset:30,vOffset:-80},mobile:{show:!1,scale:1},react:{opacityDefault:.3,opacityOnHover:.3,opacity:.95},dialog:{enable:!0,hitokoto:!0},log:!1})</script></body></html>